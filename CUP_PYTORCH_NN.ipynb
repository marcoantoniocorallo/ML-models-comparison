{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from cup_helpers import SEED\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cup_helpers import CV\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 10\n",
    "OUTPUT_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "SEEDS = list(range(40,46))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets Path\n",
    "TR_PATH = \"./data/ML-CUP23-TR.csv\"\n",
    "TS_PATH = \"./data/ML-CUP23-TS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_graph(train_losses,losses,epochs,title):\n",
    "    num_epochs = list(range(0, epochs))  \n",
    "    print(len(num_epochs))\n",
    "    print((train_losses))\n",
    "    print((losses))\n",
    "    # Plotting\n",
    "    plt.plot(num_epochs, train_losses, label=' Training Loss ')\n",
    "    plt.plot(num_epochs, losses, label=title+' Loss')\n",
    "\n",
    "    plt.title('Training and '+title+' Losses Across Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, input_size,layers_size, activation_function=\"relu\"):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.activation_function = activation_function\n",
    "    for layer_size in layers_size:\n",
    "      self.layers.append(nn.Linear(input_size,layer_size))\n",
    "      input_size = layer_size\n",
    "\n",
    "  def forward(self, x):\n",
    "    for i, layer in enumerate(self.layers):\n",
    "            if i != (len(self.layers) - 1):\n",
    "              # Apply the layer and a ReLU activation\n",
    "              if self.activation_function == \"relu\":   \n",
    "                x = torch.relu((layer(x)))\n",
    "              elif self.activation_function == \"tanh\":\n",
    "                x = torch.tanh((layer(x)))\n",
    "            else:\n",
    "              x = layer(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def reset_weights(net):\n",
    "  for param in net.parameters():\n",
    "    torch.nn.init.uniform_(param, a=-0.7, b=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_ds(path,split):\n",
    "  \"\"\"\n",
    "  parse CSV data set and\n",
    "  returns a tuple (input, target)\n",
    "  \"\"\"\n",
    "  data = pd.read_csv(path, dtype=object, delimiter=\",\", header=None, skiprows=1, names=[\"id\", \"INPUT_0\", \"INPUT_1\", \"INPUT_2\", \"INPUT_3\", \"INPUT_4\", \"INPUT_5\", \"INPUT_6\", \"INPUT_7\", \"INPUT_8\", \"INPUT_9\", \"TARGET_x\", \"TARGET_y\", \"TARGET_z\"])\n",
    "  y = data.drop([\"id\",\"INPUT_0\", \"INPUT_1\", \"INPUT_2\", \"INPUT_3\", \"INPUT_4\", \"INPUT_5\", \"INPUT_6\", \"INPUT_7\", \"INPUT_8\", \"INPUT_9\"], axis=1)\n",
    "  X = data.drop([\"id\",\"TARGET_x\", \"TARGET_y\", \"TARGET_z\"], axis=1).astype(float).to_numpy()\n",
    "\n",
    "  y = y.astype(float).to_numpy()\n",
    "  \n",
    "  if split == True:\n",
    "    # Train/internal test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n",
    "    return (np.concatenate((y_train, X_train), axis=1),np.concatenate((y_test, X_test), axis=1))\n",
    "  else:\n",
    "    return (np.concatenate((y, X), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train,dataset_internal_test = read_ds(TR_PATH,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.1704154e+00, -7.3649310e+01,  9.4287815e+00,  8.6179230e-02,\n",
       "       -9.8935485e-01,  9.1387004e-01,  5.2771106e-02,  9.8712380e-01,\n",
       "        9.9955016e-01,  8.5003810e-01, -9.1003710e-01, -7.2887754e-01,\n",
       "        4.4358963e-01])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "  def __init__(self, epochs_to_wait=1, min_delta=0):\n",
    "    self.min_training_loss = np.inf\n",
    "    self.epochs_to_wait = epochs_to_wait\n",
    "    self.min_delta = min_delta\n",
    "    self.counter = 0\n",
    "\n",
    "  def check_early_stop(self, training_loss):\n",
    "    if training_loss > (self.min_training_loss - self.min_delta):\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.epochs_to_wait:\n",
    "        return True\n",
    "    else: \n",
    "      self.counter = 0\n",
    "    if training_loss < self.min_training_loss:\n",
    "      self.min_training_loss = training_loss\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Padding(validation_losses_fold,train_losses_fold):\n",
    "    max_epochs = max(map(len, validation_losses_fold))\n",
    "\n",
    "    for validation_loss_arr in validation_losses_fold:\n",
    "        while len(validation_loss_arr) < max_epochs:\n",
    "            validation_loss_arr.append(validation_loss_arr[-1])\n",
    "\n",
    "    for train_loss_arr in train_losses_fold:\n",
    "      while len(train_loss_arr) < max_epochs:\n",
    "            train_loss_arr.append(train_loss_arr[-1])\n",
    "\n",
    "    print(len(validation_losses_fold))\n",
    "    return validation_losses_fold,train_losses_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mean(validation_avg_loss_fold,train_losses_fold,n_folds):\n",
    "    max_epochs = max(map(len, validation_avg_loss_fold))\n",
    "    validation_avg_loss = []\n",
    "    train_avg_loss = []\n",
    "    \n",
    "    for i in range(0,max_epochs):\n",
    "        temp_loss = 0\n",
    "        for j in range(0,len(validation_avg_loss_fold)):\n",
    "            temp_loss += validation_avg_loss_fold[j][i]\n",
    "        validation_avg_loss.append(temp_loss/n_folds)\n",
    "\n",
    "    for i in range(0,max_epochs):\n",
    "        temp_loss = 0\n",
    "        for j in range(0,len(train_losses_fold)):\n",
    "            temp_loss += train_losses_fold[j][i]\n",
    "        train_avg_loss.append(temp_loss/n_folds)\n",
    "    \n",
    "    return validation_avg_loss,train_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(input_size,layers_size,learning_rate,momentum,weight_decay,opt,epochs,trainloader,loss_function,testloader, activation_function):\n",
    "    # Init the neural network\n",
    "    network = Net(input_size, layers_size, activation_function)\n",
    "    network.apply(reset_weights) #reset weights with random initialization\n",
    "\n",
    "    optimizer = opt(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    train_MEE_folds = []\n",
    "    test_MEE_folds = []\n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "      # Print epoch\n",
    "      print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss and accuracy value for train\n",
    "      train_loss = 0.0\n",
    "      train_MEE = 0.0\n",
    "\n",
    "      # Set current loss and accuracy value for test\n",
    "      test_loss = 0.0\n",
    "      test_MEE = 0.0\n",
    "     \n",
    "\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[:, OUTPUT_SIZE:].to(torch.float32)\n",
    "        targets = data[:, :OUTPUT_SIZE].to(torch.float32)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        #caluclate MEE for this batch pow(2), sum(axis = 1), sqrt(), sum()\n",
    "        train_MEE += float((outputs-targets).pow(2).sum(1).sqrt().sum())\n",
    "\n",
    "      # Print loss values\n",
    "      epoch_train_loss = train_loss / len(trainloader.sampler)    \n",
    "      print(\"TRAIN ERROR MSE:\",epoch_train_loss)\n",
    "\n",
    "      train_MEE = train_MEE / len(trainloader.sampler) \n",
    "      train_MEE_folds.append(train_MEE)\n",
    "      print(\"TRAIN ERROR MEE:\",train_MEE_folds) \n",
    "      \n",
    "      with torch.no_grad():\n",
    "        # Iterate over the testing data and generate predictions\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "\n",
    "          inputs = data[:, OUTPUT_SIZE:].to(torch.float32)\n",
    "          targets = data[:, :OUTPUT_SIZE].to(torch.float32)\n",
    "        \n",
    "          outputs = network(inputs)\n",
    "\n",
    "          loss = loss_function(outputs, targets)\n",
    "          \n",
    "          test_loss += loss.item() * inputs.size(0)\n",
    "          test_MEE += float((outputs-targets).pow(2).sum(1).sqrt().sum())\n",
    "\n",
    "        epoch_test_loss = test_loss / len(testloader.sampler)    \n",
    "        print(\"TEST ERROR MSE:\",epoch_test_loss)\n",
    "     \n",
    "        test_MEE = test_MEE / len(testloader.sampler) \n",
    "        test_MEE_folds.append(test_MEE)\n",
    "        print(\"TEST ERROR MEE:\",test_MEE_folds)\n",
    "\n",
    "    plot_graph(train_MEE_folds,test_MEE_folds,epoch+1,\"test\")\n",
    "    return network,train_MEE_folds[-1],test_MEE_folds[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_model(input_size,layers_size,learning_rate,momentum,weight_decay,opt,epochs,trainloader,loss_function,validationloader,lr_decay,lr_step_size, activation_function):\n",
    "    \n",
    "    # Init the neural network \n",
    "    network = Net(input_size, layers_size, activation_function)\n",
    "    network.apply(reset_weights) #reset weights with random initialization\n",
    "    \n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    #scheduler = StepLR(optimizer, step_size=lr_step_size, gamma=lr_decay)\n",
    "\n",
    "    early_stopper = EarlyStopper(epochs_to_wait=30, min_delta=1e-5)\n",
    "\n",
    "    train_MEE_folds = []\n",
    "    validation_MEE_folds = []\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "      \n",
    "      # Set current loss value\n",
    "      train_loss = 0.0\n",
    "\n",
    "      #Set current MEE metric\n",
    "      train_MEE = 0.0\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[:, OUTPUT_SIZE:].to(torch.float32)\n",
    "        targets = data[:, :OUTPUT_SIZE].to(torch.float32)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        #caluclate MEE for this batch pow(2), sum(axis = 1), sqrt(), sum()\n",
    "        train_MEE += float((outputs-targets).pow(2).sum(1).sqrt().sum())\n",
    "\n",
    "      avg_train_loss = train_loss / len(trainloader.sampler)    \n",
    "\n",
    "      train_MEE = train_MEE / len(trainloader.sampler) \n",
    "      train_MEE_folds.append(train_MEE)\n",
    "      \n",
    "\n",
    "\n",
    "      # Evaluationfor this fold\n",
    "      valid_loss = 0.0 \n",
    "      val_MEE = 0.0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        # Iterate over the validation data and generate predictions\n",
    "        for i, data in enumerate(validationloader, 0):\n",
    "\n",
    "          # Get inputs\n",
    "          inputs = data[:, OUTPUT_SIZE:].to(torch.float32)\n",
    "          targets = data[:, :OUTPUT_SIZE].to(torch.float32)\n",
    "          \n",
    "          # Generate outputs\n",
    "          outputs = network(inputs)\n",
    "\n",
    "          loss = loss_function(outputs, targets)\n",
    "\n",
    "          # Calculate loss\n",
    "          valid_loss += loss.item() * inputs.size(0)\n",
    "          val_MEE += float((outputs-targets).pow(2).sum(1).sqrt().sum())\n",
    "\n",
    "        avg_valid_loss = valid_loss / len(validationloader.sampler) #used to find the best parameters of the model\n",
    "        val_MEE = val_MEE / len(validationloader.sampler)\n",
    "        validation_MEE_folds.append(val_MEE)\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopper.check_early_stop(avg_valid_loss):\n",
    "          print(\"Early stopping:\", epoch)\n",
    "          break\n",
    "\n",
    "\n",
    "    return avg_valid_loss,avg_train_loss,validation_MEE_folds,train_MEE_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_folds(kfold,dataset,batch_size,input_size, layers_size, learning_rate, epochs,\n",
    "    loss_function, momentum, opt, weight_decay,lr_decay,lr_step_size, activation_function):\n",
    "    \n",
    "    validation_avg_mee_fold = 0\n",
    "    train_avg_mee_fold = 0\n",
    "    validation_MEE_fold = []\n",
    "    train_MEE_fold = []\n",
    "    num_iterations = 0\n",
    "    current_config = {\n",
    "        'input_size': input_size,\n",
    "        'layers_size': layers_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'epochs': epochs,\n",
    "        'momentum': momentum,\n",
    "        'opt': opt,\n",
    "        'weight_decay': weight_decay,\n",
    "        \"batch_size\":batch_size,\n",
    "        \"loss_function\":loss_function,\n",
    "        \"lr_decay\":lr_decay,\n",
    "        \"lr_step_size\":lr_step_size,\n",
    "        \"activation_function\": activation_function,\n",
    "    }\n",
    "    print(current_config)\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(np.zeros(len(dataset)),dataset[:, 0])):\n",
    "        \n",
    "        \n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids, gen) \n",
    "        validation_subsampler = torch.utils.data.SubsetRandomSampler(val_ids, gen) \n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        \n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=batch_size, sampler=train_subsampler)\n",
    "        validationloader = torch.utils.data.DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size=batch_size, sampler=validation_subsampler)    \n",
    "        \n",
    "\n",
    "        validation_loss,train_loss,validation_MEE,train_MEE = k_fold_model(learning_rate=learning_rate,epochs=epochs,layers_size=layers_size,input_size=input_size,loss_function=loss_function,momentum=momentum\n",
    "                                                    ,opt=opt,trainloader=trainloader,weight_decay=weight_decay,validationloader=validationloader,lr_decay=lr_decay,lr_step_size=lr_step_size, activation_function=activation_function)   \n",
    "        validation_avg_mee_fold  += validation_loss\n",
    "        train_avg_mee_fold += train_loss\n",
    "        validation_MEE_fold.append(validation_MEE)\n",
    "        train_MEE_fold.append(train_MEE)\n",
    "\n",
    "        num_iterations += 1\n",
    "\n",
    "\n",
    "    #validation average over all folds\n",
    "    validation_avg_mee_fold /= num_iterations\n",
    "    train_avg_mee_fold /= num_iterations\n",
    "\n",
    "    \n",
    "    validation_MEE_fold,train_MEE_fold = Padding(validation_MEE_fold,train_MEE_fold)\n",
    "\n",
    "    validation_MEE_mean, train_MEE_mean = Mean(validation_MEE_fold, train_MEE_fold,n_folds=num_iterations)\n",
    "\n",
    "\n",
    "    plot_graph(train_MEE_mean,validation_MEE_mean,len(train_MEE_mean),\"validation\")\n",
    "\n",
    "\n",
    "    return (validation_avg_mee_fold,train_avg_mee_fold,current_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that executes the folds for each combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dogridsearch(dataset_train_part,params_grid,input_size,seeds):\n",
    "    dataset = dataset_train_part\n",
    "    # Set fixed random number seed\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = CV \n",
    "    \n",
    "    # K-fold Cross Validation model evaluation\n",
    "    best_params = None\n",
    "\n",
    "    actual_it = 0\n",
    "    total_iterations = len(params_grid[\"epochs\"]) * len(params_grid[\"optimizer\"]) * len(params_grid[\"layers_size\"]) \\\n",
    "          * len(params_grid[\"learning_rate\"]) * len(params_grid[\"batch_size\"]) * len(params_grid[\"weight_decay\"]) * len(params_grid[\"momentum\"]) * len(params_grid[\"lr_decay\"]) * len(params_grid[\"lr_step_size\"] * len(params_grid[\"activation_function\"]))\n",
    "\n",
    "\n",
    "    configurations = []\n",
    "\n",
    "    for epochs, opt, layers_size, learning_rate, batch_size, weight_decay, momentum,lr_decay,lr_step_size, activation_function in product(params_grid[\"epochs\"],params_grid[\"optimizer\"], \n",
    "                                                                                               params_grid[\"layers_size\"], params_grid[\"learning_rate\"], \n",
    "                                                                                               params_grid[\"batch_size\"], params_grid[\"weight_decay\"], \n",
    "                                                                                               params_grid[\"momentum\"],params_grid[\"lr_decay\"],params_grid[\"lr_step_size\"], params_grid[\"activation_function\"]):\n",
    "        #print the actual percentage of the grid search\n",
    "        print(f'Actual iter {(actual_it/total_iterations)*100}%')\n",
    "        for seed in seeds:\n",
    "            print(\"Working with seed:\",seed)\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "            (validation_avg_loss_fold,train_avg_loss_fold,current_config) = execute_folds(kfold=kfold,dataset=dataset,learning_rate=learning_rate,epochs=epochs,layers_size=layers_size,input_size=input_size,loss_function=loss_function,momentum=momentum\n",
    "                                                            ,opt=opt,weight_decay=weight_decay,batch_size=batch_size,lr_decay =lr_decay,lr_step_size=lr_step_size, activation_function=activation_function)\n",
    "\n",
    "\n",
    "            configurations.append((validation_avg_loss_fold, train_avg_loss_fold,current_config))\n",
    "        \n",
    "        actual_it += 1\n",
    "\n",
    "    val_mee = []\n",
    "    train_mee = []\n",
    "    #best \n",
    "    for conf_val in configurations:\n",
    "        val_mee.append(conf_val[0])\n",
    "        train_mee.append(conf_val[1])\n",
    "        if best_params is None or conf_val[0] < best_params[0]:\n",
    "                current_config = conf_val[2]\n",
    "                best_params = (conf_val[0],conf_val[1]\n",
    "                ,current_config['learning_rate'], current_config['epochs'],current_config[\"loss_function\"],current_config['layers_size'],current_config['momentum'],current_config['opt'],\n",
    "                current_config['weight_decay'],\n",
    "                current_config['batch_size'],current_config[\"lr_decay\"],current_config[\"lr_step_size\"], current_config[\"activation_function\"])\n",
    "    print(\"TRAIN MEAN MEE\",np.mean(train_mee))\n",
    "    print(\"TRAIN STD\",np.std(train_mee))\n",
    "    print(\"VALIDATION MEAN MEE\",np.mean(val_mee))\n",
    "    print(\"VALIDATION STD\",np.std(val_mee))\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the created model and plot training/test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(dataset_train_part,dataset_test_part,best_params,seeds):\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(range(len(dataset_train_part)), gen)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                        dataset_train_part, \n",
    "                        batch_size=best_params[9], sampler=train_subsampler)\n",
    "\n",
    "    test_subsampler =  torch.utils.data.SubsetRandomSampler(range(len(dataset_test_part)), gen)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                        dataset_test_part, \n",
    "                        batch_size=best_params[9], sampler=test_subsampler)\n",
    "    train_mee_seed = []\n",
    "    test_mee_seed = []\n",
    "    for seed in seeds:\n",
    "        # Start the timer\n",
    "        start = time.time()\n",
    "        print(\"Working with seed:\",seed)\n",
    "        torch.manual_seed(seed)\n",
    "        best_net,train_mee,test_mee = fit_model(learning_rate=best_params[2],epochs=best_params[3],layers_size=best_params[5],input_size=INPUT_SIZE,loss_function=best_params[4],\n",
    "                        momentum=best_params[6],opt=best_params[7],trainloader=trainloader,weight_decay=best_params[8],activation_function=best_params[12],testloader=testloader)   \n",
    "        train_mee_seed.append(train_mee)\n",
    "        test_mee_seed.append(test_mee)\n",
    "        end = time.time()\n",
    "        print(\"Refit Time: {:.2f} seconds\".format(end - start))\n",
    "    \n",
    "    print(\"TRAIN MEAN MEE\",np.mean(train_mee_seed))\n",
    "    print(\"TRAIN STD\",np.std(train_mee_seed))\n",
    "    print(\"TEST MEAN MEE\",np.mean(test_mee_seed))\n",
    "    print(\"TEST STD\",np.std(test_mee_seed))\n",
    "\n",
    "    return best_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"layers_size\": [[128, 64, 32, OUTPUT_SIZE]], #we tried all possibile combinations of 2 layers [2,4,8,16,32,64,128,256,512] and\n",
    "     # (due to computation limits) combinations of 3 layers decreasing, increasing order, and with the central layer bigger or smaller than the others\n",
    "    \"learning_rate\": [0.001], #0.0005, 0.001, 0.005, 0.01, 0.1, 0.5, 0.7, 0.8\n",
    "    \"batch_size\": [1000], #16,32,64,128,256,512, 1000 corresponds to batch method, no mini batch\n",
    "    \"weight_decay\": [0.1], #0.001, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5\n",
    "    \"momentum\": [ 0.5], #0.01, 0.1, 0.5, 0.7, 0.8\n",
    "    \"epochs\":[1200], \n",
    "    \"optimizer\":[torch.optim.SGD],\n",
    "    \"lr_decay\":[0], #0.1, 0.01, 0.5 we tried first with lr_decay and then we removed it\n",
    "    \"lr_step_size\":[0], #0,32,64\n",
    "    \"activation_function\":[\"relu\"] #relu, tanh\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = dataset_train\n",
    "\n",
    "best_params = dogridsearch(dataset_train_part=dataset,params_grid=params_grid,input_size=INPUT_SIZE,seeds = SEEDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters found\n",
    "print(f\"Best hidden size: {best_params[5]} \\nBest learning rate: {best_params[2]} \\nBest batch size: {best_params[9]} \\nBest weight decay: {best_params[8]} \\nBest momentum: {best_params[6]} \\nBest epochs: {best_params[3]} \\nBest optimizer: {best_params[7]} \\nBest lr_decay: {best_params[10]} \\nBest lr_step_size: {best_params[11]}\\nBest activation_function: {best_params[12]}\")\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = train_test_model(dataset_train_part=dataset_train,dataset_test_part=dataset_internal_test,best_params=best_params,seeds = SEEDS)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
