{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from monk_helpers import CV,SEED\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets Path\n",
    "TR_PATH = \"./monks/datasets/monks-3.train\"\n",
    "TS_PATH = \"./monks/datasets/monks-3.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_graph(train_losses,validation_losses,epochs):\n",
    "    num_epochs = list(range(1, epochs + 1))  \n",
    "    # Plotting\n",
    "    plt.plot(num_epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(num_epochs, validation_losses, label='Test Loss')\n",
    "\n",
    "    plt.title('Training and Validation Losses Across Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, units, output_size,):\n",
    "    super().__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.l1 = nn.Linear(input_size, units)\n",
    "    self.l2 = nn.Linear(units, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.tanh(self.l1(x))\n",
    "    out = torch.sigmoid(self.l2(out))\n",
    "    return out\n",
    "\n",
    "\n",
    "def reset_weights(net):\n",
    "  for param in net.parameters():\n",
    "    torch.nn.init.uniform_(param, a=-0.7, b=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ds(path):\n",
    "  \"\"\"\n",
    "  parse CSV data set and\n",
    "  returns a tuple (input, target)\n",
    "  \"\"\"\n",
    "  names = ['class', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'id']\n",
    "  data = pd.read_csv(path, dtype=object, delim_whitespace=True, header=None, skipinitialspace=True, names=names)\n",
    "\n",
    "  X = data.drop(['class','id'], axis=1)\n",
    "  X = pd.get_dummies(X).astype(float).to_numpy()\n",
    "  y = data.drop(['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'id'], axis=1)\n",
    "  y = y.astype(float).to_numpy()\n",
    "\n",
    "\n",
    "  return np.concatenate((y, X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToleranceStopper:\n",
    "  def __init__(self, patience=1, min_delta=0):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.counter = 0\n",
    "    self.min_training_loss = np.inf\n",
    "\n",
    "  def tol_stop(self, training_loss):\n",
    "    if training_loss > (self.min_training_loss - self.min_delta):\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.patience:\n",
    "        return True\n",
    "    else: \n",
    "      self.counter = 0\n",
    "    if training_loss < self.min_training_loss:\n",
    "      self.min_training_loss = training_loss\n",
    "          \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(input_size,hidden_size,output_size,learning_rate,momentum,weight_decay,opt,epochs,trainloader,loss_function,testloader):\n",
    "    # Init the neural network\n",
    "    network = Net(input_size, hidden_size, output_size)\n",
    "    network.apply(reset_weights) #reset weights with random initialization\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    if (opt.__name__ == \"RMSprop\") or (opt.__name__ == \"SGD\"):\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "      # Print epoch\n",
    "      print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss and accuracy value for train\n",
    "      train_loss = 0.0\n",
    "      epoch_train_accuracy = []\n",
    "\n",
    "\n",
    "      # Set current loss and accuracy value for test\n",
    "      test_loss = 0.0\n",
    "      epoch_test_accuracy = []\n",
    "\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[:, 1:].to(torch.float32)\n",
    "        targets = data[:, [0]].to(torch.float32)\n",
    "\n",
    "        # Early stopping\n",
    "        tolerance_stopper = ToleranceStopper(patience=10, min_delta=1e-4)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "      # Print loss values\n",
    "      epoch_train_loss = train_loss / len(trainloader.sampler.indices)    \n",
    "      print(f'Training loss: {epoch_train_loss}')\n",
    "      train_losses.append(epoch_train_loss)\n",
    "      # Update accuracy\n",
    "      for output, target in zip(outputs, targets):\n",
    "        output = 0 if output.item() < 0.5 else 1\n",
    "        if output == target.item():\n",
    "          epoch_train_accuracy.append(1)\n",
    "        else:\n",
    "          epoch_train_accuracy.append(0)\n",
    "      \n",
    "      with torch.no_grad():\n",
    "        # Iterate over the testing data and generate predictions\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "\n",
    "          inputs = data[:, 1:].to(torch.float32)\n",
    "          targets = data[:, [0]].to(torch.float32)\n",
    "        \n",
    "          outputs = network(inputs)\n",
    "\n",
    "          loss = loss_function(outputs, targets)\n",
    "          \n",
    "          test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_test_loss = test_loss / len(testloader.sampler.indices)    \n",
    "        print(f'Test loss: {epoch_test_loss}')\n",
    "        test_losses.append(epoch_test_loss)\n",
    "\n",
    "        # Update accuracy\n",
    "        for output, target in zip(outputs, targets):\n",
    "          output = 0 if output.item() < 0.5 else 1\n",
    "          if output == target.item():\n",
    "            epoch_test_accuracy.append(1)\n",
    "          else:\n",
    "            epoch_test_accuracy.append(0)\n",
    "        \n",
    "        if tolerance_stopper.tol_stop(epoch_test_loss):\n",
    "          break\n",
    "\n",
    "    plot_graph(train_losses,test_losses,epochs)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_model(input_size,hidden_size,output_size,learning_rate,momentum,weight_decay,opt,epochs,trainloader,loss_function,validationloader):\n",
    "    \n",
    "    # Init the neural network\n",
    "    network = Net(input_size, hidden_size, output_size)\n",
    "    network.apply(reset_weights) #reset weights with random initialization\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    if (opt.__name__ == \"RMSprop\") or (opt.__name__ == \"SGD\"):\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    \n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "      # Print epoch\n",
    "      #print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss value\n",
    "      train_loss = 0.0\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[:, 1:].to(torch.float32)\n",
    "        targets = data[:, [0]].to(torch.float32)\n",
    "\n",
    "        # Early stopping\n",
    "        tolerance_stopper = ToleranceStopper(patience=10, min_delta=1e-4)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        #print(\"loss per item\", loss.item())\n",
    "        #print(\"inputs size\",inputs.size(0))\n",
    "\n",
    "        # Print loss values\n",
    "      #print(\"train loaders length\",len(trainloader.sampler.indices))\n",
    "      avg_train_loss = train_loss / len(trainloader.sampler.indices)    \n",
    "      #print(f'Training loss: {avg_train_loss}')\n",
    "      # Print about testing\n",
    "      #print('Starting validation')\n",
    "\n",
    "      # Evaluationfor this fold\n",
    "      valid_loss = 0.0 \n",
    "      with torch.no_grad():\n",
    "        # Iterate over the validation data and generate predictions\n",
    "        for i, data in enumerate(validationloader, 0):\n",
    "\n",
    "          # Get inputs\n",
    "          inputs = data[:, 1:].to(torch.float32)\n",
    "          targets = data[:, [0]].to(torch.float32)\n",
    "          \n",
    "          # Generate outputs\n",
    "          outputs = network(inputs)\n",
    "\n",
    "          loss = loss_function(outputs, targets)\n",
    "\n",
    "          # Calculate loss\n",
    "          valid_loss += loss.item() * inputs.size(0)\n",
    "          #print(\"loss per item\", loss.item())\n",
    "          #print(\"inputs size\",inputs.size(0))\n",
    "        \n",
    "        #print(\"validation loaders length\",len(validationloader.sampler.indices))\n",
    "        avg_valid_loss = valid_loss / len(validationloader.sampler.indices) #used to find the best parameters of the model\n",
    "        # Early stopping\n",
    "        if tolerance_stopper.tol_stop(avg_valid_loss):\n",
    "          break\n",
    "        # Print validation results\n",
    "        #print(f'Validation loss: {avg_valid_loss:.4f}')\n",
    "        \n",
    "\n",
    "    return avg_valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "Actual iter 0.0%\n",
      "Actual iter 0.26041666666666663%\n",
      "Actual iter 0.5208333333333333%\n",
      "Actual iter 0.78125%\n",
      "Actual iter 1.0416666666666665%\n",
      "Actual iter 1.3020833333333335%\n",
      "Actual iter 1.5625%\n",
      "Actual iter 1.8229166666666667%\n",
      "Actual iter 2.083333333333333%\n",
      "Actual iter 2.34375%\n",
      "Actual iter 2.604166666666667%\n",
      "Actual iter 2.864583333333333%\n",
      "Actual iter 3.125%\n",
      "Actual iter 3.3854166666666665%\n",
      "Actual iter 3.6458333333333335%\n",
      "Actual iter 3.90625%\n",
      "Actual iter 4.166666666666666%\n",
      "Actual iter 4.427083333333334%\n",
      "Actual iter 4.6875%\n",
      "Actual iter 4.947916666666666%\n",
      "Actual iter 5.208333333333334%\n",
      "Actual iter 5.46875%\n",
      "Actual iter 5.729166666666666%\n",
      "Actual iter 5.989583333333334%\n",
      "Actual iter 6.25%\n",
      "Actual iter 6.510416666666667%\n",
      "Actual iter 6.770833333333333%\n",
      "Actual iter 7.03125%\n",
      "Actual iter 7.291666666666667%\n",
      "Actual iter 7.552083333333333%\n",
      "Actual iter 7.8125%\n",
      "Actual iter 8.072916666666668%\n",
      "Actual iter 8.333333333333332%\n",
      "Actual iter 8.59375%\n",
      "Actual iter 8.854166666666668%\n",
      "Actual iter 9.114583333333332%\n",
      "Actual iter 9.375%\n",
      "Actual iter 9.635416666666668%\n",
      "Actual iter 9.895833333333332%\n",
      "Actual iter 10.15625%\n",
      "Actual iter 10.416666666666668%\n",
      "Actual iter 10.677083333333332%\n",
      "Actual iter 10.9375%\n",
      "Actual iter 11.197916666666668%\n",
      "Actual iter 11.458333333333332%\n",
      "Actual iter 11.71875%\n",
      "Actual iter 11.979166666666668%\n",
      "Actual iter 12.239583333333332%\n",
      "Actual iter 12.5%\n",
      "Actual iter 12.760416666666666%\n",
      "Actual iter 13.020833333333334%\n",
      "Actual iter 13.28125%\n",
      "Actual iter 13.541666666666666%\n",
      "Actual iter 13.802083333333334%\n",
      "Actual iter 14.0625%\n",
      "Actual iter 14.322916666666666%\n",
      "Actual iter 14.583333333333334%\n",
      "Actual iter 14.84375%\n",
      "Actual iter 15.104166666666666%\n",
      "Actual iter 15.364583333333334%\n",
      "Actual iter 15.625%\n",
      "Actual iter 15.885416666666666%\n",
      "Actual iter 16.145833333333336%\n",
      "Actual iter 16.40625%\n",
      "Actual iter 16.666666666666664%\n",
      "Actual iter 16.927083333333336%\n",
      "Actual iter 17.1875%\n",
      "Actual iter 17.447916666666664%\n",
      "Actual iter 17.708333333333336%\n",
      "Actual iter 17.96875%\n",
      "Actual iter 18.229166666666664%\n",
      "Actual iter 18.489583333333336%\n",
      "Actual iter 18.75%\n",
      "Actual iter 19.010416666666664%\n",
      "Actual iter 19.270833333333336%\n",
      "Actual iter 19.53125%\n",
      "Actual iter 19.791666666666664%\n",
      "Actual iter 20.052083333333336%\n",
      "Actual iter 20.3125%\n",
      "Actual iter 20.572916666666664%\n",
      "Actual iter 20.833333333333336%\n",
      "Actual iter 21.09375%\n",
      "Actual iter 21.354166666666664%\n",
      "Actual iter 21.614583333333336%\n",
      "Actual iter 21.875%\n",
      "Actual iter 22.135416666666664%\n",
      "Actual iter 22.395833333333336%\n",
      "Actual iter 22.65625%\n",
      "Actual iter 22.916666666666664%\n",
      "Actual iter 23.177083333333336%\n",
      "Actual iter 23.4375%\n",
      "Actual iter 23.697916666666664%\n",
      "Actual iter 23.958333333333336%\n",
      "Actual iter 24.21875%\n",
      "Actual iter 24.479166666666664%\n",
      "Actual iter 24.739583333333336%\n",
      "Actual iter 25.0%\n",
      "Actual iter 25.260416666666668%\n",
      "Actual iter 25.520833333333332%\n",
      "Actual iter 25.78125%\n",
      "Actual iter 26.041666666666668%\n",
      "Actual iter 26.302083333333332%\n",
      "Actual iter 26.5625%\n",
      "Actual iter 26.822916666666668%\n",
      "Actual iter 27.083333333333332%\n",
      "Actual iter 27.34375%\n",
      "Actual iter 27.604166666666668%\n",
      "Actual iter 27.864583333333332%\n",
      "Actual iter 28.125%\n",
      "Actual iter 28.385416666666668%\n",
      "Actual iter 28.645833333333332%\n",
      "Actual iter 28.90625%\n",
      "Actual iter 29.166666666666668%\n",
      "Actual iter 29.427083333333332%\n",
      "Actual iter 29.6875%\n",
      "Actual iter 29.947916666666668%\n",
      "Actual iter 30.208333333333332%\n",
      "Actual iter 30.46875%\n",
      "Actual iter 30.729166666666668%\n",
      "Actual iter 30.989583333333332%\n",
      "Actual iter 31.25%\n",
      "Actual iter 31.510416666666668%\n",
      "Actual iter 31.770833333333332%\n",
      "Actual iter 32.03125%\n",
      "Actual iter 32.29166666666667%\n",
      "Actual iter 32.55208333333333%\n",
      "Actual iter 32.8125%\n",
      "Actual iter 33.07291666666667%\n",
      "Actual iter 33.33333333333333%\n",
      "Actual iter 33.59375%\n",
      "Actual iter 33.85416666666667%\n",
      "Actual iter 34.11458333333333%\n",
      "Actual iter 34.375%\n",
      "Actual iter 34.63541666666667%\n",
      "Actual iter 34.89583333333333%\n",
      "Actual iter 35.15625%\n",
      "Actual iter 35.41666666666667%\n",
      "Actual iter 35.67708333333333%\n",
      "Actual iter 35.9375%\n",
      "Actual iter 36.19791666666667%\n",
      "Actual iter 36.45833333333333%\n",
      "Actual iter 36.71875%\n",
      "Actual iter 36.97916666666667%\n",
      "Actual iter 37.23958333333333%\n",
      "Actual iter 37.5%\n",
      "Actual iter 37.76041666666667%\n",
      "Actual iter 38.02083333333333%\n",
      "Actual iter 38.28125%\n",
      "Actual iter 38.54166666666667%\n",
      "Actual iter 38.80208333333333%\n",
      "Actual iter 39.0625%\n",
      "Actual iter 39.32291666666667%\n",
      "Actual iter 39.58333333333333%\n",
      "Actual iter 39.84375%\n",
      "Actual iter 40.10416666666667%\n",
      "Actual iter 40.36458333333333%\n",
      "Actual iter 40.625%\n",
      "Actual iter 40.88541666666667%\n",
      "Actual iter 41.14583333333333%\n",
      "Actual iter 41.40625%\n",
      "Actual iter 41.66666666666667%\n",
      "Actual iter 41.92708333333333%\n",
      "Actual iter 42.1875%\n",
      "Actual iter 42.44791666666667%\n",
      "Actual iter 42.70833333333333%\n",
      "Actual iter 42.96875%\n",
      "Actual iter 43.22916666666667%\n",
      "Actual iter 43.48958333333333%\n",
      "Actual iter 43.75%\n",
      "Actual iter 44.01041666666667%\n",
      "Actual iter 44.27083333333333%\n",
      "Actual iter 44.53125%\n",
      "Actual iter 44.79166666666667%\n",
      "Actual iter 45.05208333333333%\n",
      "Actual iter 45.3125%\n",
      "Actual iter 45.57291666666667%\n",
      "Actual iter 45.83333333333333%\n",
      "Actual iter 46.09375%\n",
      "Actual iter 46.35416666666667%\n",
      "Actual iter 46.61458333333333%\n",
      "Actual iter 46.875%\n",
      "Actual iter 47.13541666666667%\n",
      "Actual iter 47.39583333333333%\n",
      "Actual iter 47.65625%\n",
      "Actual iter 47.91666666666667%\n",
      "Actual iter 48.17708333333333%\n",
      "Actual iter 48.4375%\n",
      "Actual iter 48.69791666666667%\n",
      "Actual iter 48.95833333333333%\n",
      "Actual iter 49.21875%\n",
      "Actual iter 49.47916666666667%\n",
      "Actual iter 49.73958333333333%\n",
      "Actual iter 50.0%\n",
      "Actual iter 50.260416666666664%\n",
      "Actual iter 50.520833333333336%\n",
      "Actual iter 50.78125%\n",
      "Actual iter 51.041666666666664%\n",
      "Actual iter 51.302083333333336%\n",
      "Actual iter 51.5625%\n",
      "Actual iter 51.822916666666664%\n",
      "Actual iter 52.083333333333336%\n",
      "Actual iter 52.34375%\n",
      "Actual iter 52.604166666666664%\n",
      "Actual iter 52.864583333333336%\n",
      "Actual iter 53.125%\n",
      "Actual iter 53.385416666666664%\n",
      "Actual iter 53.645833333333336%\n",
      "Actual iter 53.90625%\n",
      "Actual iter 54.166666666666664%\n",
      "Actual iter 54.427083333333336%\n",
      "Actual iter 54.6875%\n",
      "Actual iter 54.947916666666664%\n",
      "Actual iter 55.208333333333336%\n",
      "Actual iter 55.46875%\n",
      "Actual iter 55.729166666666664%\n",
      "Actual iter 55.989583333333336%\n",
      "Actual iter 56.25%\n",
      "Actual iter 56.510416666666664%\n",
      "Actual iter 56.770833333333336%\n",
      "Actual iter 57.03125%\n",
      "Actual iter 57.291666666666664%\n",
      "Actual iter 57.552083333333336%\n",
      "Actual iter 57.8125%\n",
      "Actual iter 58.072916666666664%\n",
      "Actual iter 58.333333333333336%\n",
      "Actual iter 58.59375%\n",
      "Actual iter 58.854166666666664%\n",
      "Actual iter 59.114583333333336%\n",
      "Actual iter 59.375%\n",
      "Actual iter 59.635416666666664%\n",
      "Actual iter 59.895833333333336%\n",
      "Actual iter 60.15625%\n",
      "Actual iter 60.416666666666664%\n",
      "Actual iter 60.677083333333336%\n",
      "Actual iter 60.9375%\n",
      "Actual iter 61.197916666666664%\n",
      "Actual iter 61.458333333333336%\n",
      "Actual iter 61.71875%\n",
      "Actual iter 61.979166666666664%\n",
      "Actual iter 62.239583333333336%\n",
      "Actual iter 62.5%\n",
      "Actual iter 62.760416666666664%\n",
      "Actual iter 63.020833333333336%\n",
      "Actual iter 63.28125%\n",
      "Actual iter 63.541666666666664%\n",
      "Actual iter 63.802083333333336%\n",
      "Actual iter 64.0625%\n",
      "Actual iter 64.32291666666666%\n",
      "Actual iter 64.58333333333334%\n",
      "Actual iter 64.84375%\n",
      "Actual iter 65.10416666666666%\n",
      "Actual iter 65.36458333333334%\n",
      "Actual iter 65.625%\n",
      "Actual iter 65.88541666666666%\n",
      "Actual iter 66.14583333333334%\n",
      "Actual iter 66.40625%\n",
      "Actual iter 66.66666666666666%\n",
      "Actual iter 66.92708333333334%\n",
      "Actual iter 67.1875%\n",
      "Actual iter 67.44791666666666%\n",
      "Actual iter 67.70833333333334%\n",
      "Actual iter 67.96875%\n",
      "Actual iter 68.22916666666666%\n",
      "Actual iter 68.48958333333334%\n",
      "Actual iter 68.75%\n",
      "Actual iter 69.01041666666666%\n",
      "Actual iter 69.27083333333334%\n",
      "Actual iter 69.53125%\n",
      "Actual iter 69.79166666666666%\n",
      "Actual iter 70.05208333333334%\n",
      "Actual iter 70.3125%\n",
      "Actual iter 70.57291666666666%\n",
      "Actual iter 70.83333333333334%\n",
      "Actual iter 71.09375%\n",
      "Actual iter 71.35416666666666%\n",
      "Actual iter 71.61458333333334%\n",
      "Actual iter 71.875%\n",
      "Actual iter 72.13541666666666%\n",
      "Actual iter 72.39583333333334%\n",
      "Actual iter 72.65625%\n",
      "Actual iter 72.91666666666666%\n",
      "Actual iter 73.17708333333334%\n",
      "Actual iter 73.4375%\n",
      "Actual iter 73.69791666666666%\n",
      "Actual iter 73.95833333333334%\n",
      "Actual iter 74.21875%\n",
      "Actual iter 74.47916666666666%\n",
      "Actual iter 74.73958333333334%\n",
      "Actual iter 75.0%\n",
      "Actual iter 75.26041666666666%\n",
      "Actual iter 75.52083333333334%\n",
      "Actual iter 75.78125%\n",
      "Actual iter 76.04166666666666%\n",
      "Actual iter 76.30208333333334%\n",
      "Actual iter 76.5625%\n",
      "Actual iter 76.82291666666666%\n",
      "Actual iter 77.08333333333334%\n",
      "Actual iter 77.34375%\n",
      "Actual iter 77.60416666666666%\n",
      "Actual iter 77.86458333333334%\n",
      "Actual iter 78.125%\n",
      "Actual iter 78.38541666666666%\n",
      "Actual iter 78.64583333333334%\n",
      "Actual iter 78.90625%\n",
      "Actual iter 79.16666666666666%\n",
      "Actual iter 79.42708333333334%\n",
      "Actual iter 79.6875%\n",
      "Actual iter 79.94791666666666%\n",
      "Actual iter 80.20833333333334%\n",
      "Actual iter 80.46875%\n",
      "Actual iter 80.72916666666666%\n",
      "Actual iter 80.98958333333334%\n",
      "Actual iter 81.25%\n",
      "Actual iter 81.51041666666666%\n",
      "Actual iter 81.77083333333334%\n",
      "Actual iter 82.03125%\n",
      "Actual iter 82.29166666666666%\n",
      "Actual iter 82.55208333333334%\n",
      "Actual iter 82.8125%\n",
      "Actual iter 83.07291666666666%\n",
      "Actual iter 83.33333333333334%\n",
      "Actual iter 83.59375%\n",
      "Actual iter 83.85416666666666%\n",
      "Actual iter 84.11458333333334%\n",
      "Actual iter 84.375%\n",
      "Actual iter 84.63541666666666%\n",
      "Actual iter 84.89583333333334%\n",
      "Actual iter 85.15625%\n",
      "Actual iter 85.41666666666666%\n",
      "Actual iter 85.67708333333334%\n",
      "Actual iter 85.9375%\n",
      "Actual iter 86.19791666666666%\n",
      "Actual iter 86.45833333333334%\n",
      "Actual iter 86.71875%\n",
      "Actual iter 86.97916666666666%\n",
      "Actual iter 87.23958333333334%\n",
      "Actual iter 87.5%\n",
      "Actual iter 87.76041666666666%\n",
      "Actual iter 88.02083333333334%\n",
      "Actual iter 88.28125%\n",
      "Actual iter 88.54166666666666%\n",
      "Actual iter 88.80208333333334%\n",
      "Actual iter 89.0625%\n",
      "Actual iter 89.32291666666666%\n",
      "Actual iter 89.58333333333334%\n",
      "Actual iter 89.84375%\n",
      "Actual iter 90.10416666666666%\n",
      "Actual iter 90.36458333333334%\n",
      "Actual iter 90.625%\n",
      "Actual iter 90.88541666666666%\n",
      "Actual iter 91.14583333333334%\n",
      "Actual iter 91.40625%\n",
      "Actual iter 91.66666666666666%\n",
      "Actual iter 91.92708333333334%\n",
      "Actual iter 92.1875%\n",
      "Actual iter 92.44791666666666%\n",
      "Actual iter 92.70833333333334%\n",
      "Actual iter 92.96875%\n",
      "Actual iter 93.22916666666666%\n",
      "Actual iter 93.48958333333334%\n",
      "Actual iter 93.75%\n",
      "Actual iter 94.01041666666666%\n",
      "Actual iter 94.27083333333334%\n",
      "Actual iter 94.53125%\n",
      "Actual iter 94.79166666666666%\n",
      "Actual iter 95.05208333333334%\n",
      "Actual iter 95.3125%\n",
      "Actual iter 95.57291666666666%\n",
      "Actual iter 95.83333333333334%\n",
      "Actual iter 96.09375%\n",
      "Actual iter 96.35416666666666%\n",
      "Actual iter 96.61458333333334%\n",
      "Actual iter 96.875%\n",
      "Actual iter 97.13541666666666%\n",
      "Actual iter 97.39583333333334%\n",
      "Actual iter 97.65625%\n",
      "Actual iter 97.91666666666666%\n",
      "Actual iter 98.17708333333334%\n",
      "Actual iter 98.4375%\n",
      "Actual iter 98.69791666666666%\n",
      "Actual iter 98.95833333333334%\n",
      "Actual iter 99.21875%\n",
      "Actual iter 99.47916666666666%\n",
      "Actual iter 99.73958333333334%\n",
      "Best hidden size: 5 \n",
      "Best learning rate: 0.5 \n",
      "Best batch size: 64 \n",
      "Best weight decay: 0.001 \n",
      "Best momentum: 0.4\n"
     ]
    }
   ],
   "source": [
    "input_size = 17  \n",
    "output_size = 1\n",
    "params_grid = {\n",
    "    \"hidden_size\": [2, 3, 4, 5],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 0.5],\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"weight_decay\": [0.0001, 0.001, 0.01],\n",
    "    \"momentum\": [0.01, 0.05, 0.1, 0.4],\n",
    "    \"epochs\":[5000],\n",
    "    \"optimizer\":[torch.optim.SGD]\n",
    "}\n",
    "\n",
    "params_grid_o = {\n",
    "    \"hidden_size\": [4],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"batch_size\": [64],\n",
    "    \"weight_decay\": [0.01],\n",
    "    \"momentum\": [0.09],\n",
    "    \"epochs\":[400, 600],\n",
    "    \"optimizer\":[torch.optim.SGD]\n",
    "}\n",
    "\n",
    "\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "# Set fixed random number seed\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "dataset_train_part = read_ds(TR_PATH)\n",
    "dataset_test_part = read_ds(TS_PATH)\n",
    "\n",
    "dataset = dataset_train_part\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = CV \n",
    "  \n",
    "# K-fold Cross Validation model evaluation\n",
    "best_params = None\n",
    "\n",
    "actual_it = 0\n",
    "total_iterations = len(params_grid[\"epochs\"]) * len(params_grid[\"optimizer\"]) * len(params_grid[\"hidden_size\"]) * len(params_grid[\"learning_rate\"]) * len(params_grid[\"batch_size\"]) * len(params_grid[\"weight_decay\"]) * len(params_grid[\"momentum\"])\n",
    "print(total_iterations)\n",
    "\n",
    "for epochs, opt, hidden_size, learning_rate, batch_size, weight_decay, momentum in product(params_grid[\"epochs\"],params_grid[\"optimizer\"], params_grid[\"hidden_size\"], params_grid[\"learning_rate\"], params_grid[\"batch_size\"], params_grid[\"weight_decay\"], params_grid[\"momentum\"]):\n",
    "    validation_avg_loss_fold = 0\n",
    "    num_iterations = 0\n",
    "    #print the actual percentage of the grid search\n",
    "    print(f'Actual iter {(actual_it/total_iterations)*100}%')\n",
    "    \n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(np.zeros(len(dataset)),dataset[:, 0])):\n",
    "\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids, gen) \n",
    "        validation_subsampler = torch.utils.data.SubsetRandomSampler(val_ids, gen) \n",
    "        # Print\n",
    "        #print(f'FOLD {fold}')\n",
    "\n",
    "        #print('--------------------------------')\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        \n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=batch_size, sampler=train_subsampler)\n",
    "        validationloader = torch.utils.data.DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size=batch_size, sampler=validation_subsampler)    \n",
    "        \n",
    "\n",
    "        validation_loss = k_fold_model(learning_rate=learning_rate,epochs=epochs,hidden_size=hidden_size,input_size=input_size,loss_function=loss_function,momentum=momentum\n",
    "                                                    ,opt=opt,output_size=output_size,trainloader=trainloader,weight_decay=weight_decay,validationloader=validationloader)   \n",
    "        validation_avg_loss_fold  += validation_loss\n",
    "        num_iterations += 1\n",
    "\n",
    "    actual_it = actual_it + 1\n",
    "\n",
    "    #validation average over all folds\n",
    "    validation_avg_loss_fold /= num_iterations\n",
    "\n",
    "    #best \n",
    "    if best_params is None or validation_avg_loss_fold < best_params[0]:\n",
    "        best_params = (validation_avg_loss_fold,learning_rate,epochs,hidden_size, loss_function,momentum,opt,weight_decay,batch_size)\n",
    "\n",
    "\n",
    "print(f\"Best hidden size: {best_params[3]} \\nBest learning rate: {best_params[1]} \\nBest batch size: {best_params[8]} \\nBest weight decay: {best_params[7]} \\nBest momentum: {best_params[5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hidden size: 5 \n",
      "Best learning rate: 0.5 \n",
      "Best batch size: 64 \n",
      "Best weight decay: 0.001 \n",
      "Best momentum: 0.4\n"
     ]
    }
   ],
   "source": [
    "#Best parameters found\n",
    "print(f\"Best hidden size: {best_params[3]} \\nBest learning rate: {best_params[1]} \\nBest batch size: {best_params[8]} \\nBest weight decay: {best_params[7]} \\nBest momentum: {best_params[5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Training loss: 0.24275894756199884\n",
      "Test loss: 0.24317353632715014\n",
      "Starting epoch 2\n",
      "Training loss: 0.2255203828948443\n",
      "Test loss: 0.22662917717739386\n",
      "Starting epoch 3\n",
      "Training loss: 0.21396625164102334\n",
      "Test loss: 0.21605952304822426\n",
      "Starting epoch 4\n",
      "Training loss: 0.20544431981493216\n",
      "Test loss: 0.20709747241603005\n",
      "Starting epoch 5\n",
      "Training loss: 0.19715424199573328\n",
      "Test loss: 0.19842387515085716\n",
      "Starting epoch 6\n",
      "Training loss: 0.18840809996987953\n",
      "Test loss: 0.1891381326648924\n",
      "Starting epoch 7\n",
      "Training loss: 0.17894944963885134\n",
      "Test loss: 0.17985071297045108\n",
      "Starting epoch 8\n",
      "Training loss: 0.16968134534163554\n",
      "Test loss: 0.16983197629451752\n",
      "Starting epoch 9\n",
      "Training loss: 0.15908207756574036\n",
      "Test loss: 0.1592238837922061\n",
      "Starting epoch 10\n",
      "Training loss: 0.1500666363806021\n",
      "Test loss: 0.14846778964554821\n",
      "Starting epoch 11\n",
      "Training loss: 0.14123925933095274\n",
      "Test loss: 0.13842996127075619\n",
      "Starting epoch 12\n",
      "Training loss: 0.12969966477057973\n",
      "Test loss: 0.12736014469906135\n",
      "Starting epoch 13\n",
      "Training loss: 0.12188766127238508\n",
      "Test loss: 0.11816955468168965\n",
      "Starting epoch 14\n",
      "Training loss: 0.1129029035323956\n",
      "Test loss: 0.10963175749337231\n",
      "Starting epoch 15\n",
      "Training loss: 0.10519909150287753\n",
      "Test loss: 0.10040778445976752\n",
      "Starting epoch 16\n",
      "Training loss: 0.09921114500917372\n",
      "Test loss: 0.09282027157368483\n",
      "Starting epoch 17\n",
      "Training loss: 0.09301940046372961\n",
      "Test loss: 0.08699978501708419\n",
      "Starting epoch 18\n",
      "Training loss: 0.08914015942909678\n",
      "Test loss: 0.08102317264786472\n",
      "Starting epoch 19\n",
      "Training loss: 0.08407597781204787\n",
      "Test loss: 0.07512278578899524\n",
      "Starting epoch 20\n",
      "Training loss: 0.08101451616795337\n",
      "Test loss: 0.07133879209006275\n",
      "Starting epoch 21\n",
      "Training loss: 0.07899905741214752\n",
      "Test loss: 0.06734026830505442\n",
      "Starting epoch 22\n",
      "Training loss: 0.07573518181433443\n",
      "Test loss: 0.06564883487644019\n",
      "Starting epoch 23\n",
      "Training loss: 0.07354657669536403\n",
      "Test loss: 0.06215757324739739\n",
      "Starting epoch 24\n",
      "Training loss: 0.07161766681514803\n",
      "Test loss: 0.06055570586963936\n",
      "Starting epoch 25\n",
      "Training loss: 0.07005535420335707\n",
      "Test loss: 0.05941241206946196\n",
      "Starting epoch 26\n",
      "Training loss: 0.06841155367552257\n",
      "Test loss: 0.05826803117438599\n",
      "Starting epoch 27\n",
      "Training loss: 0.06699825701166372\n",
      "Test loss: 0.0567124266591337\n",
      "Starting epoch 28\n",
      "Training loss: 0.06613742010515244\n",
      "Test loss: 0.0545764608239686\n",
      "Starting epoch 29\n",
      "Training loss: 0.06521801467313142\n",
      "Test loss: 0.05398892197344038\n",
      "Starting epoch 30\n",
      "Training loss: 0.06382752319828408\n",
      "Test loss: 0.0522785700029797\n",
      "Starting epoch 31\n",
      "Training loss: 0.06319151476758425\n",
      "Test loss: 0.051144403508967824\n",
      "Starting epoch 32\n",
      "Training loss: 0.062087167481907075\n",
      "Test loss: 0.05142393418484264\n",
      "Starting epoch 33\n",
      "Training loss: 0.06158448938952118\n",
      "Test loss: 0.05099960595921234\n",
      "Starting epoch 34\n",
      "Training loss: 0.060907510582540854\n",
      "Test loss: 0.05096021984462385\n",
      "Starting epoch 35\n",
      "Training loss: 0.06007208662932036\n",
      "Test loss: 0.04986185856439449\n",
      "Starting epoch 36\n",
      "Training loss: 0.0594770463152987\n",
      "Test loss: 0.049146985842121974\n",
      "Starting epoch 37\n",
      "Training loss: 0.05903435541225261\n",
      "Test loss: 0.04883495249130108\n",
      "Starting epoch 38\n",
      "Training loss: 0.059289637036987995\n",
      "Test loss: 0.047986669258938894\n",
      "Starting epoch 39\n",
      "Training loss: 0.057947967628963655\n",
      "Test loss: 0.04932161530962697\n",
      "Starting epoch 40\n",
      "Training loss: 0.05740056101415978\n",
      "Test loss: 0.04872551656983517\n",
      "Starting epoch 41\n",
      "Training loss: 0.05713398168321516\n",
      "Test loss: 0.04802894744056242\n",
      "Starting epoch 42\n",
      "Training loss: 0.056576367650852825\n",
      "Test loss: 0.04793440391895948\n",
      "Starting epoch 43\n",
      "Training loss: 0.056210911115173436\n",
      "Test loss: 0.04759714076364482\n",
      "Starting epoch 44\n",
      "Training loss: 0.05583183675027285\n",
      "Test loss: 0.04748158060290195\n",
      "Starting epoch 45\n",
      "Training loss: 0.05622755039910801\n",
      "Test loss: 0.04734541441279429\n",
      "Starting epoch 46\n",
      "Training loss: 0.05501524780373104\n",
      "Test loss: 0.04621038682482861\n",
      "Starting epoch 47\n",
      "Training loss: 0.05492784534810019\n",
      "Test loss: 0.046734960542784795\n",
      "Starting epoch 48\n",
      "Training loss: 0.05509639830618608\n",
      "Test loss: 0.04761768587761455\n",
      "Starting epoch 49\n",
      "Training loss: 0.0541210443270011\n",
      "Test loss: 0.04624961040638111\n",
      "Starting epoch 50\n",
      "Training loss: 0.05429828118105404\n",
      "Test loss: 0.04569198329139639\n",
      "Starting epoch 51\n",
      "Training loss: 0.05352166209553109\n",
      "Test loss: 0.04705339090691672\n",
      "Starting epoch 52\n",
      "Training loss: 0.053193773158261036\n",
      "Test loss: 0.0468127410169001\n",
      "Starting epoch 53\n",
      "Training loss: 0.0530655992324235\n",
      "Test loss: 0.04641967253000648\n",
      "Starting epoch 54\n",
      "Training loss: 0.05281502973349368\n",
      "Test loss: 0.046547566575032694\n",
      "Starting epoch 55\n",
      "Training loss: 0.053067824635349335\n",
      "Test loss: 0.046824732726370846\n",
      "Starting epoch 56\n",
      "Training loss: 0.0527875968789468\n",
      "Test loss: 0.047163541394251364\n",
      "Starting epoch 57\n",
      "Training loss: 0.05200195617851664\n",
      "Test loss: 0.04548285273766076\n",
      "Starting epoch 58\n",
      "Training loss: 0.05359233543276787\n",
      "Test loss: 0.04489379320983534\n",
      "Starting epoch 59\n",
      "Training loss: 0.05199988516139203\n",
      "Test loss: 0.04368308917791755\n",
      "Starting epoch 60\n",
      "Training loss: 0.05204795300960541\n",
      "Test loss: 0.04383837445466607\n",
      "Starting epoch 61\n",
      "Training loss: 0.05136238975969494\n",
      "Test loss: 0.046017990102646525\n",
      "Starting epoch 62\n",
      "Training loss: 0.050993548309216735\n",
      "Test loss: 0.04585852391189999\n",
      "Starting epoch 63\n",
      "Training loss: 0.05134055775697114\n",
      "Test loss: 0.04589786218410289\n",
      "Starting epoch 64\n",
      "Training loss: 0.050800927044426805\n",
      "Test loss: 0.04682987486874616\n",
      "Starting epoch 65\n",
      "Training loss: 0.051064825143481866\n",
      "Test loss: 0.04689044519155114\n",
      "Starting epoch 66\n",
      "Training loss: 0.0507183874972531\n",
      "Test loss: 0.04699132122375347\n",
      "Starting epoch 67\n",
      "Training loss: 0.050206131866720856\n",
      "Test loss: 0.04673929188262533\n",
      "Starting epoch 68\n",
      "Training loss: 0.050869323374306566\n",
      "Test loss: 0.045452002022001475\n",
      "Starting epoch 69\n",
      "Training loss: 0.04969502521342919\n",
      "Test loss: 0.04398020577651483\n",
      "Starting epoch 70\n",
      "Training loss: 0.049800004749024504\n",
      "Test loss: 0.044374560868298565\n",
      "Starting epoch 71\n",
      "Training loss: 0.05005552576946431\n",
      "Test loss: 0.045667226529783674\n",
      "Starting epoch 72\n",
      "Training loss: 0.04947238586476592\n",
      "Test loss: 0.04681793282981272\n",
      "Starting epoch 73\n",
      "Training loss: 0.0494554558860474\n",
      "Test loss: 0.046184387471940785\n",
      "Starting epoch 74\n",
      "Training loss: 0.050756594379905796\n",
      "Test loss: 0.04462222782550035\n",
      "Starting epoch 75\n",
      "Training loss: 0.04887981706711113\n",
      "Test loss: 0.04306257930066851\n",
      "Starting epoch 76\n",
      "Training loss: 0.048946433929634874\n",
      "Test loss: 0.044566408075668196\n",
      "Starting epoch 77\n",
      "Training loss: 0.048842609356172746\n",
      "Test loss: 0.04569115903642443\n",
      "Starting epoch 78\n",
      "Training loss: 0.04873249997369579\n",
      "Test loss: 0.04497054550382826\n",
      "Starting epoch 79\n",
      "Training loss: 0.048938873056016984\n",
      "Test loss: 0.044681300029710484\n",
      "Starting epoch 80\n",
      "Training loss: 0.048271779215238134\n",
      "Test loss: 0.0441111215148811\n",
      "Starting epoch 81\n",
      "Training loss: 0.04822495653003943\n",
      "Test loss: 0.044838539980075975\n",
      "Starting epoch 82\n",
      "Training loss: 0.048038363151374414\n",
      "Test loss: 0.04527069310899134\n",
      "Starting epoch 83\n",
      "Training loss: 0.0482028919409533\n",
      "Test loss: 0.0452902743730832\n",
      "Starting epoch 84\n",
      "Training loss: 0.047918154384757654\n",
      "Test loss: 0.04469560169511371\n",
      "Starting epoch 85\n",
      "Training loss: 0.04772330460245492\n",
      "Test loss: 0.045553681199197414\n",
      "Starting epoch 86\n",
      "Training loss: 0.04760726333641615\n",
      "Test loss: 0.04549293044126696\n",
      "Starting epoch 87\n",
      "Training loss: 0.047568915075943116\n",
      "Test loss: 0.04572905793234154\n",
      "Starting epoch 88\n",
      "Training loss: 0.04760013435219155\n",
      "Test loss: 0.04488826156766326\n",
      "Starting epoch 89\n",
      "Training loss: 0.047798922438113414\n",
      "Test loss: 0.044894597872539806\n",
      "Starting epoch 90\n",
      "Training loss: 0.047273294236816345\n",
      "Test loss: 0.04613040222062005\n",
      "Starting epoch 91\n",
      "Training loss: 0.04888782179990753\n",
      "Test loss: 0.04545471161879875\n",
      "Starting epoch 92\n",
      "Training loss: 0.04725192731521169\n",
      "Test loss: 0.043480743274644566\n",
      "Starting epoch 93\n",
      "Training loss: 0.047323434506772\n",
      "Test loss: 0.04502058857017093\n",
      "Starting epoch 94\n",
      "Training loss: 0.047661133660156216\n",
      "Test loss: 0.0447489227409716\n",
      "Starting epoch 95\n",
      "Training loss: 0.046755449266218746\n",
      "Test loss: 0.04387766867876053\n",
      "Starting epoch 96\n",
      "Training loss: 0.04672352542154125\n",
      "Test loss: 0.04474550706368906\n",
      "Starting epoch 97\n",
      "Training loss: 0.046603735047774236\n",
      "Test loss: 0.04518401250243187\n",
      "Starting epoch 98\n",
      "Training loss: 0.04688506860469208\n",
      "Test loss: 0.04554554199179014\n",
      "Starting epoch 99\n",
      "Training loss: 0.046789396859583304\n",
      "Test loss: 0.04607964652004065\n",
      "Starting epoch 100\n",
      "Training loss: 0.046445252709701415\n",
      "Test loss: 0.04487035644275171\n",
      "Starting epoch 101\n",
      "Training loss: 0.0464756399637363\n",
      "Test loss: 0.04558472514704422\n",
      "Starting epoch 102\n",
      "Training loss: 0.047372310742980143\n",
      "Test loss: 0.04623207146370852\n",
      "Starting epoch 103\n",
      "Training loss: 0.04648037034957135\n",
      "Test loss: 0.0443502813577652\n",
      "Starting epoch 104\n",
      "Training loss: 0.04626457124459939\n",
      "Test loss: 0.045373679173213464\n",
      "Starting epoch 105\n",
      "Training loss: 0.047468000015274424\n",
      "Test loss: 0.04497985652199498\n",
      "Starting epoch 106\n",
      "Training loss: 0.04707585116390322\n",
      "Test loss: 0.04704109593122094\n",
      "Starting epoch 107\n",
      "Training loss: 0.04611118290512288\n",
      "Test loss: 0.04472992355348887\n",
      "Starting epoch 108\n",
      "Training loss: 0.045916084444425145\n",
      "Test loss: 0.0455154485448643\n",
      "Starting epoch 109\n",
      "Training loss: 0.04574553412003595\n",
      "Test loss: 0.045702965171248826\n",
      "Starting epoch 110\n",
      "Training loss: 0.04585012353834559\n",
      "Test loss: 0.04587325636573412\n",
      "Starting epoch 111\n",
      "Training loss: 0.04563088763932713\n",
      "Test loss: 0.046239734799773606\n",
      "Starting epoch 112\n",
      "Training loss: 0.045506974834887706\n",
      "Test loss: 0.045760017854196054\n",
      "Starting epoch 113\n",
      "Training loss: 0.04538022751202349\n",
      "Test loss: 0.04528267177994604\n",
      "Starting epoch 114\n",
      "Training loss: 0.04551675543189049\n",
      "Test loss: 0.04558933498682799\n",
      "Starting epoch 115\n",
      "Training loss: 0.04544016462369043\n",
      "Test loss: 0.04488719171947903\n",
      "Starting epoch 116\n",
      "Training loss: 0.047279075398796895\n",
      "Test loss: 0.04489284519244124\n",
      "Starting epoch 117\n",
      "Training loss: 0.04507301051597126\n",
      "Test loss: 0.04344887065666693\n",
      "Starting epoch 118\n",
      "Training loss: 0.045479856003991896\n",
      "Test loss: 0.04489851253176177\n",
      "Starting epoch 119\n",
      "Training loss: 0.045298907295113704\n",
      "Test loss: 0.046408987155667054\n",
      "Starting epoch 120\n",
      "Training loss: 0.045614036990970865\n",
      "Test loss: 0.04585900819963879\n",
      "Starting epoch 121\n",
      "Training loss: 0.044785020964556055\n",
      "Test loss: 0.04660215504743435\n",
      "Starting epoch 122\n",
      "Training loss: 0.04494643919780606\n",
      "Test loss: 0.04565975021708895\n",
      "Starting epoch 123\n",
      "Training loss: 0.045786271604602455\n",
      "Test loss: 0.04480606938401858\n",
      "Starting epoch 124\n",
      "Training loss: 0.044762627572798336\n",
      "Test loss: 0.044083858805674093\n",
      "Starting epoch 125\n",
      "Training loss: 0.04484237993105513\n",
      "Test loss: 0.045319906815334605\n",
      "Starting epoch 126\n",
      "Training loss: 0.04456095849392844\n",
      "Test loss: 0.04531827099897243\n",
      "Starting epoch 127\n",
      "Training loss: 0.045994437376006704\n",
      "Test loss: 0.04619426942533917\n",
      "Starting epoch 128\n",
      "Training loss: 0.045443990924319284\n",
      "Test loss: 0.04754356970941579\n",
      "Starting epoch 129\n",
      "Training loss: 0.04449426529348874\n",
      "Test loss: 0.04494275525212288\n",
      "Starting epoch 130\n",
      "Training loss: 0.044823024727281974\n",
      "Test loss: 0.04541806987038365\n",
      "Starting epoch 131\n",
      "Training loss: 0.04450334709320889\n",
      "Test loss: 0.04649963285084124\n",
      "Starting epoch 132\n",
      "Training loss: 0.04480135892746878\n",
      "Test loss: 0.04536087714411594\n",
      "Starting epoch 133\n",
      "Training loss: 0.04417088749955912\n",
      "Test loss: 0.04457502563794454\n",
      "Starting epoch 134\n",
      "Training loss: 0.04419875563290276\n",
      "Test loss: 0.04562796576431504\n",
      "Starting epoch 135\n",
      "Training loss: 0.04493936481045895\n",
      "Test loss: 0.04587983643567121\n",
      "Starting epoch 136\n",
      "Training loss: 0.043932518570638096\n",
      "Test loss: 0.04459114600386885\n",
      "Starting epoch 137\n",
      "Training loss: 0.04398600549482908\n",
      "Test loss: 0.045041110504556586\n",
      "Starting epoch 138\n",
      "Training loss: 0.044042073007001255\n",
      "Test loss: 0.04577236125866572\n",
      "Starting epoch 139\n",
      "Training loss: 0.04383791665561863\n",
      "Test loss: 0.04539121566685261\n",
      "Starting epoch 140\n",
      "Training loss: 0.044304551831522924\n",
      "Test loss: 0.045777645651940944\n",
      "Starting epoch 141\n",
      "Training loss: 0.04415036640206321\n",
      "Test loss: 0.047118711181812815\n",
      "Starting epoch 142\n",
      "Training loss: 0.04398082679168123\n",
      "Test loss: 0.046905138115915984\n",
      "Starting epoch 143\n",
      "Training loss: 0.044595168017950215\n",
      "Test loss: 0.044657761575999086\n",
      "Starting epoch 144\n",
      "Training loss: 0.04421963768660045\n",
      "Test loss: 0.04365368049453806\n",
      "Starting epoch 145\n",
      "Training loss: 0.0458857177710924\n",
      "Test loss: 0.04382951819786319\n",
      "Starting epoch 146\n",
      "Training loss: 0.04586265975090324\n",
      "Test loss: 0.046587746452402184\n",
      "Starting epoch 147\n",
      "Training loss: 0.04402303365898914\n",
      "Test loss: 0.0441084799391252\n",
      "Starting epoch 148\n",
      "Training loss: 0.04365612676397699\n",
      "Test loss: 0.0438420593186661\n",
      "Starting epoch 149\n",
      "Training loss: 0.043497540972760464\n",
      "Test loss: 0.045459387341031325\n",
      "Starting epoch 150\n",
      "Training loss: 0.04333912947627365\n",
      "Test loss: 0.046265638812824535\n",
      "Starting epoch 151\n",
      "Training loss: 0.0434767196535087\n",
      "Test loss: 0.046268674096575486\n",
      "Starting epoch 152\n",
      "Training loss: 0.04363541561560553\n",
      "Test loss: 0.04628467118298566\n",
      "Starting epoch 153\n",
      "Training loss: 0.04330981119734342\n",
      "Test loss: 0.04683821855319871\n",
      "Starting epoch 154\n",
      "Training loss: 0.04356757496468357\n",
      "Test loss: 0.04604156894816293\n",
      "Starting epoch 155\n",
      "Training loss: 0.04410290870754445\n",
      "Test loss: 0.04570747235858882\n",
      "Starting epoch 156\n",
      "Training loss: 0.04311198099959092\n",
      "Test loss: 0.04702461317733482\n",
      "Starting epoch 157\n",
      "Training loss: 0.04315488504581764\n",
      "Test loss: 0.04620254646848749\n",
      "Starting epoch 158\n",
      "Training loss: 0.04321077539295447\n",
      "Test loss: 0.0462408314148585\n",
      "Starting epoch 159\n",
      "Training loss: 0.04336866620378416\n",
      "Test loss: 0.04612162929994089\n",
      "Starting epoch 160\n",
      "Training loss: 0.04331351102131312\n",
      "Test loss: 0.04514282141570692\n",
      "Starting epoch 161\n",
      "Training loss: 0.043513195314368265\n",
      "Test loss: 0.04616416763100359\n",
      "Starting epoch 162\n",
      "Training loss: 0.042893917223469157\n",
      "Test loss: 0.045488132508816545\n",
      "Starting epoch 163\n",
      "Training loss: 0.04323068364966111\n",
      "Test loss: 0.046215515618247015\n",
      "Starting epoch 164\n",
      "Training loss: 0.04286897185518116\n",
      "Test loss: 0.045604143015764376\n",
      "Starting epoch 165\n",
      "Training loss: 0.04363816103241483\n",
      "Test loss: 0.04603903299128568\n",
      "Starting epoch 166\n",
      "Training loss: 0.04286711708810486\n",
      "Test loss: 0.044958501778267046\n",
      "Starting epoch 167\n",
      "Training loss: 0.043247272031473334\n",
      "Test loss: 0.046054536683691874\n",
      "Starting epoch 168\n",
      "Training loss: 0.04325538348467624\n",
      "Test loss: 0.046113829921793054\n",
      "Starting epoch 169\n",
      "Training loss: 0.042600541207634034\n",
      "Test loss: 0.04757373855897674\n",
      "Starting epoch 170\n",
      "Training loss: 0.043186379260704164\n",
      "Test loss: 0.046920329608299116\n",
      "Starting epoch 171\n",
      "Training loss: 0.042529674582794066\n",
      "Test loss: 0.04505800028090124\n",
      "Starting epoch 172\n",
      "Training loss: 0.042979698658722344\n",
      "Test loss: 0.045723750083534805\n",
      "Starting epoch 173\n",
      "Training loss: 0.04248622392655396\n",
      "Test loss: 0.04739932453742734\n",
      "Starting epoch 174\n",
      "Training loss: 0.043038668691134846\n",
      "Test loss: 0.046629114973324316\n",
      "Starting epoch 175\n",
      "Training loss: 0.04254615728239544\n",
      "Test loss: 0.04664270552220168\n",
      "Starting epoch 176\n",
      "Training loss: 0.04283337954614983\n",
      "Test loss: 0.04533186072000751\n",
      "Starting epoch 177\n",
      "Training loss: 0.04231084913748209\n",
      "Test loss: 0.04482902710636457\n",
      "Starting epoch 178\n",
      "Training loss: 0.04231001626028389\n",
      "Test loss: 0.0459418746608275\n",
      "Starting epoch 179\n",
      "Training loss: 0.04227486087894831\n",
      "Test loss: 0.046305871189192487\n",
      "Starting epoch 180\n",
      "Training loss: 0.04270337125072714\n",
      "Test loss: 0.045610481666194067\n",
      "Starting epoch 181\n",
      "Training loss: 0.04211850971227787\n",
      "Test loss: 0.04517766267613128\n",
      "Starting epoch 182\n",
      "Training loss: 0.042178727014631524\n",
      "Test loss: 0.046074366128003155\n",
      "Starting epoch 183\n",
      "Training loss: 0.042353900912843766\n",
      "Test loss: 0.04641826023106222\n",
      "Starting epoch 184\n",
      "Training loss: 0.042274678278653346\n",
      "Test loss: 0.04576979329188665\n",
      "Starting epoch 185\n",
      "Training loss: 0.04228870723335469\n",
      "Test loss: 0.04569766253094982\n",
      "Starting epoch 186\n",
      "Training loss: 0.042005378691876524\n",
      "Test loss: 0.04527679465159222\n",
      "Starting epoch 187\n",
      "Training loss: 0.043267141844405506\n",
      "Test loss: 0.045646819941423555\n",
      "Starting epoch 188\n",
      "Training loss: 0.0421024850401722\n",
      "Test loss: 0.04455503455742642\n",
      "Starting epoch 189\n",
      "Training loss: 0.04226765824390239\n",
      "Test loss: 0.04481602073819549\n",
      "Starting epoch 190\n",
      "Training loss: 0.04256212931187427\n",
      "Test loss: 0.045890289531261834\n",
      "Starting epoch 191\n",
      "Training loss: 0.041722219742712424\n",
      "Test loss: 0.047582837718504446\n",
      "Starting epoch 192\n",
      "Training loss: 0.04253403140140361\n",
      "Test loss: 0.04749154547850291\n",
      "Starting epoch 193\n",
      "Training loss: 0.04198942192998089\n",
      "Test loss: 0.047455958677110846\n",
      "Starting epoch 194\n",
      "Training loss: 0.04205341924165116\n",
      "Test loss: 0.045574008452671545\n",
      "Starting epoch 195\n",
      "Training loss: 0.041872422103999093\n",
      "Test loss: 0.04509639395055948\n",
      "Starting epoch 196\n",
      "Training loss: 0.04320118686214822\n",
      "Test loss: 0.04561816894069866\n",
      "Starting epoch 197\n",
      "Training loss: 0.04173180735746368\n",
      "Test loss: 0.04448479482973063\n",
      "Starting epoch 198\n",
      "Training loss: 0.04172371517195076\n",
      "Test loss: 0.04491710331704882\n",
      "Starting epoch 199\n",
      "Training loss: 0.04422808530145004\n",
      "Test loss: 0.04515562972260846\n",
      "Starting epoch 200\n",
      "Training loss: 0.04171899547342394\n",
      "Test loss: 0.0438741142689078\n",
      "Starting epoch 201\n",
      "Training loss: 0.04197409639104468\n",
      "Test loss: 0.045835212149001936\n",
      "Starting epoch 202\n",
      "Training loss: 0.04152132479138062\n",
      "Test loss: 0.047237514345734206\n",
      "Starting epoch 203\n",
      "Training loss: 0.04224251167940312\n",
      "Test loss: 0.04671245233880149\n",
      "Starting epoch 204\n",
      "Training loss: 0.044119753554219106\n",
      "Test loss: 0.04741854565563025\n",
      "Starting epoch 205\n",
      "Training loss: 0.04146959705919516\n",
      "Test loss: 0.04899528815790459\n",
      "Starting epoch 206\n",
      "Training loss: 0.041978940245558004\n",
      "Test loss: 0.0472439703428083\n",
      "Starting epoch 207\n",
      "Training loss: 0.04164195408830877\n",
      "Test loss: 0.045957493078377515\n",
      "Starting epoch 208\n",
      "Training loss: 0.04147184762309809\n",
      "Test loss: 0.045638844508815696\n",
      "Starting epoch 209\n",
      "Training loss: 0.0412958922078375\n",
      "Test loss: 0.04667522220147981\n",
      "Starting epoch 210\n",
      "Training loss: 0.041707960919278565\n",
      "Test loss: 0.046621357953106915\n",
      "Starting epoch 211\n",
      "Training loss: 0.041516400507239044\n",
      "Test loss: 0.04666084582331004\n",
      "Starting epoch 212\n",
      "Training loss: 0.0421822478414559\n",
      "Test loss: 0.04543719247535423\n",
      "Starting epoch 213\n",
      "Training loss: 0.04120109480668287\n",
      "Test loss: 0.0469303920313164\n",
      "Starting epoch 214\n",
      "Training loss: 0.04120161518698833\n",
      "Test loss: 0.04648790729266626\n",
      "Starting epoch 215\n",
      "Training loss: 0.04112430750468715\n",
      "Test loss: 0.046069394797086716\n",
      "Starting epoch 216\n",
      "Training loss: 0.04308024158731836\n",
      "Test loss: 0.046164656816809264\n",
      "Starting epoch 217\n",
      "Training loss: 0.041807276975424565\n",
      "Test loss: 0.048296752765222835\n",
      "Starting epoch 218\n",
      "Training loss: 0.04298771459792481\n",
      "Test loss: 0.04590460526998396\n",
      "Starting epoch 219\n",
      "Training loss: 0.04098473648067381\n",
      "Test loss: 0.04770603161995058\n",
      "Starting epoch 220\n",
      "Training loss: 0.04128161237620916\n",
      "Test loss: 0.04712170562534421\n",
      "Starting epoch 221\n",
      "Training loss: 0.04113033896342653\n",
      "Test loss: 0.04705838652120696\n",
      "Starting epoch 222\n",
      "Training loss: 0.04084006547317153\n",
      "Test loss: 0.046093411605667184\n",
      "Starting epoch 223\n",
      "Training loss: 0.04207144433357676\n",
      "Test loss: 0.04616459666026963\n",
      "Starting epoch 224\n",
      "Training loss: 0.041265241984949734\n",
      "Test loss: 0.048052401178412966\n",
      "Starting epoch 225\n",
      "Training loss: 0.041725438027108305\n",
      "Test loss: 0.04756694880348665\n",
      "Starting epoch 226\n",
      "Training loss: 0.041003063321113586\n",
      "Test loss: 0.04505304854225229\n",
      "Starting epoch 227\n",
      "Training loss: 0.041136765150261705\n",
      "Test loss: 0.04574071436568543\n",
      "Starting epoch 228\n",
      "Training loss: 0.040931157828843004\n",
      "Test loss: 0.04536568994323412\n",
      "Starting epoch 229\n",
      "Training loss: 0.04103692523280128\n",
      "Test loss: 0.04675063325299157\n",
      "Starting epoch 230\n",
      "Training loss: 0.04090385187844761\n",
      "Test loss: 0.04766649269947299\n",
      "Starting epoch 231\n",
      "Training loss: 0.0414416277628453\n",
      "Test loss: 0.04676237950722376\n",
      "Starting epoch 232\n",
      "Training loss: 0.04077031744308159\n",
      "Test loss: 0.04750502813193533\n",
      "Starting epoch 233\n",
      "Training loss: 0.04084444968182532\n",
      "Test loss: 0.04713739678953533\n",
      "Starting epoch 234\n",
      "Training loss: 0.04085420059864638\n",
      "Test loss: 0.04574261964471252\n",
      "Starting epoch 235\n",
      "Training loss: 0.04312132573762878\n",
      "Test loss: 0.04667994062657709\n",
      "Starting epoch 236\n",
      "Training loss: 0.04066654368013632\n",
      "Test loss: 0.0487974606178425\n",
      "Starting epoch 237\n",
      "Training loss: 0.040797839216032965\n",
      "Test loss: 0.04678528673119015\n",
      "Starting epoch 238\n",
      "Training loss: 0.040717967465275624\n",
      "Test loss: 0.0455727178465437\n",
      "Starting epoch 239\n",
      "Training loss: 0.040555040581060235\n",
      "Test loss: 0.04612846989874487\n",
      "Starting epoch 240\n",
      "Training loss: 0.04066661684239497\n",
      "Test loss: 0.046033981911562105\n",
      "Starting epoch 241\n",
      "Training loss: 0.0406506101555023\n",
      "Test loss: 0.045400506575350406\n",
      "Starting epoch 242\n",
      "Training loss: 0.040832847509472095\n",
      "Test loss: 0.04512804139543463\n",
      "Starting epoch 243\n",
      "Training loss: 0.04052623245315474\n",
      "Test loss: 0.046981831598612994\n",
      "Starting epoch 244\n",
      "Training loss: 0.041252172628387075\n",
      "Test loss: 0.04683320927951071\n",
      "Starting epoch 245\n",
      "Training loss: 0.040207503760447266\n",
      "Test loss: 0.04477798966345964\n",
      "Starting epoch 246\n",
      "Training loss: 0.04032161911247206\n",
      "Test loss: 0.04563975692899139\n",
      "Starting epoch 247\n",
      "Training loss: 0.040521762654429576\n",
      "Test loss: 0.04563749181451621\n",
      "Starting epoch 248\n",
      "Training loss: 0.040221134048016347\n",
      "Test loss: 0.045429886767157805\n",
      "Starting epoch 249\n",
      "Training loss: 0.04028845865462647\n",
      "Test loss: 0.04601778448731811\n",
      "Starting epoch 250\n",
      "Training loss: 0.04107172616192552\n",
      "Test loss: 0.04641314858087787\n",
      "Starting epoch 251\n",
      "Training loss: 0.0400727242231369\n",
      "Test loss: 0.04765673678506304\n",
      "Starting epoch 252\n",
      "Training loss: 0.04029785383675919\n",
      "Test loss: 0.046750111160454924\n",
      "Starting epoch 253\n",
      "Training loss: 0.040349732962299566\n",
      "Test loss: 0.04614365500984369\n",
      "Starting epoch 254\n",
      "Training loss: 0.04024986360893875\n",
      "Test loss: 0.045694535943093126\n",
      "Starting epoch 255\n",
      "Training loss: 0.04040105500426449\n",
      "Test loss: 0.04646594656838311\n",
      "Starting epoch 256\n",
      "Training loss: 0.04002213502516512\n",
      "Test loss: 0.047202375544993964\n",
      "Starting epoch 257\n",
      "Training loss: 0.040499857825333954\n",
      "Test loss: 0.046667980789034454\n",
      "Starting epoch 258\n",
      "Training loss: 0.040292769853697445\n",
      "Test loss: 0.04555799067020416\n",
      "Starting epoch 259\n",
      "Training loss: 0.04097652252091736\n",
      "Test loss: 0.04642463779007947\n",
      "Starting epoch 260\n",
      "Training loss: 0.03992410193456978\n",
      "Test loss: 0.04751193619988583\n",
      "Starting epoch 261\n",
      "Training loss: 0.03992885467214662\n",
      "Test loss: 0.04620786176787482\n",
      "Starting epoch 262\n",
      "Training loss: 0.04104374019337482\n",
      "Test loss: 0.04629073747330242\n",
      "Starting epoch 263\n",
      "Training loss: 0.03992420752517513\n",
      "Test loss: 0.047742672333562816\n",
      "Starting epoch 264\n",
      "Training loss: 0.040145200814624304\n",
      "Test loss: 0.047009834812747106\n",
      "Starting epoch 265\n",
      "Training loss: 0.04009703363551468\n",
      "Test loss: 0.046734121524625354\n",
      "Starting epoch 266\n",
      "Training loss: 0.04062753237906049\n",
      "Test loss: 0.04539274044886783\n",
      "Starting epoch 267\n",
      "Training loss: 0.039821135490888455\n",
      "Test loss: 0.046968712436932104\n",
      "Starting epoch 268\n",
      "Training loss: 0.03983117725516929\n",
      "Test loss: 0.046317330527084845\n",
      "Starting epoch 269\n",
      "Training loss: 0.03968567871412293\n",
      "Test loss: 0.04576383103375082\n",
      "Starting epoch 270\n",
      "Training loss: 0.040805575605787216\n",
      "Test loss: 0.04564125146026964\n",
      "Starting epoch 271\n",
      "Training loss: 0.039685987607866034\n",
      "Test loss: 0.04764122709080025\n",
      "Starting epoch 272\n",
      "Training loss: 0.04013201911918453\n",
      "Test loss: 0.047326619978304264\n",
      "Starting epoch 273\n",
      "Training loss: 0.039687274939945484\n",
      "Test loss: 0.04569960137208303\n",
      "Starting epoch 274\n",
      "Training loss: 0.039851528638210454\n",
      "Test loss: 0.046297549477054015\n",
      "Starting epoch 275\n",
      "Training loss: 0.039739556122021596\n",
      "Test loss: 0.047283589012093015\n",
      "Starting epoch 276\n",
      "Training loss: 0.04008944989105717\n",
      "Test loss: 0.04729429384072622\n",
      "Starting epoch 277\n",
      "Training loss: 0.040555489356400534\n",
      "Test loss: 0.046885792165994644\n",
      "Starting epoch 278\n",
      "Training loss: 0.03970050439238548\n",
      "Test loss: 0.04509741895728641\n",
      "Starting epoch 279\n",
      "Training loss: 0.039979059737725336\n",
      "Test loss: 0.04595487309550798\n",
      "Starting epoch 280\n",
      "Training loss: 0.03926655247074659\n",
      "Test loss: 0.04702563059550745\n",
      "Starting epoch 281\n",
      "Training loss: 0.03936844441245814\n",
      "Test loss: 0.0465761189935384\n",
      "Starting epoch 282\n",
      "Training loss: 0.039462627751416846\n",
      "Test loss: 0.04595969134458789\n",
      "Starting epoch 283\n",
      "Training loss: 0.039549565828237376\n",
      "Test loss: 0.04541209312500777\n",
      "Starting epoch 284\n",
      "Training loss: 0.039692832737183964\n",
      "Test loss: 0.04602171301289841\n",
      "Starting epoch 285\n",
      "Training loss: 0.039959839438317254\n",
      "Test loss: 0.045137119513970834\n",
      "Starting epoch 286\n",
      "Training loss: 0.04016929009898764\n",
      "Test loss: 0.04674013307387078\n",
      "Starting epoch 287\n",
      "Training loss: 0.03931338272866656\n",
      "Test loss: 0.04796286058370714\n",
      "Starting epoch 288\n",
      "Training loss: 0.03927401599825406\n",
      "Test loss: 0.046814942801440204\n",
      "Starting epoch 289\n",
      "Training loss: 0.03928031851766539\n",
      "Test loss: 0.045293699812006066\n",
      "Starting epoch 290\n",
      "Training loss: 0.03927420317882397\n",
      "Test loss: 0.044934053249933104\n",
      "Starting epoch 291\n",
      "Training loss: 0.04027815853230289\n",
      "Test loss: 0.046305615109977896\n",
      "Starting epoch 292\n",
      "Training loss: 0.03904458746069767\n",
      "Test loss: 0.047865779863463506\n",
      "Starting epoch 293\n",
      "Training loss: 0.03990906715148785\n",
      "Test loss: 0.04760847516633846\n",
      "Starting epoch 294\n",
      "Training loss: 0.03964598226498385\n",
      "Test loss: 0.04756378561810211\n",
      "Starting epoch 295\n",
      "Training loss: 0.03918613957577064\n",
      "Test loss: 0.04702427334807537\n",
      "Starting epoch 296\n",
      "Training loss: 0.0389104530703826\n",
      "Test loss: 0.04627308787571059\n",
      "Starting epoch 297\n",
      "Training loss: 0.03906314178812699\n",
      "Test loss: 0.04560625456549503\n",
      "Starting epoch 298\n",
      "Training loss: 0.03898723631120119\n",
      "Test loss: 0.04597128058473269\n",
      "Starting epoch 299\n",
      "Training loss: 0.03965998593656743\n",
      "Test loss: 0.04536923764411498\n",
      "Starting epoch 300\n",
      "Training loss: 0.03888783243591668\n",
      "Test loss: 0.044312261424406814\n",
      "Starting epoch 301\n",
      "Training loss: 0.03885814755177889\n",
      "Test loss: 0.04563078684387384\n",
      "Starting epoch 302\n",
      "Training loss: 0.03911812365299366\n",
      "Test loss: 0.0458821316284162\n",
      "Starting epoch 303\n",
      "Training loss: 0.03901939976532928\n",
      "Test loss: 0.045216163965287035\n",
      "Starting epoch 304\n",
      "Training loss: 0.03864592591636493\n",
      "Test loss: 0.04600019035515962\n",
      "Starting epoch 305\n",
      "Training loss: 0.03887514849422408\n",
      "Test loss: 0.04571086802968272\n",
      "Starting epoch 306\n",
      "Training loss: 0.03933851597983329\n",
      "Test loss: 0.04550651115951715\n",
      "Starting epoch 307\n",
      "Training loss: 0.040860680405233725\n",
      "Test loss: 0.04591602266386703\n",
      "Starting epoch 308\n",
      "Training loss: 0.038486532347856976\n",
      "Test loss: 0.04353586811986235\n",
      "Starting epoch 309\n",
      "Training loss: 0.038660180922903\n",
      "Test loss: 0.04449905965615202\n",
      "Starting epoch 310\n",
      "Training loss: 0.0384996303464057\n",
      "Test loss: 0.045435194615964535\n",
      "Starting epoch 311\n",
      "Training loss: 0.03851496296950051\n",
      "Test loss: 0.04539259040245303\n",
      "Starting epoch 312\n",
      "Training loss: 0.03958606952037968\n",
      "Test loss: 0.045467755998726246\n",
      "Starting epoch 313\n",
      "Training loss: 0.03858368336910107\n",
      "Test loss: 0.046689677983522415\n",
      "Starting epoch 314\n",
      "Training loss: 0.03914376962013909\n",
      "Test loss: 0.04578298661443922\n",
      "Starting epoch 315\n",
      "Training loss: 0.038270648263516976\n",
      "Test loss: 0.04397243744245282\n",
      "Starting epoch 316\n",
      "Training loss: 0.03888658177657205\n",
      "Test loss: 0.04384190534000044\n",
      "Starting epoch 317\n",
      "Training loss: 0.03841768802128366\n",
      "Test loss: 0.04393702617811936\n",
      "Starting epoch 318\n",
      "Training loss: 0.03872759968469866\n",
      "Test loss: 0.044735993845043356\n",
      "Starting epoch 319\n",
      "Training loss: 0.03842405223699867\n",
      "Test loss: 0.0442628123693996\n",
      "Starting epoch 320\n",
      "Training loss: 0.038331140870930716\n",
      "Test loss: 0.04421800168024169\n",
      "Starting epoch 321\n",
      "Training loss: 0.038162522048490945\n",
      "Test loss: 0.04443647533103272\n",
      "Starting epoch 322\n",
      "Training loss: 0.03815297354928783\n",
      "Test loss: 0.044964567551182374\n",
      "Starting epoch 323\n",
      "Training loss: 0.03850891001400401\n",
      "Test loss: 0.04516246563030614\n",
      "Starting epoch 324\n",
      "Training loss: 0.03798024783857533\n",
      "Test loss: 0.045915594669403856\n",
      "Starting epoch 325\n",
      "Training loss: 0.038269612877095335\n",
      "Test loss: 0.045518472928691794\n",
      "Starting epoch 326\n",
      "Training loss: 0.03819045830579078\n",
      "Test loss: 0.04442725468564917\n",
      "Starting epoch 327\n",
      "Training loss: 0.038136468192593\n",
      "Test loss: 0.0449934681808507\n",
      "Starting epoch 328\n",
      "Training loss: 0.03878440622423516\n",
      "Test loss: 0.04460478408469094\n",
      "Starting epoch 329\n",
      "Training loss: 0.03855099125963743\n",
      "Test loss: 0.046047738373831464\n",
      "Starting epoch 330\n",
      "Training loss: 0.03787589363265233\n",
      "Test loss: 0.04666447860223276\n",
      "Starting epoch 331\n",
      "Training loss: 0.038410837533044036\n",
      "Test loss: 0.045095628748337425\n",
      "Starting epoch 332\n",
      "Training loss: 0.03783284414742814\n",
      "Test loss: 0.04345931692255868\n",
      "Starting epoch 333\n",
      "Training loss: 0.03778463864668471\n",
      "Test loss: 0.04395914229529875\n",
      "Starting epoch 334\n",
      "Training loss: 0.03778382854872062\n",
      "Test loss: 0.04422950013368218\n",
      "Starting epoch 335\n",
      "Training loss: 0.038157656239193\n",
      "Test loss: 0.04482629522681236\n",
      "Starting epoch 336\n",
      "Training loss: 0.037884574994200566\n",
      "Test loss: 0.04593781181783588\n",
      "Starting epoch 337\n",
      "Training loss: 0.03777022449085947\n",
      "Test loss: 0.04600282689487493\n",
      "Starting epoch 338\n",
      "Training loss: 0.03754409750709768\n",
      "Test loss: 0.045200161152967704\n",
      "Starting epoch 339\n",
      "Training loss: 0.03765792708050032\n",
      "Test loss: 0.04457335753573312\n",
      "Starting epoch 340\n",
      "Training loss: 0.03814089756275787\n",
      "Test loss: 0.043453040498274344\n",
      "Starting epoch 341\n",
      "Training loss: 0.037384584118596846\n",
      "Test loss: 0.04285980071182604\n",
      "Starting epoch 342\n",
      "Training loss: 0.0374997406587249\n",
      "Test loss: 0.043398869810280974\n",
      "Starting epoch 343\n",
      "Training loss: 0.037516834428075886\n",
      "Test loss: 0.04396152951651149\n",
      "Starting epoch 344\n",
      "Training loss: 0.03742784322773824\n",
      "Test loss: 0.04389898361707175\n",
      "Starting epoch 345\n",
      "Training loss: 0.03822839941035529\n",
      "Test loss: 0.04478265462374246\n",
      "Starting epoch 346\n",
      "Training loss: 0.03730111611915416\n",
      "Test loss: 0.045796057416333094\n",
      "Starting epoch 347\n",
      "Training loss: 0.03723656343387776\n",
      "Test loss: 0.04434080352937734\n",
      "Starting epoch 348\n",
      "Training loss: 0.0374829072932728\n",
      "Test loss: 0.04331783770962998\n",
      "Starting epoch 349\n",
      "Training loss: 0.03700191686387922\n",
      "Test loss: 0.04439159234364828\n",
      "Starting epoch 350\n",
      "Training loss: 0.03703433372935311\n",
      "Test loss: 0.04436637888903971\n",
      "Starting epoch 351\n",
      "Training loss: 0.037021444163850094\n",
      "Test loss: 0.0442437119781971\n",
      "Starting epoch 352\n",
      "Training loss: 0.036935415271608554\n",
      "Test loss: 0.04345461132901686\n",
      "Starting epoch 353\n",
      "Training loss: 0.03688534953799404\n",
      "Test loss: 0.043323293793946505\n",
      "Starting epoch 354\n",
      "Training loss: 0.0372905006296322\n",
      "Test loss: 0.043940146212224605\n",
      "Starting epoch 355\n",
      "Training loss: 0.0379939826785541\n",
      "Test loss: 0.043681171964164135\n",
      "Starting epoch 356\n",
      "Training loss: 0.03688490305279122\n",
      "Test loss: 0.04530149560283731\n",
      "Starting epoch 357\n",
      "Training loss: 0.036905104813517116\n",
      "Test loss: 0.04393462588389715\n",
      "Starting epoch 358\n",
      "Training loss: 0.03731998080601458\n",
      "Test loss: 0.04390192680336811\n",
      "Starting epoch 359\n",
      "Training loss: 0.03677457345069432\n",
      "Test loss: 0.044812408448369416\n",
      "Starting epoch 360\n",
      "Training loss: 0.03662812276208987\n",
      "Test loss: 0.04361874141074993\n",
      "Starting epoch 361\n",
      "Training loss: 0.03675624013679926\n",
      "Test loss: 0.043570878743021575\n",
      "Starting epoch 362\n",
      "Training loss: 0.03689974897586908\n",
      "Test loss: 0.042822384309989435\n",
      "Starting epoch 363\n",
      "Training loss: 0.03676531506610698\n",
      "Test loss: 0.04362397385692155\n",
      "Starting epoch 364\n",
      "Training loss: 0.03657432474562379\n",
      "Test loss: 0.04297604149690381\n",
      "Starting epoch 365\n",
      "Training loss: 0.03638001638235616\n",
      "Test loss: 0.04371465639107757\n",
      "Starting epoch 366\n",
      "Training loss: 0.03636734740289509\n",
      "Test loss: 0.04339291762422632\n",
      "Starting epoch 367\n",
      "Training loss: 0.0363283888360516\n",
      "Test loss: 0.04286384927453818\n",
      "Starting epoch 368\n",
      "Training loss: 0.036372040993854646\n",
      "Test loss: 0.04310271171508012\n",
      "Starting epoch 369\n",
      "Training loss: 0.03630240573013415\n",
      "Test loss: 0.04256188220999859\n",
      "Starting epoch 370\n",
      "Training loss: 0.03660879626137312\n",
      "Test loss: 0.042451549183439324\n",
      "Starting epoch 371\n",
      "Training loss: 0.035997148236778916\n",
      "Test loss: 0.04194446266801269\n",
      "Starting epoch 372\n",
      "Training loss: 0.036410357925246974\n",
      "Test loss: 0.04215490121256422\n",
      "Starting epoch 373\n",
      "Training loss: 0.03590135093106598\n",
      "Test loss: 0.042075524796490314\n",
      "Starting epoch 374\n",
      "Training loss: 0.03623247928306705\n",
      "Test loss: 0.043400404077989084\n",
      "Starting epoch 375\n",
      "Training loss: 0.0365272155794941\n",
      "Test loss: 0.04410485681836252\n",
      "Starting epoch 376\n",
      "Training loss: 0.035937536752126256\n",
      "Test loss: 0.042218332972239564\n",
      "Starting epoch 377\n",
      "Training loss: 0.03610287200598443\n",
      "Test loss: 0.04250546641371868\n",
      "Starting epoch 378\n",
      "Training loss: 0.035565229529728654\n",
      "Test loss: 0.04364630842098483\n",
      "Starting epoch 379\n",
      "Training loss: 0.03565804739711714\n",
      "Test loss: 0.04359939835827659\n",
      "Starting epoch 380\n",
      "Training loss: 0.0355100545604698\n",
      "Test loss: 0.04260649125057238\n",
      "Starting epoch 381\n",
      "Training loss: 0.03555028754301735\n",
      "Test loss: 0.04210696190043732\n",
      "Starting epoch 382\n",
      "Training loss: 0.03566040129202311\n",
      "Test loss: 0.042708356071401526\n",
      "Starting epoch 383\n",
      "Training loss: 0.03618979612823393\n",
      "Test loss: 0.04253323610734056\n",
      "Starting epoch 384\n",
      "Training loss: 0.03608993905000999\n",
      "Test loss: 0.044038997342189155\n",
      "Starting epoch 385\n",
      "Training loss: 0.03542304680239959\n",
      "Test loss: 0.04450118852158388\n",
      "Starting epoch 386\n",
      "Training loss: 0.035410446649203536\n",
      "Test loss: 0.042694662870080384\n",
      "Starting epoch 387\n",
      "Training loss: 0.034944112962264504\n",
      "Test loss: 0.04273124742839071\n",
      "Starting epoch 388\n",
      "Training loss: 0.034977613169638835\n",
      "Test loss: 0.0425629069407781\n",
      "Starting epoch 389\n",
      "Training loss: 0.03504494431077457\n",
      "Test loss: 0.0421993367511917\n",
      "Starting epoch 390\n",
      "Training loss: 0.03470244612850127\n",
      "Test loss: 0.04296056505430628\n",
      "Starting epoch 391\n",
      "Training loss: 0.03527452760055417\n",
      "Test loss: 0.042583631420577014\n",
      "Starting epoch 392\n",
      "Training loss: 0.03599300504219337\n",
      "Test loss: 0.041322626207989675\n",
      "Starting epoch 393\n",
      "Training loss: 0.0344618195515187\n",
      "Test loss: 0.042977847848777416\n",
      "Starting epoch 394\n",
      "Training loss: 0.03468816064786716\n",
      "Test loss: 0.0420944462702782\n",
      "Starting epoch 395\n",
      "Training loss: 0.03444825265495503\n",
      "Test loss: 0.041152930094136134\n",
      "Starting epoch 396\n",
      "Training loss: 0.03413302994898108\n",
      "Test loss: 0.04183856487550117\n",
      "Starting epoch 397\n",
      "Training loss: 0.03415052149994451\n",
      "Test loss: 0.04212541629870733\n",
      "Starting epoch 398\n",
      "Training loss: 0.0343127435714495\n",
      "Test loss: 0.04163391443176402\n",
      "Starting epoch 399\n",
      "Training loss: 0.03427085671268525\n",
      "Test loss: 0.04132256446475232\n",
      "Starting epoch 400\n",
      "Training loss: 0.03490185212405002\n",
      "Test loss: 0.042195029142830104\n",
      "Starting epoch 401\n",
      "Training loss: 0.03418644206201444\n",
      "Test loss: 0.0405884390282962\n",
      "Starting epoch 402\n",
      "Training loss: 0.034028609664958034\n",
      "Test loss: 0.04015768885060593\n",
      "Starting epoch 403\n",
      "Training loss: 0.033764360747376425\n",
      "Test loss: 0.040222569817194236\n",
      "Starting epoch 404\n",
      "Training loss: 0.03355323868330385\n",
      "Test loss: 0.04089675367706352\n",
      "Starting epoch 405\n",
      "Training loss: 0.03331295810029155\n",
      "Test loss: 0.04169440159091243\n",
      "Starting epoch 406\n",
      "Training loss: 0.03349754812776065\n",
      "Test loss: 0.041708157018378926\n",
      "Starting epoch 407\n",
      "Training loss: 0.03378511861455245\n",
      "Test loss: 0.040905330813041436\n",
      "Starting epoch 408\n",
      "Training loss: 0.03330468044417803\n",
      "Test loss: 0.04209049487555468\n",
      "Starting epoch 409\n",
      "Training loss: 0.03330770458598606\n",
      "Test loss: 0.04252275383030927\n",
      "Starting epoch 410\n",
      "Training loss: 0.033313223389817063\n",
      "Test loss: 0.04243558093353554\n",
      "Starting epoch 411\n",
      "Training loss: 0.03332282138652489\n",
      "Test loss: 0.042015338523520365\n",
      "Starting epoch 412\n",
      "Training loss: 0.03416309835480862\n",
      "Test loss: 0.04068739291418482\n",
      "Starting epoch 413\n",
      "Training loss: 0.03370989401076661\n",
      "Test loss: 0.03953996211014412\n",
      "Starting epoch 414\n",
      "Training loss: 0.03260945027968923\n",
      "Test loss: 0.041664215770584566\n",
      "Starting epoch 415\n",
      "Training loss: 0.03297165961417019\n",
      "Test loss: 0.041829683576469066\n",
      "Starting epoch 416\n",
      "Training loss: 0.032564984726124124\n",
      "Test loss: 0.042190997412911165\n",
      "Starting epoch 417\n",
      "Training loss: 0.03241579744537346\n",
      "Test loss: 0.040899433885459545\n",
      "Starting epoch 418\n",
      "Training loss: 0.03234541110816549\n",
      "Test loss: 0.04009185600335951\n",
      "Starting epoch 419\n",
      "Training loss: 0.03278190346404177\n",
      "Test loss: 0.040084767189842684\n",
      "Starting epoch 420\n",
      "Training loss: 0.03249129903365354\n",
      "Test loss: 0.04130612272355291\n",
      "Starting epoch 421\n",
      "Training loss: 0.03204098710271179\n",
      "Test loss: 0.040404306862641265\n",
      "Starting epoch 422\n",
      "Training loss: 0.03183706428428165\n",
      "Test loss: 0.04008733012058117\n",
      "Starting epoch 423\n",
      "Training loss: 0.03175147668626465\n",
      "Test loss: 0.0406841592932189\n",
      "Starting epoch 424\n",
      "Training loss: 0.0320375241400277\n",
      "Test loss: 0.04127291217446327\n",
      "Starting epoch 425\n",
      "Training loss: 0.031963571386991955\n",
      "Test loss: 0.040222580510157126\n",
      "Starting epoch 426\n",
      "Training loss: 0.03178003650219714\n",
      "Test loss: 0.04099158915104689\n",
      "Starting epoch 427\n",
      "Training loss: 0.031456236223705476\n",
      "Test loss: 0.040180806898408465\n",
      "Starting epoch 428\n",
      "Training loss: 0.031297641058192875\n",
      "Test loss: 0.04076066351047269\n",
      "Starting epoch 429\n",
      "Training loss: 0.03146563024550188\n",
      "Test loss: 0.04100507063170274\n",
      "Starting epoch 430\n",
      "Training loss: 0.03094909077540773\n",
      "Test loss: 0.041436946088517154\n",
      "Starting epoch 431\n",
      "Training loss: 0.030983980073303472\n",
      "Test loss: 0.04056051897781866\n",
      "Starting epoch 432\n",
      "Training loss: 0.030849192688455346\n",
      "Test loss: 0.03980002614359061\n",
      "Starting epoch 433\n",
      "Training loss: 0.033023527777585826\n",
      "Test loss: 0.039890194242751156\n",
      "Starting epoch 434\n",
      "Training loss: 0.030558900419073026\n",
      "Test loss: 0.04223807307857054\n",
      "Starting epoch 435\n",
      "Training loss: 0.03062591176541125\n",
      "Test loss: 0.04099988371685699\n",
      "Starting epoch 436\n",
      "Training loss: 0.030538798111384033\n",
      "Test loss: 0.04022565442654821\n",
      "Starting epoch 437\n",
      "Training loss: 0.03036176349173804\n",
      "Test loss: 0.040810564187941725\n",
      "Starting epoch 438\n",
      "Training loss: 0.030548025044749995\n",
      "Test loss: 0.04037485230300161\n",
      "Starting epoch 439\n",
      "Training loss: 0.029992282818086812\n",
      "Test loss: 0.04106901096249068\n",
      "Starting epoch 440\n",
      "Training loss: 0.030023057807664403\n",
      "Test loss: 0.0410541043513351\n",
      "Starting epoch 441\n",
      "Training loss: 0.030019036509463044\n",
      "Test loss: 0.04101245960703603\n",
      "Starting epoch 442\n",
      "Training loss: 0.02976767682149762\n",
      "Test loss: 0.04103990409661223\n",
      "Starting epoch 443\n",
      "Training loss: 0.029788865310857532\n",
      "Test loss: 0.04004425803820292\n",
      "Starting epoch 444\n",
      "Training loss: 0.03012992960752034\n",
      "Test loss: 0.04033431697084948\n",
      "Starting epoch 445\n",
      "Training loss: 0.029501437408025147\n",
      "Test loss: 0.04126866851691847\n",
      "Starting epoch 446\n",
      "Training loss: 0.02969018565338166\n",
      "Test loss: 0.04053005141516527\n",
      "Starting epoch 447\n",
      "Training loss: 0.029700494233946332\n",
      "Test loss: 0.039660528371179546\n",
      "Starting epoch 448\n",
      "Training loss: 0.02937452308833599\n",
      "Test loss: 0.040752679385520796\n",
      "Starting epoch 449\n",
      "Training loss: 0.028910182538579722\n",
      "Test loss: 0.03999355387080599\n",
      "Starting epoch 450\n",
      "Training loss: 0.02904845652033071\n",
      "Test loss: 0.03987633091983972\n",
      "Starting epoch 451\n",
      "Training loss: 0.029071284916068686\n",
      "Test loss: 0.04044378259115749\n",
      "Starting epoch 452\n",
      "Training loss: 0.028973586979459544\n",
      "Test loss: 0.040015064317871024\n",
      "Starting epoch 453\n",
      "Training loss: 0.02862233625816517\n",
      "Test loss: 0.040598212430874504\n",
      "Starting epoch 454\n",
      "Training loss: 0.028703397994891543\n",
      "Test loss: 0.04043391364178172\n",
      "Starting epoch 455\n",
      "Training loss: 0.0287501722207812\n",
      "Test loss: 0.03957171696755621\n",
      "Starting epoch 456\n",
      "Training loss: 0.028316738221366874\n",
      "Test loss: 0.0403899071669137\n",
      "Starting epoch 457\n",
      "Training loss: 0.028130927688029946\n",
      "Test loss: 0.04002989483652292\n",
      "Starting epoch 458\n",
      "Training loss: 0.028339593930811178\n",
      "Test loss: 0.03981191264810385\n",
      "Starting epoch 459\n",
      "Training loss: 0.02819797903543613\n",
      "Test loss: 0.04032026148504681\n",
      "Starting epoch 460\n",
      "Training loss: 0.028091159023222376\n",
      "Test loss: 0.040678951130421075\n",
      "Starting epoch 461\n",
      "Training loss: 0.027987703222964632\n",
      "Test loss: 0.04021722588826109\n",
      "Starting epoch 462\n",
      "Training loss: 0.027839925018001775\n",
      "Test loss: 0.039943802204948885\n",
      "Starting epoch 463\n",
      "Training loss: 0.027995936542016563\n",
      "Test loss: 0.03951552524058907\n",
      "Starting epoch 464\n",
      "Training loss: 0.027997643427282084\n",
      "Test loss: 0.04026658626066314\n",
      "Starting epoch 465\n",
      "Training loss: 0.028302063646375157\n",
      "Test loss: 0.039609346400808404\n",
      "Starting epoch 466\n",
      "Training loss: 0.027436772208721913\n",
      "Test loss: 0.04065549221855623\n",
      "Starting epoch 467\n",
      "Training loss: 0.027474132900843856\n",
      "Test loss: 0.04037666251813924\n",
      "Starting epoch 468\n",
      "Training loss: 0.027089153889749872\n",
      "Test loss: 0.04033786458549676\n",
      "Starting epoch 469\n",
      "Training loss: 0.027022433054984592\n",
      "Test loss: 0.040123295314885954\n",
      "Starting epoch 470\n",
      "Training loss: 0.027402454620746315\n",
      "Test loss: 0.03967822343111038\n",
      "Starting epoch 471\n",
      "Training loss: 0.027275777009666942\n",
      "Test loss: 0.03897378000396269\n",
      "Starting epoch 472\n",
      "Training loss: 0.02946057781332829\n",
      "Test loss: 0.03979082736704084\n",
      "Starting epoch 473\n",
      "Training loss: 0.026881214109112005\n",
      "Test loss: 0.0383777617028466\n",
      "Starting epoch 474\n",
      "Training loss: 0.026798373668408784\n",
      "Test loss: 0.03926154557201597\n",
      "Starting epoch 475\n",
      "Training loss: 0.02700545086113156\n",
      "Test loss: 0.039854861381981105\n",
      "Starting epoch 476\n",
      "Training loss: 0.026803952412771396\n",
      "Test loss: 0.040580612779767426\n",
      "Starting epoch 477\n",
      "Training loss: 0.026707870000209966\n",
      "Test loss: 0.039417481532803285\n",
      "Starting epoch 478\n",
      "Training loss: 0.02702058216587442\n",
      "Test loss: 0.04001541408123793\n",
      "Starting epoch 479\n",
      "Training loss: 0.02626272892487831\n",
      "Test loss: 0.03896154311520082\n",
      "Starting epoch 480\n",
      "Training loss: 0.026258121382017604\n",
      "Test loss: 0.03936135982749639\n",
      "Starting epoch 481\n",
      "Training loss: 0.026078074666686723\n",
      "Test loss: 0.04000223069279282\n",
      "Starting epoch 482\n",
      "Training loss: 0.026184658718402268\n",
      "Test loss: 0.03962736024900719\n",
      "Starting epoch 483\n",
      "Training loss: 0.026114283892952027\n",
      "Test loss: 0.039506219879344655\n",
      "Starting epoch 484\n",
      "Training loss: 0.0260860804224112\n",
      "Test loss: 0.03936555954041304\n",
      "Starting epoch 485\n",
      "Training loss: 0.02567316781057686\n",
      "Test loss: 0.03986838597942282\n",
      "Starting epoch 486\n",
      "Training loss: 0.025872510812077365\n",
      "Test loss: 0.03981716834284641\n",
      "Starting epoch 487\n",
      "Training loss: 0.0258085911513352\n",
      "Test loss: 0.039412878867652684\n",
      "Starting epoch 488\n",
      "Training loss: 0.025412780706022608\n",
      "Test loss: 0.03972162361498232\n",
      "Starting epoch 489\n",
      "Training loss: 0.025383130631974487\n",
      "Test loss: 0.039802370110043776\n",
      "Starting epoch 490\n",
      "Training loss: 0.025350600236751994\n",
      "Test loss: 0.03974287943155677\n",
      "Starting epoch 491\n",
      "Training loss: 0.025397247940179755\n",
      "Test loss: 0.04019903067361425\n",
      "Starting epoch 492\n",
      "Training loss: 0.0253750420496112\n",
      "Test loss: 0.040179369350274406\n",
      "Starting epoch 493\n",
      "Training loss: 0.025202470113996598\n",
      "Test loss: 0.04015739068940834\n",
      "Starting epoch 494\n",
      "Training loss: 0.02519565860389686\n",
      "Test loss: 0.039553813774276664\n",
      "Starting epoch 495\n",
      "Training loss: 0.025614997830058706\n",
      "Test loss: 0.040099853856696024\n",
      "Starting epoch 496\n",
      "Training loss: 0.024847163940917273\n",
      "Test loss: 0.040961245281828776\n",
      "Starting epoch 497\n",
      "Training loss: 0.024772826215771378\n",
      "Test loss: 0.04046388053231769\n",
      "Starting epoch 498\n",
      "Training loss: 0.024944484875094694\n",
      "Test loss: 0.03962858159233023\n",
      "Starting epoch 499\n",
      "Training loss: 0.02466790390307786\n",
      "Test loss: 0.04001331481116789\n",
      "Starting epoch 500\n",
      "Training loss: 0.024585079752885906\n",
      "Test loss: 0.04024616366735211\n",
      "Starting epoch 501\n",
      "Training loss: 0.025022423475003632\n",
      "Test loss: 0.03946078458318004\n",
      "Starting epoch 502\n",
      "Training loss: 0.024608570914410176\n",
      "Test loss: 0.03881977291570769\n",
      "Starting epoch 503\n",
      "Training loss: 0.024235255069664268\n",
      "Test loss: 0.039674511524262254\n",
      "Starting epoch 504\n",
      "Training loss: 0.02422600294478604\n",
      "Test loss: 0.03998528282951425\n",
      "Starting epoch 505\n",
      "Training loss: 0.024514379285153796\n",
      "Test loss: 0.040164016739085866\n",
      "Starting epoch 506\n",
      "Training loss: 0.024022668539012065\n",
      "Test loss: 0.04047099356022146\n",
      "Starting epoch 507\n",
      "Training loss: 0.024458313635626776\n",
      "Test loss: 0.039931274536583156\n",
      "Starting epoch 508\n",
      "Training loss: 0.0242376907377458\n",
      "Test loss: 0.03878156219919523\n",
      "Starting epoch 509\n",
      "Training loss: 0.02393583037325593\n",
      "Test loss: 0.038752266171353834\n",
      "Starting epoch 510\n",
      "Training loss: 0.02405913497825138\n",
      "Test loss: 0.03981871475224142\n",
      "Starting epoch 511\n",
      "Training loss: 0.023903552320648412\n",
      "Test loss: 0.03986967030774664\n",
      "Starting epoch 512\n",
      "Training loss: 0.023861004955700187\n",
      "Test loss: 0.040246517569930466\n",
      "Starting epoch 513\n",
      "Training loss: 0.023875930941984303\n",
      "Test loss: 0.0398045160152294\n",
      "Starting epoch 514\n",
      "Training loss: 0.02352622065876351\n",
      "Test loss: 0.040400616410705775\n",
      "Starting epoch 515\n",
      "Training loss: 0.02340563392785729\n",
      "Test loss: 0.03989752961529626\n",
      "Starting epoch 516\n",
      "Training loss: 0.023446835241601117\n",
      "Test loss: 0.03959663936661349\n",
      "Starting epoch 517\n",
      "Training loss: 0.023211743896369073\n",
      "Test loss: 0.04022957246612619\n",
      "Starting epoch 518\n",
      "Training loss: 0.023853597063265862\n",
      "Test loss: 0.04018064712484678\n",
      "Starting epoch 519\n",
      "Training loss: 0.023224437151287424\n",
      "Test loss: 0.04083967126078076\n",
      "Starting epoch 520\n",
      "Training loss: 0.023820291929801958\n",
      "Test loss: 0.0403398011845571\n",
      "Starting epoch 521\n",
      "Training loss: 0.023549153759587008\n",
      "Test loss: 0.0391387808378096\n",
      "Starting epoch 522\n",
      "Training loss: 0.02330891143713818\n",
      "Test loss: 0.039897123282706295\n",
      "Starting epoch 523\n",
      "Training loss: 0.02287720400290411\n",
      "Test loss: 0.039561450757362226\n",
      "Starting epoch 524\n",
      "Training loss: 0.022920502777226636\n",
      "Test loss: 0.0397132970392704\n",
      "Starting epoch 525\n",
      "Training loss: 0.022737555419568154\n",
      "Test loss: 0.03992335691496178\n",
      "Starting epoch 526\n",
      "Training loss: 0.022715980980972775\n",
      "Test loss: 0.03975709237986141\n",
      "Starting epoch 527\n",
      "Training loss: 0.023390680337782768\n",
      "Test loss: 0.039793954955206975\n",
      "Starting epoch 528\n",
      "Training loss: 0.0234810401242776\n",
      "Test loss: 0.039293331266553315\n",
      "Starting epoch 529\n",
      "Training loss: 0.02258870445314001\n",
      "Test loss: 0.04041194957163599\n",
      "Starting epoch 530\n",
      "Training loss: 0.022630638763552806\n",
      "Test loss: 0.04017270756540475\n",
      "Starting epoch 531\n",
      "Training loss: 0.022498927430295555\n",
      "Test loss: 0.03972106399359526\n",
      "Starting epoch 532\n",
      "Training loss: 0.022605071668742132\n",
      "Test loss: 0.039413005872457114\n",
      "Starting epoch 533\n",
      "Training loss: 0.02248791614394696\n",
      "Test loss: 0.039272084556244036\n",
      "Starting epoch 534\n",
      "Training loss: 0.022167698464921264\n",
      "Test loss: 0.04022244253644237\n",
      "Starting epoch 535\n",
      "Training loss: 0.02280144488103077\n",
      "Test loss: 0.04067897113660971\n",
      "Starting epoch 536\n",
      "Training loss: 0.022055275921450287\n",
      "Test loss: 0.0395154245197773\n",
      "Starting epoch 537\n",
      "Training loss: 0.022105392160229994\n",
      "Test loss: 0.039738470481501684\n",
      "Starting epoch 538\n",
      "Training loss: 0.022023837891269903\n",
      "Test loss: 0.040578428793836524\n",
      "Starting epoch 539\n",
      "Training loss: 0.021935674651968676\n",
      "Test loss: 0.04096123410595788\n",
      "Starting epoch 540\n",
      "Training loss: 0.021727243804785072\n",
      "Test loss: 0.040704275170962013\n",
      "Starting epoch 541\n",
      "Training loss: 0.021879977806181203\n",
      "Test loss: 0.04001994099881914\n",
      "Starting epoch 542\n",
      "Training loss: 0.02166219886209144\n",
      "Test loss: 0.03948346980743938\n",
      "Starting epoch 543\n",
      "Training loss: 0.0225089610111518\n",
      "Test loss: 0.039912937553944414\n",
      "Starting epoch 544\n",
      "Training loss: 0.022020584430362356\n",
      "Test loss: 0.04085019472296591\n",
      "Starting epoch 545\n",
      "Training loss: 0.021531226937888098\n",
      "Test loss: 0.039900025007901366\n",
      "Starting epoch 546\n",
      "Training loss: 0.0214483983753646\n",
      "Test loss: 0.03978490919150688\n",
      "Starting epoch 547\n",
      "Training loss: 0.021460392161226662\n",
      "Test loss: 0.03979349288123625\n",
      "Starting epoch 548\n",
      "Training loss: 0.021385269605966865\n",
      "Test loss: 0.039497858120335474\n",
      "Starting epoch 549\n",
      "Training loss: 0.02135267807934128\n",
      "Test loss: 0.03947110874233423\n",
      "Starting epoch 550\n",
      "Training loss: 0.02157764240610795\n",
      "Test loss: 0.03980373494603016\n",
      "Starting epoch 551\n",
      "Training loss: 0.021520908433394353\n",
      "Test loss: 0.04061587265244237\n",
      "Starting epoch 552\n",
      "Training loss: 0.02198262305044737\n",
      "Test loss: 0.04009584215021244\n",
      "Starting epoch 553\n",
      "Training loss: 0.02255558909573516\n",
      "Test loss: 0.04069492883152432\n",
      "Starting epoch 554\n",
      "Training loss: 0.02082436372999285\n",
      "Test loss: 0.03923217414153947\n",
      "Starting epoch 555\n",
      "Training loss: 0.021261567173556227\n",
      "Test loss: 0.039510266579411646\n",
      "Starting epoch 556\n",
      "Training loss: 0.021519507388355303\n",
      "Test loss: 0.039516850064198174\n",
      "Starting epoch 557\n",
      "Training loss: 0.020971782505512238\n",
      "Test loss: 0.04088278321756257\n",
      "Starting epoch 558\n",
      "Training loss: 0.020904595826248654\n",
      "Test loss: 0.040400304314163\n",
      "Starting epoch 559\n",
      "Training loss: 0.020695913918927066\n",
      "Test loss: 0.03965959711759179\n",
      "Starting epoch 560\n",
      "Training loss: 0.020529546698585886\n",
      "Test loss: 0.03971710932199602\n",
      "Starting epoch 561\n",
      "Training loss: 0.020590187340486246\n",
      "Test loss: 0.04017425742414263\n",
      "Starting epoch 562\n",
      "Training loss: 0.020532839366647064\n",
      "Test loss: 0.04046162204058082\n",
      "Starting epoch 563\n",
      "Training loss: 0.02079387467171325\n",
      "Test loss: 0.040590500114140685\n",
      "Starting epoch 564\n",
      "Training loss: 0.020389668948826243\n",
      "Test loss: 0.04109625959837878\n",
      "Starting epoch 565\n",
      "Training loss: 0.02028457927288579\n",
      "Test loss: 0.04068457535295575\n",
      "Starting epoch 566\n",
      "Training loss: 0.020393318420306582\n",
      "Test loss: 0.039969526369262626\n",
      "Starting epoch 567\n",
      "Training loss: 0.02045865201192801\n",
      "Test loss: 0.04007260908407194\n",
      "Starting epoch 568\n",
      "Training loss: 0.02013105438014523\n",
      "Test loss: 0.03962255986752333\n",
      "Starting epoch 569\n",
      "Training loss: 0.020109328426054267\n",
      "Test loss: 0.03996829654055613\n",
      "Starting epoch 570\n",
      "Training loss: 0.020057247035571785\n",
      "Test loss: 0.039791474808697345\n",
      "Starting epoch 571\n",
      "Training loss: 0.019974048966999915\n",
      "Test loss: 0.03969658580091265\n",
      "Starting epoch 572\n",
      "Training loss: 0.019964442893740584\n",
      "Test loss: 0.040118663399307815\n",
      "Starting epoch 573\n",
      "Training loss: 0.02014791696775155\n",
      "Test loss: 0.040124808403628844\n",
      "Starting epoch 574\n",
      "Training loss: 0.019856992360876232\n",
      "Test loss: 0.04036753845435602\n",
      "Starting epoch 575\n",
      "Training loss: 0.019813126655387096\n",
      "Test loss: 0.04038974463387772\n",
      "Starting epoch 576\n",
      "Training loss: 0.01979431540506785\n",
      "Test loss: 0.04004615772929457\n",
      "Starting epoch 577\n",
      "Training loss: 0.019901964294373013\n",
      "Test loss: 0.04027443862071744\n",
      "Starting epoch 578\n",
      "Training loss: 0.02015390543297666\n",
      "Test loss: 0.04077149896572033\n",
      "Starting epoch 579\n",
      "Training loss: 0.020918658308562686\n",
      "Test loss: 0.04089111186288021\n",
      "Starting epoch 580\n",
      "Training loss: 0.01956671635147001\n",
      "Test loss: 0.038992736488580704\n",
      "Starting epoch 581\n",
      "Training loss: 0.019458988528759755\n",
      "Test loss: 0.03969819636808501\n",
      "Starting epoch 582\n",
      "Training loss: 0.019575075841829426\n",
      "Test loss: 0.03994537731287656\n",
      "Starting epoch 583\n",
      "Training loss: 0.019303501201946228\n",
      "Test loss: 0.04056071490049362\n",
      "Starting epoch 584\n",
      "Training loss: 0.019591401896027268\n",
      "Test loss: 0.04032198884696872\n",
      "Starting epoch 585\n",
      "Training loss: 0.019163081758334987\n",
      "Test loss: 0.03971518203616142\n",
      "Starting epoch 586\n",
      "Training loss: 0.01928308945088113\n",
      "Test loss: 0.039737066047059164\n",
      "Starting epoch 587\n",
      "Training loss: 0.01929942784128619\n",
      "Test loss: 0.039910497350825205\n",
      "Starting epoch 588\n",
      "Training loss: 0.019576643669947248\n",
      "Test loss: 0.04030643237961663\n",
      "Starting epoch 589\n",
      "Training loss: 0.0191645190791517\n",
      "Test loss: 0.03938271408831632\n",
      "Starting epoch 590\n",
      "Training loss: 0.019410689468266535\n",
      "Test loss: 0.0400169909828239\n",
      "Starting epoch 591\n",
      "Training loss: 0.019203541220211593\n",
      "Test loss: 0.04112520544893212\n",
      "Starting epoch 592\n",
      "Training loss: 0.019088265715075322\n",
      "Test loss: 0.041358369368093985\n",
      "Starting epoch 593\n",
      "Training loss: 0.01893931549408885\n",
      "Test loss: 0.039972391014021856\n",
      "Starting epoch 594\n",
      "Training loss: 0.019509902804112824\n",
      "Test loss: 0.039687837853475856\n",
      "Starting epoch 595\n",
      "Training loss: 0.018763959987974557\n",
      "Test loss: 0.04070906203102182\n",
      "Starting epoch 596\n",
      "Training loss: 0.018775915085780817\n",
      "Test loss: 0.04005020815465185\n",
      "Starting epoch 597\n",
      "Training loss: 0.01898066507133304\n",
      "Test loss: 0.040093482230548504\n",
      "Starting epoch 598\n",
      "Training loss: 0.018857938252755852\n",
      "Test loss: 0.04018879074741293\n",
      "Starting epoch 599\n",
      "Training loss: 0.019439824261381976\n",
      "Test loss: 0.039836974607573614\n",
      "Starting epoch 600\n",
      "Training loss: 0.018865161895996234\n",
      "Test loss: 0.04075313387093721\n",
      "Starting epoch 601\n",
      "Training loss: 0.018430696708745645\n",
      "Test loss: 0.04009067377558461\n",
      "Starting epoch 602\n",
      "Training loss: 0.01837532521515596\n",
      "Test loss: 0.039856794117777435\n",
      "Starting epoch 603\n",
      "Training loss: 0.018459663039348165\n",
      "Test loss: 0.040063838164011635\n",
      "Starting epoch 604\n",
      "Training loss: 0.018415884313280464\n",
      "Test loss: 0.04061092436313629\n",
      "Starting epoch 605\n",
      "Training loss: 0.018400684090667082\n",
      "Test loss: 0.040005031076294405\n",
      "Starting epoch 606\n",
      "Training loss: 0.018312788537901935\n",
      "Test loss: 0.040454174081484474\n",
      "Starting epoch 607\n",
      "Training loss: 0.018268097589006188\n",
      "Test loss: 0.04058988716591288\n",
      "Starting epoch 608\n",
      "Training loss: 0.018176254251452743\n",
      "Test loss: 0.039814400452154654\n",
      "Starting epoch 609\n",
      "Training loss: 0.0182008828479247\n",
      "Test loss: 0.039712166206704244\n",
      "Starting epoch 610\n",
      "Training loss: 0.018387678399926326\n",
      "Test loss: 0.040165643138742005\n",
      "Starting epoch 611\n",
      "Training loss: 0.018175684938543154\n",
      "Test loss: 0.039724199722210564\n",
      "Starting epoch 612\n",
      "Training loss: 0.018155924884266542\n",
      "Test loss: 0.039454524853715194\n",
      "Starting epoch 613\n",
      "Training loss: 0.01793312156175981\n",
      "Test loss: 0.03951947404830544\n",
      "Starting epoch 614\n",
      "Training loss: 0.01837598679007077\n",
      "Test loss: 0.039758417962325945\n",
      "Starting epoch 615\n",
      "Training loss: 0.01814638862966514\n",
      "Test loss: 0.03950617338220278\n",
      "Starting epoch 616\n",
      "Training loss: 0.01796525582426884\n",
      "Test loss: 0.03926646309318366\n",
      "Starting epoch 617\n",
      "Training loss: 0.017736498663415673\n",
      "Test loss: 0.03995078250213906\n",
      "Starting epoch 618\n",
      "Training loss: 0.017682906698252333\n",
      "Test loss: 0.04039319480458895\n",
      "Starting epoch 619\n",
      "Training loss: 0.01792256347835064\n",
      "Test loss: 0.04045735313384621\n",
      "Starting epoch 620\n",
      "Training loss: 0.01766822758878841\n",
      "Test loss: 0.040848137741839444\n",
      "Starting epoch 621\n",
      "Training loss: 0.017723017631739866\n",
      "Test loss: 0.04044613869929755\n",
      "Starting epoch 622\n",
      "Training loss: 0.01758780860204677\n",
      "Test loss: 0.03981153901528429\n",
      "Starting epoch 623\n",
      "Training loss: 0.017490266410053752\n",
      "Test loss: 0.04024462263893198\n",
      "Starting epoch 624\n",
      "Training loss: 0.017629975178202646\n",
      "Test loss: 0.04022714992364248\n",
      "Starting epoch 625\n",
      "Training loss: 0.017668059614838146\n",
      "Test loss: 0.04051833406642631\n",
      "Starting epoch 626\n",
      "Training loss: 0.017682395967059446\n",
      "Test loss: 0.03988881271194528\n",
      "Starting epoch 627\n",
      "Training loss: 0.017901424288016853\n",
      "Test loss: 0.039934421992964216\n",
      "Starting epoch 628\n",
      "Training loss: 0.017568852691376795\n",
      "Test loss: 0.03932659148618027\n",
      "Starting epoch 629\n",
      "Training loss: 0.017594549255292923\n",
      "Test loss: 0.039459759231518815\n",
      "Starting epoch 630\n",
      "Training loss: 0.017230226215524752\n",
      "Test loss: 0.04083364622460471\n",
      "Starting epoch 631\n",
      "Training loss: 0.01725470660956668\n",
      "Test loss: 0.04050246129433314\n",
      "Starting epoch 632\n",
      "Training loss: 0.01755959041355575\n",
      "Test loss: 0.0407824967470434\n",
      "Starting epoch 633\n",
      "Training loss: 0.017293076320994097\n",
      "Test loss: 0.040971353788066794\n",
      "Starting epoch 634\n",
      "Training loss: 0.01734841489767442\n",
      "Test loss: 0.04012993543788239\n",
      "Starting epoch 635\n",
      "Training loss: 0.01711709382104092\n",
      "Test loss: 0.040420492835066935\n",
      "Starting epoch 636\n",
      "Training loss: 0.016942790846844188\n",
      "Test loss: 0.03998450403688131\n",
      "Starting epoch 637\n",
      "Training loss: 0.017021645990307213\n",
      "Test loss: 0.03980483635570164\n",
      "Starting epoch 638\n",
      "Training loss: 0.017580752520531904\n",
      "Test loss: 0.03988285852527177\n",
      "Starting epoch 639\n",
      "Training loss: 0.016862164999617905\n",
      "Test loss: 0.041070500043807204\n",
      "Starting epoch 640\n",
      "Training loss: 0.017028456355338215\n",
      "Test loss: 0.04082394032566636\n",
      "Starting epoch 641\n",
      "Training loss: 0.016734631518360046\n",
      "Test loss: 0.03977898977420948\n",
      "Starting epoch 642\n",
      "Training loss: 0.016697260612224946\n",
      "Test loss: 0.03959538787603378\n",
      "Starting epoch 643\n",
      "Training loss: 0.01745465068054981\n",
      "Test loss: 0.039825835161738925\n",
      "Starting epoch 644\n",
      "Training loss: 0.016719382592156284\n",
      "Test loss: 0.03941230586281529\n",
      "Starting epoch 645\n",
      "Training loss: 0.016733023857117675\n",
      "Test loss: 0.040434077175127134\n",
      "Starting epoch 646\n",
      "Training loss: 0.01657091719327403\n",
      "Test loss: 0.04016711010977074\n",
      "Starting epoch 647\n",
      "Training loss: 0.016746568936304967\n",
      "Test loss: 0.040454524465733104\n",
      "Starting epoch 648\n",
      "Training loss: 0.016526650973275058\n",
      "Test loss: 0.04068358546053922\n",
      "Starting epoch 649\n",
      "Training loss: 0.01646503519083633\n",
      "Test loss: 0.04023981563470982\n",
      "Starting epoch 650\n",
      "Training loss: 0.016497889258822458\n",
      "Test loss: 0.04028617680348732\n",
      "Starting epoch 651\n",
      "Training loss: 0.016570435485756787\n",
      "Test loss: 0.040078872745787655\n",
      "Starting epoch 652\n",
      "Training loss: 0.016393294649534536\n",
      "Test loss: 0.040047235441980536\n",
      "Starting epoch 653\n",
      "Training loss: 0.016466781069509318\n",
      "Test loss: 0.03986528284709762\n",
      "Starting epoch 654\n",
      "Training loss: 0.016441261640093366\n",
      "Test loss: 0.03998219456385683\n",
      "Starting epoch 655\n",
      "Training loss: 0.01626024499046998\n",
      "Test loss: 0.0404937652250131\n",
      "Starting epoch 656\n",
      "Training loss: 0.0170099814956794\n",
      "Test loss: 0.04057008704101598\n",
      "Starting epoch 657\n",
      "Training loss: 0.01630154012351251\n",
      "Test loss: 0.041456479165289134\n",
      "Starting epoch 658\n",
      "Training loss: 0.016175238003374122\n",
      "Test loss: 0.04054635404436677\n",
      "Starting epoch 659\n",
      "Training loss: 0.01639232961613624\n",
      "Test loss: 0.040365008292374785\n",
      "Starting epoch 660\n",
      "Training loss: 0.016081898977033427\n",
      "Test loss: 0.040583528164360255\n",
      "Starting epoch 661\n",
      "Training loss: 0.016236687414958827\n",
      "Test loss: 0.04035784980213201\n",
      "Starting epoch 662\n",
      "Training loss: 0.01655987860848669\n",
      "Test loss: 0.04092174478703075\n",
      "Starting epoch 663\n",
      "Training loss: 0.01605948092812886\n",
      "Test loss: 0.041298646203897615\n",
      "Starting epoch 664\n",
      "Training loss: 0.0167485633712323\n",
      "Test loss: 0.040769845592202966\n",
      "Starting epoch 665\n",
      "Training loss: 0.015942326059839765\n",
      "Test loss: 0.041279747530266094\n",
      "Starting epoch 666\n",
      "Training loss: 0.015940343609965237\n",
      "Test loss: 0.04063857691707434\n",
      "Starting epoch 667\n",
      "Training loss: 0.016045839082999308\n",
      "Test loss: 0.040080717454353966\n",
      "Starting epoch 668\n",
      "Training loss: 0.0157811205040236\n",
      "Test loss: 0.04056159586266235\n",
      "Starting epoch 669\n",
      "Training loss: 0.015940184811832476\n",
      "Test loss: 0.040421010443457854\n",
      "Starting epoch 670\n",
      "Training loss: 0.015716494396939628\n",
      "Test loss: 0.03989298855540929\n",
      "Starting epoch 671\n",
      "Training loss: 0.01567380005096803\n",
      "Test loss: 0.03989239657918612\n",
      "Starting epoch 672\n",
      "Training loss: 0.015658629523803952\n",
      "Test loss: 0.04021022648171142\n",
      "Starting epoch 673\n",
      "Training loss: 0.015588613990388933\n",
      "Test loss: 0.040098204311949236\n",
      "Starting epoch 674\n",
      "Training loss: 0.015647970811753977\n",
      "Test loss: 0.04013094506054013\n",
      "Starting epoch 675\n",
      "Training loss: 0.015754831527344516\n",
      "Test loss: 0.040238171401951045\n",
      "Starting epoch 676\n",
      "Training loss: 0.0157257294984626\n",
      "Test loss: 0.04077812332521986\n",
      "Starting epoch 677\n",
      "Training loss: 0.01578382366016263\n",
      "Test loss: 0.040823806491163045\n",
      "Starting epoch 678\n",
      "Training loss: 0.015776211914957546\n",
      "Test loss: 0.03994991278482808\n",
      "Starting epoch 679\n",
      "Training loss: 0.015535876704532593\n",
      "Test loss: 0.04031079124521326\n",
      "Starting epoch 680\n",
      "Training loss: 0.015403396083561123\n",
      "Test loss: 0.04036783413202674\n",
      "Starting epoch 681\n",
      "Training loss: 0.01611708122931543\n",
      "Test loss: 0.04076280817389488\n",
      "Starting epoch 682\n",
      "Training loss: 0.01539383889710317\n",
      "Test loss: 0.041447389043039747\n",
      "Starting epoch 683\n",
      "Training loss: 0.015535704073969458\n",
      "Test loss: 0.04063507031511377\n",
      "Starting epoch 684\n",
      "Training loss: 0.015511664241308072\n",
      "Test loss: 0.04028920870688227\n",
      "Starting epoch 685\n",
      "Training loss: 0.015926554707474398\n",
      "Test loss: 0.040388107851699544\n",
      "Starting epoch 686\n",
      "Training loss: 0.015547167128104656\n",
      "Test loss: 0.04101512229277028\n",
      "Starting epoch 687\n",
      "Training loss: 0.015218817949539325\n",
      "Test loss: 0.040118837660109555\n",
      "Starting epoch 688\n",
      "Training loss: 0.015438859549457909\n",
      "Test loss: 0.04021379751739679\n",
      "Starting epoch 689\n",
      "Training loss: 0.015141600804006467\n",
      "Test loss: 0.039987766908274755\n",
      "Starting epoch 690\n",
      "Training loss: 0.015390641697239681\n",
      "Test loss: 0.04019849533560099\n",
      "Starting epoch 691\n",
      "Training loss: 0.01512586807862657\n",
      "Test loss: 0.04068459266865695\n",
      "Starting epoch 692\n",
      "Training loss: 0.015207649697167952\n",
      "Test loss: 0.040526279558738075\n",
      "Starting epoch 693\n",
      "Training loss: 0.015348257741234342\n",
      "Test loss: 0.040311292779666406\n",
      "Starting epoch 694\n",
      "Training loss: 0.015023255079496102\n",
      "Test loss: 0.039687407927380666\n",
      "Starting epoch 695\n",
      "Training loss: 0.015098439102045825\n",
      "Test loss: 0.04000611844714041\n",
      "Starting epoch 696\n",
      "Training loss: 0.014991892898669009\n",
      "Test loss: 0.04047394468000642\n",
      "Starting epoch 697\n",
      "Training loss: 0.015093447655805798\n",
      "Test loss: 0.040666654292080134\n",
      "Starting epoch 698\n",
      "Training loss: 0.01496605559817103\n",
      "Test loss: 0.039950108224595035\n",
      "Starting epoch 699\n",
      "Training loss: 0.014896938562026767\n",
      "Test loss: 0.04029554694339081\n",
      "Starting epoch 700\n",
      "Training loss: 0.015027425480914897\n",
      "Test loss: 0.04027643730794942\n",
      "Starting epoch 701\n",
      "Training loss: 0.01554834848788918\n",
      "Test loss: 0.04065646024213897\n",
      "Starting epoch 702\n",
      "Training loss: 0.015002442981864586\n",
      "Test loss: 0.039894202517138586\n",
      "Starting epoch 703\n",
      "Training loss: 0.014882210123001552\n",
      "Test loss: 0.03980266495987221\n",
      "Starting epoch 704\n",
      "Training loss: 0.015157069461267502\n",
      "Test loss: 0.04034085406197442\n",
      "Starting epoch 705\n",
      "Training loss: 0.014778519369906089\n",
      "Test loss: 0.0413537435233593\n",
      "Starting epoch 706\n",
      "Training loss: 0.015163421004888464\n",
      "Test loss: 0.04108768770540202\n",
      "Starting epoch 707\n",
      "Training loss: 0.01511860109071751\n",
      "Test loss: 0.04099883649636198\n",
      "Starting epoch 708\n",
      "Training loss: 0.014666742400922736\n",
      "Test loss: 0.03996820499499639\n",
      "Starting epoch 709\n",
      "Training loss: 0.014809211044282209\n",
      "Test loss: 0.03996391097704569\n",
      "Starting epoch 710\n",
      "Training loss: 0.014669441572222554\n",
      "Test loss: 0.04023290784270675\n",
      "Starting epoch 711\n",
      "Training loss: 0.014731696440426052\n",
      "Test loss: 0.040341803459105666\n",
      "Starting epoch 712\n",
      "Training loss: 0.014561454963977219\n",
      "Test loss: 0.04065411751744924\n",
      "Starting epoch 713\n",
      "Training loss: 0.014620128988486822\n",
      "Test loss: 0.0407996394292072\n",
      "Starting epoch 714\n",
      "Training loss: 0.01460571649682815\n",
      "Test loss: 0.040607575327157974\n",
      "Starting epoch 715\n",
      "Training loss: 0.014514731334858253\n",
      "Test loss: 0.04079330188256723\n",
      "Starting epoch 716\n",
      "Training loss: 0.014571958053551737\n",
      "Test loss: 0.04054076183173391\n",
      "Starting epoch 717\n",
      "Training loss: 0.014772790529933131\n",
      "Test loss: 0.040947945719515835\n",
      "Starting epoch 718\n",
      "Training loss: 0.014664020328248133\n",
      "Test loss: 0.0410806372485779\n",
      "Starting epoch 719\n",
      "Training loss: 0.014573850195671692\n",
      "Test loss: 0.04060360113227809\n",
      "Starting epoch 720\n",
      "Training loss: 0.01448391029824976\n",
      "Test loss: 0.040997834531245406\n",
      "Starting epoch 721\n",
      "Training loss: 0.014418788101585185\n",
      "Test loss: 0.04113887809216976\n",
      "Starting epoch 722\n",
      "Training loss: 0.01441908525455682\n",
      "Test loss: 0.0406885945962535\n",
      "Starting epoch 723\n",
      "Training loss: 0.014288938848576585\n",
      "Test loss: 0.04099527208341493\n",
      "Starting epoch 724\n",
      "Training loss: 0.01434730303275292\n",
      "Test loss: 0.04068589845189342\n",
      "Starting epoch 725\n",
      "Training loss: 0.014300597098762871\n",
      "Test loss: 0.040305481326800806\n",
      "Starting epoch 726\n",
      "Training loss: 0.014324433032850751\n",
      "Test loss: 0.04032476218762221\n",
      "Starting epoch 727\n",
      "Training loss: 0.014252645482660318\n",
      "Test loss: 0.040426694339624154\n",
      "Starting epoch 728\n",
      "Training loss: 0.014219541637013193\n",
      "Test loss: 0.040638581608180645\n",
      "Starting epoch 729\n",
      "Training loss: 0.014243440030783903\n",
      "Test loss: 0.04059662711289194\n",
      "Starting epoch 730\n",
      "Training loss: 0.014244184233858937\n",
      "Test loss: 0.040764538364277944\n",
      "Starting epoch 731\n",
      "Training loss: 0.014346766453541692\n",
      "Test loss: 0.04070116400166794\n",
      "Starting epoch 732\n",
      "Training loss: 0.014221535690250944\n",
      "Test loss: 0.04024586523020709\n",
      "Starting epoch 733\n",
      "Training loss: 0.014073048672470891\n",
      "Test loss: 0.039981869911706006\n",
      "Starting epoch 734\n",
      "Training loss: 0.014265362280191944\n",
      "Test loss: 0.04031286471419864\n",
      "Starting epoch 735\n",
      "Training loss: 0.014110975150690704\n",
      "Test loss: 0.040261526212648106\n",
      "Starting epoch 736\n",
      "Training loss: 0.014224781364691063\n",
      "Test loss: 0.040464167655618104\n",
      "Starting epoch 737\n",
      "Training loss: 0.014110489152982587\n",
      "Test loss: 0.04014704038423521\n",
      "Starting epoch 738\n",
      "Training loss: 0.014087571670896694\n",
      "Test loss: 0.04032647154397435\n",
      "Starting epoch 739\n",
      "Training loss: 0.014464037813490531\n",
      "Test loss: 0.040697485898379925\n",
      "Starting epoch 740\n",
      "Training loss: 0.01395711372988146\n",
      "Test loss: 0.04167217892353182\n",
      "Starting epoch 741\n",
      "Training loss: 0.014149852906216363\n",
      "Test loss: 0.04121301198999087\n",
      "Starting epoch 742\n",
      "Training loss: 0.013954603174304376\n",
      "Test loss: 0.04115632286778203\n",
      "Starting epoch 743\n",
      "Training loss: 0.013913830070466291\n",
      "Test loss: 0.040615712878880675\n",
      "Starting epoch 744\n",
      "Training loss: 0.014861047145773153\n",
      "Test loss: 0.04054701038532787\n",
      "Starting epoch 745\n",
      "Training loss: 0.014517486797737294\n",
      "Test loss: 0.040078847496597854\n",
      "Starting epoch 746\n",
      "Training loss: 0.013869287858365989\n",
      "Test loss: 0.041447576963239245\n",
      "Starting epoch 747\n",
      "Training loss: 0.013912321938598742\n",
      "Test loss: 0.04114748137416663\n",
      "Starting epoch 748\n",
      "Training loss: 0.013800526465304563\n",
      "Test loss: 0.041082150130360214\n",
      "Starting epoch 749\n",
      "Training loss: 0.013875581553114242\n",
      "Test loss: 0.04097572258777089\n",
      "Starting epoch 750\n",
      "Training loss: 0.013888056771677048\n",
      "Test loss: 0.04070884989643538\n",
      "Starting epoch 751\n",
      "Training loss: 0.013902854525529947\n",
      "Test loss: 0.04117685128693228\n",
      "Starting epoch 752\n",
      "Training loss: 0.013936892823606242\n",
      "Test loss: 0.04148164915817755\n",
      "Starting epoch 753\n",
      "Training loss: 0.014232564198433375\n",
      "Test loss: 0.040886550589844035\n",
      "Starting epoch 754\n",
      "Training loss: 0.013821232697514237\n",
      "Test loss: 0.0415007789377813\n",
      "Starting epoch 755\n",
      "Training loss: 0.013809123061230926\n",
      "Test loss: 0.040762359759321916\n",
      "Starting epoch 756\n",
      "Training loss: 0.01372148331682213\n",
      "Test loss: 0.04044087596789554\n",
      "Starting epoch 757\n",
      "Training loss: 0.013773119016016116\n",
      "Test loss: 0.04099181998107168\n",
      "Starting epoch 758\n",
      "Training loss: 0.01370158034269927\n",
      "Test loss: 0.04059624009662204\n",
      "Starting epoch 759\n",
      "Training loss: 0.013615780494740753\n",
      "Test loss: 0.04064744200419496\n",
      "Starting epoch 760\n",
      "Training loss: 0.013534870762072626\n",
      "Test loss: 0.040890484772346636\n",
      "Starting epoch 761\n",
      "Training loss: 0.013783054456847613\n",
      "Test loss: 0.04098399383602319\n",
      "Starting epoch 762\n",
      "Training loss: 0.014226427210158989\n",
      "Test loss: 0.04076247248384687\n",
      "Starting epoch 763\n",
      "Training loss: 0.013478170713928879\n",
      "Test loss: 0.041690217400038684\n",
      "Starting epoch 764\n",
      "Training loss: 0.01348450961599096\n",
      "Test loss: 0.041557147133129614\n",
      "Starting epoch 765\n",
      "Training loss: 0.01355228038718466\n",
      "Test loss: 0.041057798321600315\n",
      "Starting epoch 766\n",
      "Training loss: 0.013541952554197585\n",
      "Test loss: 0.040610253121013996\n",
      "Starting epoch 767\n",
      "Training loss: 0.013526208179651713\n",
      "Test loss: 0.040864175806442894\n",
      "Starting epoch 768\n",
      "Training loss: 0.013535829413621152\n",
      "Test loss: 0.04137203497467218\n",
      "Starting epoch 769\n",
      "Training loss: 0.01337747619349937\n",
      "Test loss: 0.041640256704003724\n",
      "Starting epoch 770\n",
      "Training loss: 0.013356512702512936\n",
      "Test loss: 0.04101642255705816\n",
      "Starting epoch 771\n",
      "Training loss: 0.013407296455297315\n",
      "Test loss: 0.040769055071804255\n",
      "Starting epoch 772\n",
      "Training loss: 0.013419044036112849\n",
      "Test loss: 0.040754046429086616\n",
      "Starting epoch 773\n",
      "Training loss: 0.013367003257401654\n",
      "Test loss: 0.04070772989480584\n",
      "Starting epoch 774\n",
      "Training loss: 0.013260444870493451\n",
      "Test loss: 0.0408885238899125\n",
      "Starting epoch 775\n",
      "Training loss: 0.013363119184238012\n",
      "Test loss: 0.040864191121525235\n",
      "Starting epoch 776\n",
      "Training loss: 0.013279278732103402\n",
      "Test loss: 0.040893510397937566\n",
      "Starting epoch 777\n",
      "Training loss: 0.013402194700768737\n",
      "Test loss: 0.04105502601574968\n",
      "Starting epoch 778\n",
      "Training loss: 0.013459117686162229\n",
      "Test loss: 0.04167334473243466\n",
      "Starting epoch 779\n",
      "Training loss: 0.013311683368243154\n",
      "Test loss: 0.04166892281285039\n",
      "Starting epoch 780\n",
      "Training loss: 0.013244900027992295\n",
      "Test loss: 0.04165789609154066\n",
      "Starting epoch 781\n",
      "Training loss: 0.013297830402973245\n",
      "Test loss: 0.04096983442151988\n",
      "Starting epoch 782\n",
      "Training loss: 0.013299428354032704\n",
      "Test loss: 0.041303334964646235\n",
      "Starting epoch 783\n",
      "Training loss: 0.013263426995912537\n",
      "Test loss: 0.04088554986649089\n",
      "Starting epoch 784\n",
      "Training loss: 0.013462497547391008\n",
      "Test loss: 0.04108558967709541\n",
      "Starting epoch 785\n",
      "Training loss: 0.013166234095687748\n",
      "Test loss: 0.04066977332587595\n",
      "Starting epoch 786\n",
      "Training loss: 0.013107447136865288\n",
      "Test loss: 0.04101691788269414\n",
      "Starting epoch 787\n",
      "Training loss: 0.013057073914125318\n",
      "Test loss: 0.04139285803669029\n",
      "Starting epoch 788\n",
      "Training loss: 0.013227003687595735\n",
      "Test loss: 0.04129581553516565\n",
      "Starting epoch 789\n",
      "Training loss: 0.013174781789545153\n",
      "Test loss: 0.04145997528124739\n",
      "Starting epoch 790\n",
      "Training loss: 0.013063283881447355\n",
      "Test loss: 0.04118040818031187\n",
      "Starting epoch 791\n",
      "Training loss: 0.0131410009487242\n",
      "Test loss: 0.04130193839470545\n",
      "Starting epoch 792\n",
      "Training loss: 0.01305944076357562\n",
      "Test loss: 0.04111108887526724\n",
      "Starting epoch 793\n",
      "Training loss: 0.01316212502415063\n",
      "Test loss: 0.04118362841782747\n",
      "Starting epoch 794\n",
      "Training loss: 0.01302259619973722\n",
      "Test loss: 0.04175418408380614\n",
      "Starting epoch 795\n",
      "Training loss: 0.013171389576841573\n",
      "Test loss: 0.041902060310045876\n",
      "Starting epoch 796\n",
      "Training loss: 0.012970349400258456\n",
      "Test loss: 0.04113042464962712\n",
      "Starting epoch 797\n",
      "Training loss: 0.013011361548646551\n",
      "Test loss: 0.041045336328722815\n",
      "Starting epoch 798\n",
      "Training loss: 0.012996584856424664\n",
      "Test loss: 0.04153232055681723\n",
      "Starting epoch 799\n",
      "Training loss: 0.013000740165837476\n",
      "Test loss: 0.04128212150600222\n",
      "Starting epoch 800\n",
      "Training loss: 0.012992753738873318\n",
      "Test loss: 0.04093247810723605\n",
      "Starting epoch 801\n",
      "Training loss: 0.013184448078152586\n",
      "Test loss: 0.041290375162605884\n",
      "Starting epoch 802\n",
      "Training loss: 0.012891849670864519\n",
      "Test loss: 0.041698473471182364\n",
      "Starting epoch 803\n",
      "Training loss: 0.012851149393398254\n",
      "Test loss: 0.041611263528466225\n",
      "Starting epoch 804\n",
      "Training loss: 0.013112406612786114\n",
      "Test loss: 0.04126515956940474\n",
      "Starting epoch 805\n",
      "Training loss: 0.012895819150888528\n",
      "Test loss: 0.04073010012507439\n",
      "Starting epoch 806\n",
      "Training loss: 0.013314164915412176\n",
      "Test loss: 0.04103563525886447\n",
      "Starting epoch 807\n",
      "Training loss: 0.012847422225187059\n",
      "Test loss: 0.04070287528965208\n",
      "Starting epoch 808\n",
      "Training loss: 0.012776489125289878\n",
      "Test loss: 0.041185607098870806\n",
      "Starting epoch 809\n",
      "Training loss: 0.013010977446788648\n",
      "Test loss: 0.04149191164308124\n",
      "Starting epoch 810\n",
      "Training loss: 0.01293065253889463\n",
      "Test loss: 0.0420775314172109\n",
      "Starting epoch 811\n",
      "Training loss: 0.012929451758744286\n",
      "Test loss: 0.04233041212514595\n",
      "Starting epoch 812\n",
      "Training loss: 0.013011043723367277\n",
      "Test loss: 0.04189877615620693\n",
      "Starting epoch 813\n",
      "Training loss: 0.012772258340579564\n",
      "Test loss: 0.04179084356184359\n",
      "Starting epoch 814\n",
      "Training loss: 0.012822826652497541\n",
      "Test loss: 0.04135521390923747\n",
      "Starting epoch 815\n",
      "Training loss: 0.012787266802348074\n",
      "Test loss: 0.04155330332341017\n",
      "Starting epoch 816\n",
      "Training loss: 0.012703914653326645\n",
      "Test loss: 0.04151109054132744\n",
      "Starting epoch 817\n",
      "Training loss: 0.012778162528745463\n",
      "Test loss: 0.041412000026967793\n",
      "Starting epoch 818\n",
      "Training loss: 0.012675974869215097\n",
      "Test loss: 0.04142406113721706\n",
      "Starting epoch 819\n",
      "Training loss: 0.01266208262045364\n",
      "Test loss: 0.04133170060123558\n",
      "Starting epoch 820\n",
      "Training loss: 0.012939093848232363\n",
      "Test loss: 0.04141540797772231\n",
      "Starting epoch 821\n",
      "Training loss: 0.012605214430416217\n",
      "Test loss: 0.04097244350446595\n",
      "Starting epoch 822\n",
      "Training loss: 0.012735728189715596\n",
      "Test loss: 0.04139425571042078\n",
      "Starting epoch 823\n",
      "Training loss: 0.012667306027085077\n",
      "Test loss: 0.04159817651466087\n",
      "Starting epoch 824\n",
      "Training loss: 0.012736714735138611\n",
      "Test loss: 0.041593385584376474\n",
      "Starting epoch 825\n",
      "Training loss: 0.012706602114390154\n",
      "Test loss: 0.04127748269173834\n",
      "Starting epoch 826\n",
      "Training loss: 0.012590605154877803\n",
      "Test loss: 0.04112229385861644\n",
      "Starting epoch 827\n",
      "Training loss: 0.012595494919013782\n",
      "Test loss: 0.04152138434626438\n",
      "Starting epoch 828\n",
      "Training loss: 0.012528142510134666\n",
      "Test loss: 0.04188834724050981\n",
      "Starting epoch 829\n",
      "Training loss: 0.012672391659045806\n",
      "Test loss: 0.04159801867273119\n",
      "Starting epoch 830\n",
      "Training loss: 0.01255845945694896\n",
      "Test loss: 0.04124790706016399\n",
      "Starting epoch 831\n",
      "Training loss: 0.012572939584001165\n",
      "Test loss: 0.041383857390394914\n",
      "Starting epoch 832\n",
      "Training loss: 0.012541855120512306\n",
      "Test loss: 0.041630035611214464\n",
      "Starting epoch 833\n",
      "Training loss: 0.01288610186855324\n",
      "Test loss: 0.04174908388544012\n",
      "Starting epoch 834\n",
      "Training loss: 0.012504964746290544\n",
      "Test loss: 0.041308639778031245\n",
      "Starting epoch 835\n",
      "Training loss: 0.012685461534706295\n",
      "Test loss: 0.041472381049836124\n",
      "Starting epoch 836\n",
      "Training loss: 0.012537846250123665\n",
      "Test loss: 0.04121086001396179\n",
      "Starting epoch 837\n",
      "Training loss: 0.012565225530721124\n",
      "Test loss: 0.04153595561230624\n",
      "Starting epoch 838\n",
      "Training loss: 0.012454052074033706\n",
      "Test loss: 0.04159989759877876\n",
      "Starting epoch 839\n",
      "Training loss: 0.012476840941998803\n",
      "Test loss: 0.041781290261833755\n",
      "Starting epoch 840\n",
      "Training loss: 0.012433461386893616\n",
      "Test loss: 0.04184040054678917\n",
      "Starting epoch 841\n",
      "Training loss: 0.012447005382082502\n",
      "Test loss: 0.0417190116174795\n",
      "Starting epoch 842\n",
      "Training loss: 0.01243996588116298\n",
      "Test loss: 0.041725980324877635\n",
      "Starting epoch 843\n",
      "Training loss: 0.012429788632348913\n",
      "Test loss: 0.04150167894032267\n",
      "Starting epoch 844\n",
      "Training loss: 0.012452983480618626\n",
      "Test loss: 0.041524521040695685\n",
      "Starting epoch 845\n",
      "Training loss: 0.012406089648482253\n",
      "Test loss: 0.04167237491519363\n",
      "Starting epoch 846\n",
      "Training loss: 0.012960820168745323\n",
      "Test loss: 0.04183895651389052\n",
      "Starting epoch 847\n",
      "Training loss: 0.012392834622840413\n",
      "Test loss: 0.04275212616280273\n",
      "Starting epoch 848\n",
      "Training loss: 0.012497024931257865\n",
      "Test loss: 0.04221369960793742\n",
      "Starting epoch 849\n",
      "Training loss: 0.012337872453156064\n",
      "Test loss: 0.04152021101779408\n",
      "Starting epoch 850\n",
      "Training loss: 0.012452602691826273\n",
      "Test loss: 0.04166132853262954\n",
      "Starting epoch 851\n",
      "Training loss: 0.01246946234805662\n",
      "Test loss: 0.04218761374553045\n",
      "Starting epoch 852\n",
      "Training loss: 0.01262431939850088\n",
      "Test loss: 0.04187686900021853\n",
      "Starting epoch 853\n",
      "Training loss: 0.012460650570812772\n",
      "Test loss: 0.04210516410293402\n",
      "Starting epoch 854\n",
      "Training loss: 0.012311959081924841\n",
      "Test loss: 0.04168080124590132\n",
      "Starting epoch 855\n",
      "Training loss: 0.012594575856308469\n",
      "Test loss: 0.04157389852183836\n",
      "Starting epoch 856\n",
      "Training loss: 0.01228854660189054\n",
      "Test loss: 0.04245799924764368\n",
      "Starting epoch 857\n",
      "Training loss: 0.01236394346981752\n",
      "Test loss: 0.04232561167467524\n",
      "Starting epoch 858\n",
      "Training loss: 0.0122919267532034\n",
      "Test loss: 0.04203501067779682\n",
      "Starting epoch 859\n",
      "Training loss: 0.012237020493408695\n",
      "Test loss: 0.04192297546951859\n",
      "Starting epoch 860\n",
      "Training loss: 0.012294152385142983\n",
      "Test loss: 0.04201369663631475\n",
      "Starting epoch 861\n",
      "Training loss: 0.012273545086872383\n",
      "Test loss: 0.041874532484345965\n",
      "Starting epoch 862\n",
      "Training loss: 0.012759248726069927\n",
      "Test loss: 0.04222495840103538\n",
      "Starting epoch 863\n",
      "Training loss: 0.012680448194751974\n",
      "Test loss: 0.04304396540478424\n",
      "Starting epoch 864\n",
      "Training loss: 0.012210665316488898\n",
      "Test loss: 0.04184502797822157\n",
      "Starting epoch 865\n",
      "Training loss: 0.012263481581553083\n",
      "Test loss: 0.041476807039644986\n",
      "Starting epoch 866\n",
      "Training loss: 0.012257662739177218\n",
      "Test loss: 0.042044333561703014\n",
      "Starting epoch 867\n",
      "Training loss: 0.012346794492885714\n",
      "Test loss: 0.04170873292066433\n",
      "Starting epoch 868\n",
      "Training loss: 0.012174003635395746\n",
      "Test loss: 0.04131931701192149\n",
      "Starting epoch 869\n",
      "Training loss: 0.012169685947601913\n",
      "Test loss: 0.041633580845815164\n",
      "Starting epoch 870\n",
      "Training loss: 0.012609126351651598\n",
      "Test loss: 0.04193632387452655\n",
      "Starting epoch 871\n",
      "Training loss: 0.012176422799219851\n",
      "Test loss: 0.04293103667872923\n",
      "Starting epoch 872\n",
      "Training loss: 0.012627978030531133\n",
      "Test loss: 0.04245486000069865\n",
      "Starting epoch 873\n",
      "Training loss: 0.012764262921008908\n",
      "Test loss: 0.04236155403433023\n",
      "Starting epoch 874\n",
      "Training loss: 0.01239970460778377\n",
      "Test loss: 0.041529231118383234\n",
      "Starting epoch 875\n",
      "Training loss: 0.012790179674009809\n",
      "Test loss: 0.04214501022188752\n",
      "Starting epoch 876\n",
      "Training loss: 0.012155219149149831\n",
      "Test loss: 0.041578554168895436\n",
      "Starting epoch 877\n",
      "Training loss: 0.01229890690902706\n",
      "Test loss: 0.04185798936695964\n",
      "Starting epoch 878\n",
      "Training loss: 0.012121180438848793\n",
      "Test loss: 0.04169461365651201\n",
      "Starting epoch 879\n",
      "Training loss: 0.012187657282367105\n",
      "Test loss: 0.04173804598825949\n",
      "Starting epoch 880\n",
      "Training loss: 0.012073091895426394\n",
      "Test loss: 0.0419245413677008\n",
      "Starting epoch 881\n",
      "Training loss: 0.012114496290928027\n",
      "Test loss: 0.041736191207612\n",
      "Starting epoch 882\n",
      "Training loss: 0.012098161289926435\n",
      "Test loss: 0.04228441403419883\n",
      "Starting epoch 883\n",
      "Training loss: 0.012041425927862769\n",
      "Test loss: 0.0421041419936551\n",
      "Starting epoch 884\n",
      "Training loss: 0.012155990803339442\n",
      "Test loss: 0.04196444898843765\n",
      "Starting epoch 885\n",
      "Training loss: 0.01218057106264302\n",
      "Test loss: 0.0425263966123263\n",
      "Starting epoch 886\n",
      "Training loss: 0.012115144812059208\n",
      "Test loss: 0.041994589277439646\n",
      "Starting epoch 887\n",
      "Training loss: 0.012054327417348252\n",
      "Test loss: 0.041962830556763545\n",
      "Starting epoch 888\n",
      "Training loss: 0.01208698399151202\n",
      "Test loss: 0.04248398321646231\n",
      "Starting epoch 889\n",
      "Training loss: 0.011990207048957466\n",
      "Test loss: 0.04214297166025197\n",
      "Starting epoch 890\n",
      "Training loss: 0.011961045919261018\n",
      "Test loss: 0.0423529600655591\n",
      "Starting epoch 891\n",
      "Training loss: 0.01195836555762369\n",
      "Test loss: 0.04232338912509106\n",
      "Starting epoch 892\n",
      "Training loss: 0.012211770254385764\n",
      "Test loss: 0.04243255744653719\n",
      "Starting epoch 893\n",
      "Training loss: 0.012189910273815765\n",
      "Test loss: 0.04179142926026274\n",
      "Starting epoch 894\n",
      "Training loss: 0.01194219207238467\n",
      "Test loss: 0.04244096232233224\n",
      "Starting epoch 895\n",
      "Training loss: 0.012170587864811303\n",
      "Test loss: 0.04250763225610609\n",
      "Starting epoch 896\n",
      "Training loss: 0.011999610689331273\n",
      "Test loss: 0.04287529943717851\n",
      "Starting epoch 897\n",
      "Training loss: 0.01198112955469577\n",
      "Test loss: 0.042345255337379595\n",
      "Starting epoch 898\n",
      "Training loss: 0.011878655276826171\n",
      "Test loss: 0.04202108036864687\n",
      "Starting epoch 899\n",
      "Training loss: 0.011892676674073836\n",
      "Test loss: 0.04209258241785897\n",
      "Starting epoch 900\n",
      "Training loss: 0.012099186202786008\n",
      "Test loss: 0.04218109241790242\n",
      "Starting epoch 901\n",
      "Training loss: 0.011876659330408105\n",
      "Test loss: 0.04177505509169013\n",
      "Starting epoch 902\n",
      "Training loss: 0.011858710087835789\n",
      "Test loss: 0.041958057356101496\n",
      "Starting epoch 903\n",
      "Training loss: 0.012086978777632361\n",
      "Test loss: 0.042135509489863006\n",
      "Starting epoch 904\n",
      "Training loss: 0.011903637455257236\n",
      "Test loss: 0.04161445051431656\n",
      "Starting epoch 905\n",
      "Training loss: 0.011826281084633265\n",
      "Test loss: 0.04173510507852943\n",
      "Starting epoch 906\n",
      "Training loss: 0.01185574029862392\n",
      "Test loss: 0.04199905859099494\n",
      "Starting epoch 907\n",
      "Training loss: 0.011849316340855887\n",
      "Test loss: 0.04233515259154417\n",
      "Starting epoch 908\n",
      "Training loss: 0.011842119538026755\n",
      "Test loss: 0.04228893136260686\n",
      "Starting epoch 909\n",
      "Training loss: 0.011917021217160538\n",
      "Test loss: 0.04228308258785142\n",
      "Starting epoch 910\n",
      "Training loss: 0.011881989671192209\n",
      "Test loss: 0.04244481378959285\n",
      "Starting epoch 911\n",
      "Training loss: 0.011883095303764109\n",
      "Test loss: 0.0426327275733153\n",
      "Starting epoch 912\n",
      "Training loss: 0.011868203287852592\n",
      "Test loss: 0.04284244334256208\n",
      "Starting epoch 913\n",
      "Training loss: 0.011829852462425584\n",
      "Test loss: 0.042345949138204254\n",
      "Starting epoch 914\n",
      "Training loss: 0.011783872990578901\n",
      "Test loss: 0.04241501636527203\n",
      "Starting epoch 915\n",
      "Training loss: 0.011860336268656567\n",
      "Test loss: 0.04226512461900711\n",
      "Starting epoch 916\n",
      "Training loss: 0.01194807960361731\n",
      "Test loss: 0.042380579505805614\n",
      "Starting epoch 917\n",
      "Training loss: 0.011781034899539635\n",
      "Test loss: 0.042809496737188764\n",
      "Starting epoch 918\n",
      "Training loss: 0.012139305060149217\n",
      "Test loss: 0.042416177276108\n",
      "Starting epoch 919\n",
      "Training loss: 0.011738300018134664\n",
      "Test loss: 0.041836712371419976\n",
      "Starting epoch 920\n",
      "Training loss: 0.011962418520792585\n",
      "Test loss: 0.041922056288630875\n",
      "Starting epoch 921\n",
      "Training loss: 0.011807114343907013\n",
      "Test loss: 0.04275862748424212\n",
      "Starting epoch 922\n",
      "Training loss: 0.011791814454510564\n",
      "Test loss: 0.04258955987515273\n",
      "Starting epoch 923\n",
      "Training loss: 0.011758800137971268\n",
      "Test loss: 0.042706114412457856\n",
      "Starting epoch 924\n",
      "Training loss: 0.011885380250264387\n",
      "Test loss: 0.04285910063319736\n",
      "Starting epoch 925\n",
      "Training loss: 0.011828684874001096\n",
      "Test loss: 0.04211410700722977\n",
      "Starting epoch 926\n",
      "Training loss: 0.01173227382671149\n",
      "Test loss: 0.04234558199014929\n",
      "Starting epoch 927\n",
      "Training loss: 0.011854294763847452\n",
      "Test loss: 0.042436451064767663\n",
      "Starting epoch 928\n",
      "Training loss: 0.011804220480386351\n",
      "Test loss: 0.04291344558199247\n",
      "Starting epoch 929\n",
      "Training loss: 0.011917882385190393\n",
      "Test loss: 0.042832417965487195\n",
      "Starting epoch 930\n",
      "Training loss: 0.0116753861705055\n",
      "Test loss: 0.04231199332409435\n",
      "Starting epoch 931\n",
      "Training loss: 0.01171776931732893\n",
      "Test loss: 0.0424226118872563\n",
      "Starting epoch 932\n",
      "Training loss: 0.011766560681042124\n",
      "Test loss: 0.042494254255736316\n",
      "Starting epoch 933\n",
      "Training loss: 0.011860203929245472\n",
      "Test loss: 0.042671967021845\n",
      "Starting epoch 934\n",
      "Training loss: 0.011898945803280736\n",
      "Test loss: 0.04265910221470727\n",
      "Starting epoch 935\n",
      "Training loss: 0.011882406552551223\n",
      "Test loss: 0.042235533948297856\n",
      "Starting epoch 936\n",
      "Training loss: 0.011874825800540017\n",
      "Test loss: 0.042868205725594806\n",
      "Starting epoch 937\n",
      "Training loss: 0.011713403322901881\n",
      "Test loss: 0.0423782832092709\n",
      "Starting epoch 938\n",
      "Training loss: 0.011641547786163503\n",
      "Test loss: 0.042610091191750986\n",
      "Starting epoch 939\n",
      "Training loss: 0.011761814661201884\n",
      "Test loss: 0.04276230572550385\n",
      "Starting epoch 940\n",
      "Training loss: 0.011733334145096481\n",
      "Test loss: 0.04302399812473191\n",
      "Starting epoch 941\n",
      "Training loss: 0.011746579117034792\n",
      "Test loss: 0.04246661646498574\n",
      "Starting epoch 942\n",
      "Training loss: 0.011695676590086983\n",
      "Test loss: 0.04265345067337707\n",
      "Starting epoch 943\n",
      "Training loss: 0.011660545087251508\n",
      "Test loss: 0.04291909036261064\n",
      "Starting epoch 944\n",
      "Training loss: 0.011725316908149446\n",
      "Test loss: 0.04303919910280793\n",
      "Starting epoch 945\n",
      "Training loss: 0.012015081117631958\n",
      "Test loss: 0.042561998383866415\n",
      "Starting epoch 946\n",
      "Training loss: 0.011662737374789402\n",
      "Test loss: 0.04204509572850333\n",
      "Starting epoch 947\n",
      "Training loss: 0.011673014000302455\n",
      "Test loss: 0.04256870190578478\n",
      "Starting epoch 948\n",
      "Training loss: 0.011649860542450772\n",
      "Test loss: 0.04290118827312081\n",
      "Starting epoch 949\n",
      "Training loss: 0.011579426997875581\n",
      "Test loss: 0.04296208600755091\n",
      "Starting epoch 950\n",
      "Training loss: 0.01170990486308688\n",
      "Test loss: 0.04278507207830747\n",
      "Starting epoch 951\n",
      "Training loss: 0.011615444356422932\n",
      "Test loss: 0.04289926919672224\n",
      "Starting epoch 952\n",
      "Training loss: 0.011723317892947158\n",
      "Test loss: 0.04254661610832921\n",
      "Starting epoch 953\n",
      "Training loss: 0.011812700111357892\n",
      "Test loss: 0.0421351522759155\n",
      "Starting epoch 954\n",
      "Training loss: 0.011601875459805865\n",
      "Test loss: 0.042820326432033824\n",
      "Starting epoch 955\n",
      "Training loss: 0.011853042226590093\n",
      "Test loss: 0.04285413992625696\n",
      "Starting epoch 956\n",
      "Training loss: 0.011528683108750914\n",
      "Test loss: 0.042272249374676635\n",
      "Starting epoch 957\n",
      "Training loss: 0.011554818508810684\n",
      "Test loss: 0.04233693731603799\n",
      "Starting epoch 958\n",
      "Training loss: 0.011547634988778928\n",
      "Test loss: 0.04252948073877229\n",
      "Starting epoch 959\n",
      "Training loss: 0.011607968981270908\n",
      "Test loss: 0.04285207928882705\n",
      "Starting epoch 960\n",
      "Training loss: 0.011596398702898964\n",
      "Test loss: 0.04291677661240101\n",
      "Starting epoch 961\n",
      "Training loss: 0.011539511916945215\n",
      "Test loss: 0.042864522241331915\n",
      "Starting epoch 962\n",
      "Training loss: 0.011559171067764525\n",
      "Test loss: 0.042609006304431846\n",
      "Starting epoch 963\n",
      "Training loss: 0.011702306003722011\n",
      "Test loss: 0.04277636566095882\n",
      "Starting epoch 964\n",
      "Training loss: 0.011502302282291358\n",
      "Test loss: 0.04230974104117464\n",
      "Starting epoch 965\n",
      "Training loss: 0.011663399606210286\n",
      "Test loss: 0.0423430012608016\n",
      "Starting epoch 966\n",
      "Training loss: 0.01158600598268333\n",
      "Test loss: 0.04273540871562781\n",
      "Starting epoch 967\n",
      "Training loss: 0.011531546498175527\n",
      "Test loss: 0.04247461369744054\n",
      "Starting epoch 968\n",
      "Training loss: 0.011569858559208815\n",
      "Test loss: 0.04264748455197723\n",
      "Starting epoch 969\n",
      "Training loss: 0.011625372392476583\n",
      "Test loss: 0.04292738134110415\n",
      "Starting epoch 970\n",
      "Training loss: 0.011595454494484136\n",
      "Test loss: 0.04241821804532298\n",
      "Starting epoch 971\n",
      "Training loss: 0.011642670335217577\n",
      "Test loss: 0.042779242136964095\n",
      "Starting epoch 972\n",
      "Training loss: 0.011503331699087972\n",
      "Test loss: 0.042418109666970044\n",
      "Starting epoch 973\n",
      "Training loss: 0.01149146368757623\n",
      "Test loss: 0.04224981960875017\n",
      "Starting epoch 974\n",
      "Training loss: 0.011497667639470492\n",
      "Test loss: 0.042318848686085805\n",
      "Starting epoch 975\n",
      "Training loss: 0.01166437911327745\n",
      "Test loss: 0.04253873146242566\n",
      "Starting epoch 976\n",
      "Training loss: 0.011471904814243317\n",
      "Test loss: 0.04307088518032321\n",
      "Starting epoch 977\n",
      "Training loss: 0.011457123244028599\n",
      "Test loss: 0.04284655302762985\n",
      "Starting epoch 978\n",
      "Training loss: 0.01141665202611294\n",
      "Test loss: 0.042655976144252\n",
      "Starting epoch 979\n",
      "Training loss: 0.011480769081438174\n",
      "Test loss: 0.04246082977840194\n",
      "Starting epoch 980\n",
      "Training loss: 0.01138805480460163\n",
      "Test loss: 0.04264570296638542\n",
      "Starting epoch 981\n",
      "Training loss: 0.011428858382535762\n",
      "Test loss: 0.042899888974648935\n",
      "Starting epoch 982\n",
      "Training loss: 0.011545770786336212\n",
      "Test loss: 0.042772622847998584\n",
      "Starting epoch 983\n",
      "Training loss: 0.011504794776317526\n",
      "Test loss: 0.04310813918709755\n",
      "Starting epoch 984\n",
      "Training loss: 0.011447869256505223\n",
      "Test loss: 0.042733523856710504\n",
      "Starting epoch 985\n",
      "Training loss: 0.011415841676233733\n",
      "Test loss: 0.042992884638132875\n",
      "Starting epoch 986\n",
      "Training loss: 0.011426610414121971\n",
      "Test loss: 0.042730542796629446\n",
      "Starting epoch 987\n",
      "Training loss: 0.011404956446685752\n",
      "Test loss: 0.04258974910610252\n",
      "Starting epoch 988\n",
      "Training loss: 0.011433094327689195\n",
      "Test loss: 0.04274265951028577\n",
      "Starting epoch 989\n",
      "Training loss: 0.011413437909767276\n",
      "Test loss: 0.042710457066143\n",
      "Starting epoch 990\n",
      "Training loss: 0.011483303652923615\n",
      "Test loss: 0.04261900636333006\n",
      "Starting epoch 991\n",
      "Training loss: 0.011454682751390778\n",
      "Test loss: 0.04287509054497436\n",
      "Starting epoch 992\n",
      "Training loss: 0.011572077229130463\n",
      "Test loss: 0.04308020958194026\n",
      "Starting epoch 993\n",
      "Training loss: 0.011868661635967552\n",
      "Test loss: 0.04276535315094171\n",
      "Starting epoch 994\n",
      "Training loss: 0.011381985741804858\n",
      "Test loss: 0.04335081936032684\n",
      "Starting epoch 995\n",
      "Training loss: 0.011411613540449103\n",
      "Test loss: 0.04320450554843302\n",
      "Starting epoch 996\n",
      "Training loss: 0.011419641770056038\n",
      "Test loss: 0.04292232453547142\n",
      "Starting epoch 997\n",
      "Training loss: 0.01142649134224067\n",
      "Test loss: 0.0425562237009958\n",
      "Starting epoch 998\n",
      "Training loss: 0.011424242289828473\n",
      "Test loss: 0.04312770661932451\n",
      "Starting epoch 999\n",
      "Training loss: 0.011451411564819148\n",
      "Test loss: 0.043136802674443635\n",
      "Starting epoch 1000\n",
      "Training loss: 0.011345624526748891\n",
      "Test loss: 0.0430168290105131\n",
      "Starting epoch 1001\n",
      "Training loss: 0.011370029590535359\n",
      "Test loss: 0.042959467887326526\n",
      "Starting epoch 1002\n",
      "Training loss: 0.011357680344801457\n",
      "Test loss: 0.042855017439082814\n",
      "Starting epoch 1003\n",
      "Training loss: 0.011452151889928052\n",
      "Test loss: 0.04259134649678513\n",
      "Starting epoch 1004\n",
      "Training loss: 0.011403915670807244\n",
      "Test loss: 0.0429041827166522\n",
      "Starting epoch 1005\n",
      "Training loss: 0.011422592291577917\n",
      "Test loss: 0.04283832179175483\n",
      "Starting epoch 1006\n",
      "Training loss: 0.011287305656759465\n",
      "Test loss: 0.043046103721415555\n",
      "Starting epoch 1007\n",
      "Training loss: 0.011517497191785788\n",
      "Test loss: 0.04310982605373418\n",
      "Starting epoch 1008\n",
      "Training loss: 0.01129369434641033\n",
      "Test loss: 0.04320048513235869\n",
      "Starting epoch 1009\n",
      "Training loss: 0.011275160050050158\n",
      "Test loss: 0.04295617693828212\n",
      "Starting epoch 1010\n",
      "Training loss: 0.011440825633338241\n",
      "Test loss: 0.042790209115655335\n",
      "Starting epoch 1011\n",
      "Training loss: 0.01142021508307242\n",
      "Test loss: 0.043093458783847315\n",
      "Starting epoch 1012\n",
      "Training loss: 0.01133422074137164\n",
      "Test loss: 0.042862757902454446\n",
      "Starting epoch 1013\n",
      "Training loss: 0.011362733914837485\n",
      "Test loss: 0.042582415458228856\n",
      "Starting epoch 1014\n",
      "Training loss: 0.01128494454028665\n",
      "Test loss: 0.042731256120734744\n",
      "Starting epoch 1015\n",
      "Training loss: 0.011336449121476197\n",
      "Test loss: 0.04305233247578144\n",
      "Starting epoch 1016\n",
      "Training loss: 0.011330629989016251\n",
      "Test loss: 0.04319163370463583\n",
      "Starting epoch 1017\n",
      "Training loss: 0.011336951234118372\n",
      "Test loss: 0.0432368240284699\n",
      "Starting epoch 1018\n",
      "Training loss: 0.011274078433386615\n",
      "Test loss: 0.04327715402124105\n",
      "Starting epoch 1019\n",
      "Training loss: 0.011401070449806627\n",
      "Test loss: 0.042958227434643996\n",
      "Starting epoch 1020\n",
      "Training loss: 0.011245774906739349\n",
      "Test loss: 0.04256930767937943\n",
      "Starting epoch 1021\n",
      "Training loss: 0.011339099085355391\n",
      "Test loss: 0.042533789933831605\n",
      "Starting epoch 1022\n",
      "Training loss: 0.0112500771361052\n",
      "Test loss: 0.04314710682740918\n",
      "Starting epoch 1023\n",
      "Training loss: 0.011234999298438674\n",
      "Test loss: 0.043391855226622686\n",
      "Starting epoch 1024\n",
      "Training loss: 0.01126878948302054\n",
      "Test loss: 0.04328835976344568\n",
      "Starting epoch 1025\n",
      "Training loss: 0.011559294551977368\n",
      "Test loss: 0.04277122448439951\n",
      "Starting epoch 1026\n",
      "Training loss: 0.01132412445655123\n",
      "Test loss: 0.042674499529379385\n",
      "Starting epoch 1027\n",
      "Training loss: 0.011379384603656706\n",
      "Test loss: 0.0432403150394007\n",
      "Starting epoch 1028\n",
      "Training loss: 0.011360820352298314\n",
      "Test loss: 0.04296151941849126\n",
      "Starting epoch 1029\n",
      "Training loss: 0.011291071864180878\n",
      "Test loss: 0.04335698443982336\n",
      "Starting epoch 1030\n",
      "Training loss: 0.01127387253475971\n",
      "Test loss: 0.04346918431973016\n",
      "Starting epoch 1031\n",
      "Training loss: 0.01124785916849238\n",
      "Test loss: 0.04357703333651578\n",
      "Starting epoch 1032\n",
      "Training loss: 0.011365098558122018\n",
      "Test loss: 0.04318136901215271\n",
      "Starting epoch 1033\n",
      "Training loss: 0.011279919062603692\n",
      "Test loss: 0.04280112849341498\n",
      "Starting epoch 1034\n",
      "Training loss: 0.011504812532516777\n",
      "Test loss: 0.042974789523416095\n",
      "Starting epoch 1035\n",
      "Training loss: 0.011227891536154708\n",
      "Test loss: 0.04277172477708922\n",
      "Starting epoch 1036\n",
      "Training loss: 0.011255849122268255\n",
      "Test loss: 0.04293536712174063\n",
      "Starting epoch 1037\n",
      "Training loss: 0.01120792124725756\n",
      "Test loss: 0.04297825918291454\n",
      "Starting epoch 1038\n",
      "Training loss: 0.011247998973751654\n",
      "Test loss: 0.042918121373211895\n",
      "Starting epoch 1039\n",
      "Training loss: 0.011293078375766511\n",
      "Test loss: 0.04274003690591565\n",
      "Starting epoch 1040\n",
      "Training loss: 0.011228829660437634\n",
      "Test loss: 0.042746050076352224\n",
      "Starting epoch 1041\n",
      "Training loss: 0.011312343241249929\n",
      "Test loss: 0.04267390210319449\n",
      "Starting epoch 1042\n",
      "Training loss: 0.011372304139811485\n",
      "Test loss: 0.043410188208023705\n",
      "Starting epoch 1043\n",
      "Training loss: 0.01122460274605966\n",
      "Test loss: 0.042878558238347374\n",
      "Starting epoch 1044\n",
      "Training loss: 0.011259932315251867\n",
      "Test loss: 0.042792673188227194\n",
      "Starting epoch 1045\n",
      "Training loss: 0.011240010256650017\n",
      "Test loss: 0.042571468347752536\n",
      "Starting epoch 1046\n",
      "Training loss: 0.011200369557090958\n",
      "Test loss: 0.04282359599515244\n",
      "Starting epoch 1047\n",
      "Training loss: 0.011197396813601743\n",
      "Test loss: 0.042970407754182816\n",
      "Starting epoch 1048\n",
      "Training loss: 0.011337226119319924\n",
      "Test loss: 0.04327618192743372\n",
      "Starting epoch 1049\n",
      "Training loss: 0.011535024881118634\n",
      "Test loss: 0.04318022617587337\n",
      "Starting epoch 1050\n",
      "Training loss: 0.011521899828412493\n",
      "Test loss: 0.0439907677333664\n",
      "Starting epoch 1051\n",
      "Training loss: 0.011378201976662776\n",
      "Test loss: 0.043050912105374865\n",
      "Starting epoch 1052\n",
      "Training loss: 0.011277999560974661\n",
      "Test loss: 0.04352240761121114\n",
      "Starting epoch 1053\n",
      "Training loss: 0.011191552001066873\n",
      "Test loss: 0.04391911280927835\n",
      "Starting epoch 1054\n",
      "Training loss: 0.011118964246306264\n",
      "Test loss: 0.043534931623273425\n",
      "Starting epoch 1055\n",
      "Training loss: 0.011258274835885549\n",
      "Test loss: 0.04338089328397204\n",
      "Starting epoch 1056\n",
      "Training loss: 0.011169726731347257\n",
      "Test loss: 0.0433681678302862\n",
      "Starting epoch 1057\n",
      "Training loss: 0.011276880553999885\n",
      "Test loss: 0.04351868052725439\n",
      "Starting epoch 1058\n",
      "Training loss: 0.011272778184931786\n",
      "Test loss: 0.043583486091207574\n",
      "Starting epoch 1059\n",
      "Training loss: 0.011420969225344111\n",
      "Test loss: 0.04288496353008129\n",
      "Starting epoch 1060\n",
      "Training loss: 0.011165398005090777\n",
      "Test loss: 0.043334327500175546\n",
      "Starting epoch 1061\n",
      "Training loss: 0.011111862255168742\n",
      "Test loss: 0.043222165976961456\n",
      "Starting epoch 1062\n",
      "Training loss: 0.011171743411143294\n",
      "Test loss: 0.0430730321203117\n",
      "Starting epoch 1063\n",
      "Training loss: 0.011090029137911366\n",
      "Test loss: 0.0434063587475706\n",
      "Starting epoch 1064\n",
      "Training loss: 0.011148478026639243\n",
      "Test loss: 0.04343175529329865\n",
      "Starting epoch 1065\n",
      "Training loss: 0.011131870606150783\n",
      "Test loss: 0.04367419553023798\n",
      "Starting epoch 1066\n",
      "Training loss: 0.011170280547659905\n",
      "Test loss: 0.04348237577963759\n",
      "Starting epoch 1067\n",
      "Training loss: 0.011061280515411349\n",
      "Test loss: 0.04366403687055464\n",
      "Starting epoch 1068\n",
      "Training loss: 0.011195525801816925\n",
      "Test loss: 0.04341003133191003\n",
      "Starting epoch 1069\n",
      "Training loss: 0.011138963986371384\n",
      "Test loss: 0.04308649869980635\n",
      "Starting epoch 1070\n",
      "Training loss: 0.011276385914839681\n",
      "Test loss: 0.04350044454137484\n",
      "Starting epoch 1071\n",
      "Training loss: 0.011082651460024177\n",
      "Test loss: 0.04311381894405241\n",
      "Starting epoch 1072\n",
      "Training loss: 0.011291112475952164\n",
      "Test loss: 0.0431104171331282\n",
      "Starting epoch 1073\n",
      "Training loss: 0.01110200886232931\n",
      "Test loss: 0.04369884922548577\n",
      "Starting epoch 1074\n",
      "Training loss: 0.011146009655394515\n",
      "Test loss: 0.04354118942110627\n",
      "Starting epoch 1075\n",
      "Training loss: 0.011176600884340826\n",
      "Test loss: 0.0429949753538326\n",
      "Starting epoch 1076\n",
      "Training loss: 0.011164697864260829\n",
      "Test loss: 0.042885535017207814\n",
      "Starting epoch 1077\n",
      "Training loss: 0.01108272956897978\n",
      "Test loss: 0.04315105742878384\n",
      "Starting epoch 1078\n",
      "Training loss: 0.011148279181635771\n",
      "Test loss: 0.043314335591815134\n",
      "Starting epoch 1079\n",
      "Training loss: 0.011160870250619825\n",
      "Test loss: 0.043018228408915025\n",
      "Starting epoch 1080\n",
      "Training loss: 0.011068506380085085\n",
      "Test loss: 0.04316589691572719\n",
      "Starting epoch 1081\n",
      "Training loss: 0.011514717453449477\n",
      "Test loss: 0.04354993999004364\n",
      "Starting epoch 1082\n",
      "Training loss: 0.011378670431917808\n",
      "Test loss: 0.04410739691445121\n",
      "Starting epoch 1083\n",
      "Training loss: 0.011171265596859768\n",
      "Test loss: 0.043325597489321674\n",
      "Starting epoch 1084\n",
      "Training loss: 0.011086702652153422\n",
      "Test loss: 0.043442000807435426\n",
      "Starting epoch 1085\n",
      "Training loss: 0.01126303813862996\n",
      "Test loss: 0.04369390169503512\n",
      "Starting epoch 1086\n",
      "Training loss: 0.0110451573750279\n",
      "Test loss: 0.04413739495255329\n",
      "Starting epoch 1087\n",
      "Training loss: 0.011100200798789987\n",
      "Test loss: 0.04389134863460505\n",
      "Starting epoch 1088\n",
      "Training loss: 0.011049139572948705\n",
      "Test loss: 0.04355452416671647\n",
      "Starting epoch 1089\n",
      "Training loss: 0.011051525545046955\n",
      "Test loss: 0.04363144396079911\n",
      "Starting epoch 1090\n",
      "Training loss: 0.011020841993025093\n",
      "Test loss: 0.043413827678671586\n",
      "Starting epoch 1091\n",
      "Training loss: 0.011047213261977572\n",
      "Test loss: 0.04328352212905884\n",
      "Starting epoch 1092\n",
      "Training loss: 0.01104929918026338\n",
      "Test loss: 0.04347474969647549\n",
      "Starting epoch 1093\n",
      "Training loss: 0.011283192753058965\n",
      "Test loss: 0.04342221454889686\n",
      "Starting epoch 1094\n",
      "Training loss: 0.011189900506593164\n",
      "Test loss: 0.044022219669487744\n",
      "Starting epoch 1095\n",
      "Training loss: 0.011039773925955667\n",
      "Test loss: 0.043512350845116156\n",
      "Starting epoch 1096\n",
      "Training loss: 0.011158017227884199\n",
      "Test loss: 0.04319138273044869\n",
      "Starting epoch 1097\n",
      "Training loss: 0.011045325181034744\n",
      "Test loss: 0.043743554088804454\n",
      "Starting epoch 1098\n",
      "Training loss: 0.011111067104168603\n",
      "Test loss: 0.04388820904272574\n",
      "Starting epoch 1099\n",
      "Training loss: 0.011044712309710315\n",
      "Test loss: 0.043405810440028156\n",
      "Starting epoch 1100\n",
      "Training loss: 0.011097463000504697\n",
      "Test loss: 0.04323147416666702\n",
      "Starting epoch 1101\n",
      "Training loss: 0.01105609328531828\n",
      "Test loss: 0.043279731163272155\n",
      "Starting epoch 1102\n",
      "Training loss: 0.011136817318372062\n",
      "Test loss: 0.04348271077981702\n",
      "Starting epoch 1103\n",
      "Training loss: 0.011011224201895663\n",
      "Test loss: 0.04316587235640596\n",
      "Starting epoch 1104\n",
      "Training loss: 0.011040566916592786\n",
      "Test loss: 0.04369687040646871\n",
      "Starting epoch 1105\n",
      "Training loss: 0.0110043429571097\n",
      "Test loss: 0.04361890352986477\n",
      "Starting epoch 1106\n",
      "Training loss: 0.011360030560219874\n",
      "Test loss: 0.04361916692168624\n",
      "Starting epoch 1107\n",
      "Training loss: 0.011333503867270516\n",
      "Test loss: 0.04425133813034605\n",
      "Starting epoch 1108\n",
      "Training loss: 0.01113874483548227\n",
      "Test loss: 0.043198489480548434\n",
      "Starting epoch 1109\n",
      "Training loss: 0.011022923575317273\n",
      "Test loss: 0.0435152943763468\n",
      "Starting epoch 1110\n",
      "Training loss: 0.011190504385311096\n",
      "Test loss: 0.04356591720823889\n",
      "Starting epoch 1111\n",
      "Training loss: 0.011013208689992546\n",
      "Test loss: 0.043899042186913664\n",
      "Starting epoch 1112\n",
      "Training loss: 0.011161334629430145\n",
      "Test loss: 0.043716197074563416\n",
      "Starting epoch 1113\n",
      "Training loss: 0.010988688981923901\n",
      "Test loss: 0.04392481760846244\n",
      "Starting epoch 1114\n",
      "Training loss: 0.011007671030696298\n",
      "Test loss: 0.043556733815758315\n",
      "Starting epoch 1115\n",
      "Training loss: 0.01099105044955113\n",
      "Test loss: 0.043676661534441844\n",
      "Starting epoch 1116\n",
      "Training loss: 0.011052263366272215\n",
      "Test loss: 0.04365964620201676\n",
      "Starting epoch 1117\n",
      "Training loss: 0.011008195884403636\n",
      "Test loss: 0.04391640052199364\n",
      "Starting epoch 1118\n",
      "Training loss: 0.011116902404999147\n",
      "Test loss: 0.04353569130654688\n",
      "Starting epoch 1119\n",
      "Training loss: 0.010971154910741283\n",
      "Test loss: 0.04304023238795775\n",
      "Starting epoch 1120\n",
      "Training loss: 0.01104766912147647\n",
      "Test loss: 0.04352241540672602\n",
      "Starting epoch 1121\n",
      "Training loss: 0.011250434046397443\n",
      "Test loss: 0.04342054078976313\n",
      "Starting epoch 1122\n",
      "Training loss: 0.010980982547167872\n",
      "Test loss: 0.04281009899245368\n",
      "Starting epoch 1123\n",
      "Training loss: 0.011238242636938563\n",
      "Test loss: 0.04324173334020155\n",
      "Starting epoch 1124\n",
      "Training loss: 0.011042782319251631\n",
      "Test loss: 0.0430755166819802\n",
      "Starting epoch 1125\n",
      "Training loss: 0.01093642780038177\n",
      "Test loss: 0.04344671247182069\n",
      "Starting epoch 1126\n",
      "Training loss: 0.011051225384361431\n",
      "Test loss: 0.043814212635711385\n",
      "Starting epoch 1127\n",
      "Training loss: 0.010941483324668447\n",
      "Test loss: 0.044116604797266146\n",
      "Starting epoch 1128\n",
      "Training loss: 0.01114183317747761\n",
      "Test loss: 0.04384784510842076\n",
      "Starting epoch 1129\n",
      "Training loss: 0.0109914418493138\n",
      "Test loss: 0.043514472190980556\n",
      "Starting epoch 1130\n",
      "Training loss: 0.010981420955819185\n",
      "Test loss: 0.04356168472656497\n",
      "Starting epoch 1131\n",
      "Training loss: 0.011027295600439682\n",
      "Test loss: 0.04351901000848523\n",
      "Starting epoch 1132\n",
      "Training loss: 0.010963655107456153\n",
      "Test loss: 0.043716838514363324\n",
      "Starting epoch 1133\n",
      "Training loss: 0.011069164718272255\n",
      "Test loss: 0.04356217632691065\n",
      "Starting epoch 1134\n",
      "Training loss: 0.011036168264805293\n",
      "Test loss: 0.04330121849973997\n",
      "Starting epoch 1135\n",
      "Training loss: 0.011034996607569887\n",
      "Test loss: 0.0437033900094253\n",
      "Starting epoch 1136\n",
      "Training loss: 0.011040131309542988\n",
      "Test loss: 0.04363150066799588\n",
      "Starting epoch 1137\n",
      "Training loss: 0.011334625744550932\n",
      "Test loss: 0.043370979803579825\n",
      "Starting epoch 1138\n",
      "Training loss: 0.010956755014838742\n",
      "Test loss: 0.04424978778870017\n",
      "Starting epoch 1139\n",
      "Training loss: 0.010945826333749001\n",
      "Test loss: 0.04414664291673236\n",
      "Starting epoch 1140\n",
      "Training loss: 0.011012346728048364\n",
      "Test loss: 0.043824967134881904\n",
      "Starting epoch 1141\n",
      "Training loss: 0.010937146018030213\n",
      "Test loss: 0.04376306771128266\n",
      "Starting epoch 1142\n",
      "Training loss: 0.01098027232973302\n",
      "Test loss: 0.04372133369799013\n",
      "Starting epoch 1143\n",
      "Training loss: 0.010966855070752199\n",
      "Test loss: 0.04359325973523988\n",
      "Starting epoch 1144\n",
      "Training loss: 0.0109198678010067\n",
      "Test loss: 0.043902441307350444\n",
      "Starting epoch 1145\n",
      "Training loss: 0.010920009270432542\n",
      "Test loss: 0.04402239151574947\n",
      "Starting epoch 1146\n",
      "Training loss: 0.011037106030300016\n",
      "Test loss: 0.04396627822683917\n",
      "Starting epoch 1147\n",
      "Training loss: 0.010884676953075363\n",
      "Test loss: 0.04423859411919558\n",
      "Starting epoch 1148\n",
      "Training loss: 0.01098603055980362\n",
      "Test loss: 0.04418069634724547\n",
      "Starting epoch 1149\n",
      "Training loss: 0.011021635159239417\n",
      "Test loss: 0.04397082018355528\n",
      "Starting epoch 1150\n",
      "Training loss: 0.010958531734029778\n",
      "Test loss: 0.043912188736376936\n",
      "Starting epoch 1151\n",
      "Training loss: 0.011077312172436323\n",
      "Test loss: 0.0435637717169744\n",
      "Starting epoch 1152\n",
      "Training loss: 0.01093220985570892\n",
      "Test loss: 0.04351541151603063\n",
      "Starting epoch 1153\n",
      "Training loss: 0.01090933487857463\n",
      "Test loss: 0.04357510170450917\n",
      "Starting epoch 1154\n",
      "Training loss: 0.01091762016847974\n",
      "Test loss: 0.0438218482390598\n",
      "Starting epoch 1155\n",
      "Training loss: 0.010917240143066546\n",
      "Test loss: 0.04364974037916572\n",
      "Starting epoch 1156\n",
      "Training loss: 0.010871182516461513\n",
      "Test loss: 0.043518858237399\n",
      "Starting epoch 1157\n",
      "Training loss: 0.010931902580329628\n",
      "Test loss: 0.04360867926368007\n",
      "Starting epoch 1158\n",
      "Training loss: 0.010930133968225269\n",
      "Test loss: 0.04393647335193775\n",
      "Starting epoch 1159\n",
      "Training loss: 0.011050462539567322\n",
      "Test loss: 0.04442195435640989\n",
      "Starting epoch 1160\n",
      "Training loss: 0.010851412355044826\n",
      "Test loss: 0.04365793643174348\n",
      "Starting epoch 1161\n",
      "Training loss: 0.010917656932820062\n",
      "Test loss: 0.0434558032149518\n",
      "Starting epoch 1162\n",
      "Training loss: 0.010952830436776896\n",
      "Test loss: 0.04348086303582898\n",
      "Starting epoch 1163\n",
      "Training loss: 0.01086429298901167\n",
      "Test loss: 0.04400150132951913\n",
      "Starting epoch 1164\n",
      "Training loss: 0.010898749160839886\n",
      "Test loss: 0.043994215282577055\n",
      "Starting epoch 1165\n",
      "Training loss: 0.011081195100531226\n",
      "Test loss: 0.04417625124807711\n",
      "Starting epoch 1166\n",
      "Training loss: 0.010833113103127871\n",
      "Test loss: 0.04361481095353762\n",
      "Starting epoch 1167\n",
      "Training loss: 0.01100394073263055\n",
      "Test loss: 0.04365687596577185\n",
      "Starting epoch 1168\n",
      "Training loss: 0.010947615732667877\n",
      "Test loss: 0.043755348357889384\n",
      "Starting epoch 1169\n",
      "Training loss: 0.010982502206060731\n",
      "Test loss: 0.04367315175908583\n",
      "Starting epoch 1170\n",
      "Training loss: 0.010878451992986633\n",
      "Test loss: 0.04371656235997324\n",
      "Starting epoch 1171\n",
      "Training loss: 0.011235820908160483\n",
      "Test loss: 0.04369358056121402\n",
      "Starting epoch 1172\n",
      "Training loss: 0.010967796989029548\n",
      "Test loss: 0.043515272438526154\n",
      "Starting epoch 1173\n",
      "Training loss: 0.01085310795756637\n",
      "Test loss: 0.04373926421006521\n",
      "Starting epoch 1174\n",
      "Training loss: 0.011001382282644998\n",
      "Test loss: 0.04405228724634206\n",
      "Starting epoch 1175\n",
      "Training loss: 0.01092312797965085\n",
      "Test loss: 0.0442284068989533\n",
      "Starting epoch 1176\n",
      "Training loss: 0.010876163000576809\n",
      "Test loss: 0.044158838551353524\n",
      "Starting epoch 1177\n",
      "Training loss: 0.011146200576522311\n",
      "Test loss: 0.04384320118912944\n",
      "Starting epoch 1178\n",
      "Training loss: 0.010836013058414225\n",
      "Test loss: 0.043229944797025785\n",
      "Starting epoch 1179\n",
      "Training loss: 0.010903510341390234\n",
      "Test loss: 0.04341938995100834\n",
      "Starting epoch 1180\n",
      "Training loss: 0.010903885603317471\n",
      "Test loss: 0.04400263367979615\n",
      "Starting epoch 1181\n",
      "Training loss: 0.010932652676691775\n",
      "Test loss: 0.04416102853914102\n",
      "Starting epoch 1182\n",
      "Training loss: 0.010961084657150214\n",
      "Test loss: 0.04450495817043163\n",
      "Starting epoch 1183\n",
      "Training loss: 0.010957690948223481\n",
      "Test loss: 0.04443929703147323\n",
      "Starting epoch 1184\n",
      "Training loss: 0.010928327629922843\n",
      "Test loss: 0.043776078080689465\n",
      "Starting epoch 1185\n",
      "Training loss: 0.010828664388935098\n",
      "Test loss: 0.04400075806511773\n",
      "Starting epoch 1186\n",
      "Training loss: 0.010908124968409538\n",
      "Test loss: 0.044284694310691625\n",
      "Starting epoch 1187\n",
      "Training loss: 0.010831742318438703\n",
      "Test loss: 0.043781425803899765\n",
      "Starting epoch 1188\n",
      "Training loss: 0.010811692409095217\n",
      "Test loss: 0.04366332692680536\n",
      "Starting epoch 1189\n",
      "Training loss: 0.01087783892318362\n",
      "Test loss: 0.0437517500034085\n",
      "Starting epoch 1190\n",
      "Training loss: 0.010839019215009252\n",
      "Test loss: 0.043606234507428274\n",
      "Starting epoch 1191\n",
      "Training loss: 0.010826577592763256\n",
      "Test loss: 0.04392701884110769\n",
      "Starting epoch 1192\n",
      "Training loss: 0.01103706992246577\n",
      "Test loss: 0.0439713771144549\n",
      "Starting epoch 1193\n",
      "Training loss: 0.010800278363902061\n",
      "Test loss: 0.043706772642003164\n",
      "Starting epoch 1194\n",
      "Training loss: 0.010817293627340286\n",
      "Test loss: 0.04400918784516829\n",
      "Starting epoch 1195\n",
      "Training loss: 0.010825753303580597\n",
      "Test loss: 0.04424899147340545\n",
      "Starting epoch 1196\n",
      "Training loss: 0.010801886437369174\n",
      "Test loss: 0.044303536208139524\n",
      "Starting epoch 1197\n",
      "Training loss: 0.010930982937456155\n",
      "Test loss: 0.04410532075497839\n",
      "Starting epoch 1198\n",
      "Training loss: 0.010844783262029046\n",
      "Test loss: 0.04425068392797753\n",
      "Starting epoch 1199\n",
      "Training loss: 0.010853135118596867\n",
      "Test loss: 0.043865407437638\n",
      "Starting epoch 1200\n",
      "Training loss: 0.010839234045172324\n",
      "Test loss: 0.04385431441995832\n",
      "Starting epoch 1201\n",
      "Training loss: 0.010785474350340054\n",
      "Test loss: 0.043945282283756465\n",
      "Starting epoch 1202\n",
      "Training loss: 0.010832298592832244\n",
      "Test loss: 0.04388935353468965\n",
      "Starting epoch 1203\n",
      "Training loss: 0.01098485429939188\n",
      "Test loss: 0.044251564614198824\n",
      "Starting epoch 1204\n",
      "Training loss: 0.010797864803280986\n",
      "Test loss: 0.04371670205835943\n",
      "Starting epoch 1205\n",
      "Training loss: 0.010957024075457306\n",
      "Test loss: 0.044006628501746387\n",
      "Starting epoch 1206\n",
      "Training loss: 0.010932042293983405\n",
      "Test loss: 0.04468879269229041\n",
      "Starting epoch 1207\n",
      "Training loss: 0.011030775174254277\n",
      "Test loss: 0.04493524361815718\n",
      "Starting epoch 1208\n",
      "Training loss: 0.01090275829077744\n",
      "Test loss: 0.04496960686864676\n",
      "Starting epoch 1209\n",
      "Training loss: 0.010935347282984217\n",
      "Test loss: 0.04480920221518587\n",
      "Starting epoch 1210\n",
      "Training loss: 0.010840672755339106\n",
      "Test loss: 0.04467714550318541\n",
      "Starting epoch 1211\n",
      "Training loss: 0.01086381089980485\n",
      "Test loss: 0.04399821410576502\n",
      "Starting epoch 1212\n",
      "Training loss: 0.010805517465609019\n",
      "Test loss: 0.04368922893923742\n",
      "Starting epoch 1213\n",
      "Training loss: 0.010814217827664535\n",
      "Test loss: 0.04379088928302129\n",
      "Starting epoch 1214\n",
      "Training loss: 0.01086157844325558\n",
      "Test loss: 0.043899838364234674\n",
      "Starting epoch 1215\n",
      "Training loss: 0.011009847867440005\n",
      "Test loss: 0.043965767724094565\n",
      "Starting epoch 1216\n",
      "Training loss: 0.010821142127035095\n",
      "Test loss: 0.04466148617642897\n",
      "Starting epoch 1217\n",
      "Training loss: 0.01080782706925615\n",
      "Test loss: 0.04449832894735866\n",
      "Starting epoch 1218\n",
      "Training loss: 0.010870450390044783\n",
      "Test loss: 0.04404995969876095\n",
      "Starting epoch 1219\n",
      "Training loss: 0.010747093966871988\n",
      "Test loss: 0.04399199024946601\n",
      "Starting epoch 1220\n",
      "Training loss: 0.01087275131407087\n",
      "Test loss: 0.04398623646961318\n",
      "Starting epoch 1221\n",
      "Training loss: 0.010858669999192973\n",
      "Test loss: 0.04367192662148564\n",
      "Starting epoch 1222\n",
      "Training loss: 0.010777555528234263\n",
      "Test loss: 0.04409291195096793\n",
      "Starting epoch 1223\n",
      "Training loss: 0.010796441979034514\n",
      "Test loss: 0.04428719191087617\n",
      "Starting epoch 1224\n",
      "Training loss: 0.010827743295641219\n",
      "Test loss: 0.044092685260154585\n",
      "Starting epoch 1225\n",
      "Training loss: 0.010829302650250372\n",
      "Test loss: 0.044039762406437484\n",
      "Starting epoch 1226\n",
      "Training loss: 0.010734871335205485\n",
      "Test loss: 0.04429576338993178\n",
      "Starting epoch 1227\n",
      "Training loss: 0.011049268645096998\n",
      "Test loss: 0.044197627791651976\n",
      "Starting epoch 1228\n",
      "Training loss: 0.010908746786537717\n",
      "Test loss: 0.04384523630142212\n",
      "Starting epoch 1229\n",
      "Training loss: 0.01082319497573571\n",
      "Test loss: 0.04412409470037178\n",
      "Starting epoch 1230\n",
      "Training loss: 0.010738357977911096\n",
      "Test loss: 0.04449906607192976\n",
      "Starting epoch 1231\n",
      "Training loss: 0.010786424299366161\n",
      "Test loss: 0.04442876742945777\n",
      "Starting epoch 1232\n",
      "Training loss: 0.010817443555007215\n",
      "Test loss: 0.04454972712254083\n",
      "Starting epoch 1233\n",
      "Training loss: 0.010931059031090776\n",
      "Test loss: 0.044437614304048044\n",
      "Starting epoch 1234\n",
      "Training loss: 0.010746543967455138\n",
      "Test loss: 0.04375907461400385\n",
      "Starting epoch 1235\n",
      "Training loss: 0.0109237047736762\n",
      "Test loss: 0.04404241991815744\n",
      "Starting epoch 1236\n",
      "Training loss: 0.01073855390680618\n",
      "Test loss: 0.04391332281132539\n",
      "Starting epoch 1237\n",
      "Training loss: 0.010747776748459847\n",
      "Test loss: 0.044114836733098385\n",
      "Starting epoch 1238\n",
      "Training loss: 0.010814713314175606\n",
      "Test loss: 0.04447012470552215\n",
      "Starting epoch 1239\n",
      "Training loss: 0.010928483221863136\n",
      "Test loss: 0.04422710670365228\n",
      "Starting epoch 1240\n",
      "Training loss: 0.010977235302084783\n",
      "Test loss: 0.044711455012913105\n",
      "Starting epoch 1241\n",
      "Training loss: 0.010806670137604729\n",
      "Test loss: 0.04401571000063861\n",
      "Starting epoch 1242\n",
      "Training loss: 0.01084055490486446\n",
      "Test loss: 0.04406060775121053\n",
      "Starting epoch 1243\n",
      "Training loss: 0.01083598259958576\n",
      "Test loss: 0.04442055895924568\n",
      "Starting epoch 1244\n",
      "Training loss: 0.011096086612612497\n",
      "Test loss: 0.04448942646936134\n",
      "Starting epoch 1245\n",
      "Training loss: 0.010898500971007542\n",
      "Test loss: 0.04489078662461705\n",
      "Starting epoch 1246\n",
      "Training loss: 0.01073151106228594\n",
      "Test loss: 0.044107060258587204\n",
      "Starting epoch 1247\n",
      "Training loss: 0.010739429838588981\n",
      "Test loss: 0.04414904731567259\n",
      "Starting epoch 1248\n",
      "Training loss: 0.010808954366528596\n",
      "Test loss: 0.04424257824818293\n",
      "Starting epoch 1249\n",
      "Training loss: 0.010893569831721118\n",
      "Test loss: 0.0444273138763728\n",
      "Starting epoch 1250\n",
      "Training loss: 0.011059295599822138\n",
      "Test loss: 0.043978988848350664\n",
      "Starting epoch 1251\n",
      "Training loss: 0.01078419599559952\n",
      "Test loss: 0.04480043398561301\n",
      "Starting epoch 1252\n",
      "Training loss: 0.010881956117074997\n",
      "Test loss: 0.04454254372804253\n",
      "Starting epoch 1253\n",
      "Training loss: 0.010856209719767336\n",
      "Test loss: 0.04467457650160348\n",
      "Starting epoch 1254\n",
      "Training loss: 0.01087481643027458\n",
      "Test loss: 0.04416267491049237\n",
      "Starting epoch 1255\n",
      "Training loss: 0.010692672315435331\n",
      "Test loss: 0.04455946303076214\n",
      "Starting epoch 1256\n",
      "Training loss: 0.010719003400108854\n",
      "Test loss: 0.04448321944585553\n",
      "Starting epoch 1257\n",
      "Training loss: 0.010758978100951578\n",
      "Test loss: 0.044351737946271896\n",
      "Starting epoch 1258\n",
      "Training loss: 0.010882629875521192\n",
      "Test loss: 0.04454264431088059\n",
      "Starting epoch 1259\n",
      "Training loss: 0.010888694464916089\n",
      "Test loss: 0.043914727176781056\n",
      "Starting epoch 1260\n",
      "Training loss: 0.01084890115822925\n",
      "Test loss: 0.043543047444136056\n",
      "Starting epoch 1261\n",
      "Training loss: 0.010841181425408263\n",
      "Test loss: 0.04390896539445276\n",
      "Starting epoch 1262\n",
      "Training loss: 0.01077707346956261\n",
      "Test loss: 0.04441203928931996\n",
      "Starting epoch 1263\n",
      "Training loss: 0.010743572987371781\n",
      "Test loss: 0.04431234162162851\n",
      "Starting epoch 1264\n",
      "Training loss: 0.010919942436587126\n",
      "Test loss: 0.04451256852459024\n",
      "Starting epoch 1265\n",
      "Training loss: 0.010803525259748835\n",
      "Test loss: 0.04381168819963932\n",
      "Starting epoch 1266\n",
      "Training loss: 0.01070201279381748\n",
      "Test loss: 0.043832922009406264\n",
      "Starting epoch 1267\n",
      "Training loss: 0.01079622920236138\n",
      "Test loss: 0.044171332485145993\n",
      "Starting epoch 1268\n",
      "Training loss: 0.010703108487192725\n",
      "Test loss: 0.044664208949716004\n",
      "Starting epoch 1269\n",
      "Training loss: 0.010779442067151188\n",
      "Test loss: 0.04459386125758842\n",
      "Starting epoch 1270\n",
      "Training loss: 0.010710277640428699\n",
      "Test loss: 0.04478997143882292\n",
      "Starting epoch 1271\n",
      "Training loss: 0.011078098620914046\n",
      "Test loss: 0.04479988250467512\n",
      "Starting epoch 1272\n",
      "Training loss: 0.010694415797097762\n",
      "Test loss: 0.04513399606501615\n",
      "Starting epoch 1273\n",
      "Training loss: 0.010832721764435534\n",
      "Test loss: 0.04477838302652041\n",
      "Starting epoch 1274\n",
      "Training loss: 0.010720550800200368\n",
      "Test loss: 0.04402161789713083\n",
      "Starting epoch 1275\n",
      "Training loss: 0.010712142713123658\n",
      "Test loss: 0.043915438638241204\n",
      "Starting epoch 1276\n",
      "Training loss: 0.010790276585421602\n",
      "Test loss: 0.044345219516091876\n",
      "Starting epoch 1277\n",
      "Training loss: 0.010720071703439853\n",
      "Test loss: 0.04478330730840012\n",
      "Starting epoch 1278\n",
      "Training loss: 0.010756530676830987\n",
      "Test loss: 0.04440538826640005\n",
      "Starting epoch 1279\n",
      "Training loss: 0.01082765385813889\n",
      "Test loss: 0.044766585170118896\n",
      "Starting epoch 1280\n",
      "Training loss: 0.010748319877464264\n",
      "Test loss: 0.044802982912019444\n",
      "Starting epoch 1281\n",
      "Training loss: 0.010757307125041719\n",
      "Test loss: 0.044428721760158184\n",
      "Starting epoch 1282\n",
      "Training loss: 0.010824731123618415\n",
      "Test loss: 0.0447313598598595\n",
      "Starting epoch 1283\n",
      "Training loss: 0.010694140034010176\n",
      "Test loss: 0.044019915715411854\n",
      "Starting epoch 1284\n",
      "Training loss: 0.010811758242913933\n",
      "Test loss: 0.044216096263240884\n",
      "Starting epoch 1285\n",
      "Training loss: 0.01070682049469381\n",
      "Test loss: 0.044242882480223976\n",
      "Starting epoch 1286\n",
      "Training loss: 0.010764456079265133\n",
      "Test loss: 0.04446116883169721\n",
      "Starting epoch 1287\n",
      "Training loss: 0.010963754667366137\n",
      "Test loss: 0.044158225534138854\n",
      "Starting epoch 1288\n",
      "Training loss: 0.010736199767618883\n",
      "Test loss: 0.04374010295227722\n",
      "Starting epoch 1289\n",
      "Training loss: 0.010699616302354414\n",
      "Test loss: 0.04438777778435637\n",
      "Starting epoch 1290\n",
      "Training loss: 0.010752123673675491\n",
      "Test loss: 0.044403135500572344\n",
      "Starting epoch 1291\n",
      "Training loss: 0.010806475689665217\n",
      "Test loss: 0.044197650281367476\n",
      "Starting epoch 1292\n",
      "Training loss: 0.010639115343572663\n",
      "Test loss: 0.044650741680352775\n",
      "Starting epoch 1293\n",
      "Training loss: 0.010718756828640328\n",
      "Test loss: 0.04472398730339827\n",
      "Starting epoch 1294\n",
      "Training loss: 0.010729448320191414\n",
      "Test loss: 0.04468031958849342\n",
      "Starting epoch 1295\n",
      "Training loss: 0.010652354597801068\n",
      "Test loss: 0.044346319028624785\n",
      "Starting epoch 1296\n",
      "Training loss: 0.01079545821994543\n",
      "Test loss: 0.04448039402012472\n",
      "Starting epoch 1297\n",
      "Training loss: 0.01079614832997322\n",
      "Test loss: 0.044917303102987784\n",
      "Starting epoch 1298\n",
      "Training loss: 0.010722478668464989\n",
      "Test loss: 0.044458378623757094\n",
      "Starting epoch 1299\n",
      "Training loss: 0.010714602366578384\n",
      "Test loss: 0.044775263164882305\n",
      "Starting epoch 1300\n",
      "Training loss: 0.010775598682096748\n",
      "Test loss: 0.044359407422167284\n",
      "Starting epoch 1301\n",
      "Training loss: 0.010827760410601975\n",
      "Test loss: 0.04412602578048353\n",
      "Starting epoch 1302\n",
      "Training loss: 0.010718160293629913\n",
      "Test loss: 0.04449764045852202\n",
      "Starting epoch 1303\n",
      "Training loss: 0.01087037577736573\n",
      "Test loss: 0.044389822554809076\n",
      "Starting epoch 1304\n",
      "Training loss: 0.010735894125870993\n",
      "Test loss: 0.04488542220658726\n",
      "Starting epoch 1305\n",
      "Training loss: 0.010784039457069069\n",
      "Test loss: 0.04496268058816592\n",
      "Starting epoch 1306\n",
      "Training loss: 0.010776654679755696\n",
      "Test loss: 0.044862285946254375\n",
      "Starting epoch 1307\n",
      "Training loss: 0.010687024302047784\n",
      "Test loss: 0.0444028751441726\n",
      "Starting epoch 1308\n",
      "Training loss: 0.010653005424337308\n",
      "Test loss: 0.04436902253440133\n",
      "Starting epoch 1309\n",
      "Training loss: 0.010707769573467677\n",
      "Test loss: 0.04418759882726051\n",
      "Starting epoch 1310\n",
      "Training loss: 0.010748732590773066\n",
      "Test loss: 0.04470348730683327\n",
      "Starting epoch 1311\n",
      "Training loss: 0.010708145369760325\n",
      "Test loss: 0.0447549473632265\n",
      "Starting epoch 1312\n",
      "Training loss: 0.010703911439927875\n",
      "Test loss: 0.044692473003157866\n",
      "Starting epoch 1313\n",
      "Training loss: 0.010700388918401764\n",
      "Test loss: 0.04463138638271226\n",
      "Starting epoch 1314\n",
      "Training loss: 0.010678272969165787\n",
      "Test loss: 0.044785355804143126\n",
      "Starting epoch 1315\n",
      "Training loss: 0.010867085994877776\n",
      "Test loss: 0.045165609292410036\n",
      "Starting epoch 1316\n",
      "Training loss: 0.010712405292653158\n",
      "Test loss: 0.04534412547945976\n",
      "Starting epoch 1317\n",
      "Training loss: 0.010787618987750812\n",
      "Test loss: 0.04443307151948964\n",
      "Starting epoch 1318\n",
      "Training loss: 0.01062575391814357\n",
      "Test loss: 0.04407960107481038\n",
      "Starting epoch 1319\n",
      "Training loss: 0.010618117324946845\n",
      "Test loss: 0.04440164469458439\n",
      "Starting epoch 1320\n",
      "Training loss: 0.010662160829076023\n",
      "Test loss: 0.04446689384403052\n",
      "Starting epoch 1321\n",
      "Training loss: 0.010655082121002869\n",
      "Test loss: 0.044705384997306044\n",
      "Starting epoch 1322\n",
      "Training loss: 0.01057408791279695\n",
      "Test loss: 0.04459123078871657\n",
      "Starting epoch 1323\n",
      "Training loss: 0.010672086269640531\n",
      "Test loss: 0.0447948187313698\n",
      "Starting epoch 1324\n",
      "Training loss: 0.01084864145663918\n",
      "Test loss: 0.04444584947217394\n",
      "Starting epoch 1325\n",
      "Training loss: 0.010676102437933937\n",
      "Test loss: 0.04409004651285984\n",
      "Starting epoch 1326\n",
      "Training loss: 0.01080453453860322\n",
      "Test loss: 0.044116363412252176\n",
      "Starting epoch 1327\n",
      "Training loss: 0.010681724435359728\n",
      "Test loss: 0.04493477623219843\n",
      "Starting epoch 1328\n",
      "Training loss: 0.010656551809095945\n",
      "Test loss: 0.04474348223043813\n",
      "Starting epoch 1329\n",
      "Training loss: 0.010659741191956841\n",
      "Test loss: 0.04467849461016832\n",
      "Starting epoch 1330\n",
      "Training loss: 0.010826544271262943\n",
      "Test loss: 0.04456744667280604\n",
      "Starting epoch 1331\n",
      "Training loss: 0.010603105511943826\n",
      "Test loss: 0.04494970340143751\n",
      "Starting epoch 1332\n",
      "Training loss: 0.01064228564195457\n",
      "Test loss: 0.0446310484678381\n",
      "Starting epoch 1333\n",
      "Training loss: 0.010663857897285555\n",
      "Test loss: 0.044620058602756925\n",
      "Starting epoch 1334\n",
      "Training loss: 0.010719149022317324\n",
      "Test loss: 0.04422790467463158\n",
      "Starting epoch 1335\n",
      "Training loss: 0.010632855924548672\n",
      "Test loss: 0.04468317316086204\n",
      "Starting epoch 1336\n",
      "Training loss: 0.01063312791654321\n",
      "Test loss: 0.04454447618789143\n",
      "Starting epoch 1337\n",
      "Training loss: 0.010978643385479684\n",
      "Test loss: 0.044539101214872465\n",
      "Starting epoch 1338\n",
      "Training loss: 0.010760515363367855\n",
      "Test loss: 0.04512837018679689\n",
      "Starting epoch 1339\n",
      "Training loss: 0.010738821837623588\n",
      "Test loss: 0.04456331649864161\n",
      "Starting epoch 1340\n",
      "Training loss: 0.010670184707421749\n",
      "Test loss: 0.04481378342542383\n",
      "Starting epoch 1341\n",
      "Training loss: 0.010621399771361078\n",
      "Test loss: 0.0445879991683695\n",
      "Starting epoch 1342\n",
      "Training loss: 0.010737334759753258\n",
      "Test loss: 0.044376196822634446\n",
      "Starting epoch 1343\n",
      "Training loss: 0.01078653689779219\n",
      "Test loss: 0.0448251512867433\n",
      "Starting epoch 1344\n",
      "Training loss: 0.010581563562765474\n",
      "Test loss: 0.044486468450890645\n",
      "Starting epoch 1345\n",
      "Training loss: 0.010616179158697362\n",
      "Test loss: 0.04476270697045105\n",
      "Starting epoch 1346\n",
      "Training loss: 0.01056176608764246\n",
      "Test loss: 0.044613317483001284\n",
      "Starting epoch 1347\n",
      "Training loss: 0.010654195028616757\n",
      "Test loss: 0.044699187769933986\n",
      "Starting epoch 1348\n",
      "Training loss: 0.010645435603915667\n",
      "Test loss: 0.044926885239504\n",
      "Starting epoch 1349\n",
      "Training loss: 0.010596369515310546\n",
      "Test loss: 0.04480459341020496\n",
      "Starting epoch 1350\n",
      "Training loss: 0.010607363381346718\n",
      "Test loss: 0.04476816593496888\n",
      "Starting epoch 1351\n",
      "Training loss: 0.010690490318248506\n",
      "Test loss: 0.04483992399440871\n",
      "Starting epoch 1352\n",
      "Training loss: 0.010611743208204136\n",
      "Test loss: 0.04444030106619552\n",
      "Starting epoch 1353\n",
      "Training loss: 0.010952498717996919\n",
      "Test loss: 0.04452620156937175\n",
      "Starting epoch 1354\n",
      "Training loss: 0.010568008483311192\n",
      "Test loss: 0.04522037395724544\n",
      "Starting epoch 1355\n",
      "Training loss: 0.010668789265585727\n",
      "Test loss: 0.04494452738651523\n",
      "Starting epoch 1356\n",
      "Training loss: 0.010969247470502971\n",
      "Test loss: 0.04476549062463972\n",
      "Starting epoch 1357\n",
      "Training loss: 0.010658636902932261\n",
      "Test loss: 0.04400454944482556\n",
      "Starting epoch 1358\n",
      "Training loss: 0.010615218781911935\n",
      "Test loss: 0.04407459773399212\n",
      "Starting epoch 1359\n",
      "Training loss: 0.010636519648317919\n",
      "Test loss: 0.04454319123868589\n",
      "Starting epoch 1360\n",
      "Training loss: 0.010575221882003253\n",
      "Test loss: 0.04446885376064866\n",
      "Starting epoch 1361\n",
      "Training loss: 0.010637820942602197\n",
      "Test loss: 0.044771405281843965\n",
      "Starting epoch 1362\n",
      "Training loss: 0.010634448348743017\n",
      "Test loss: 0.04467731465895971\n",
      "Starting epoch 1363\n",
      "Training loss: 0.010876582248411218\n",
      "Test loss: 0.04495430958491785\n",
      "Starting epoch 1364\n",
      "Training loss: 0.01061095908039906\n",
      "Test loss: 0.04439570133884748\n",
      "Starting epoch 1365\n",
      "Training loss: 0.010665260026323014\n",
      "Test loss: 0.04433388069823936\n",
      "Starting epoch 1366\n",
      "Training loss: 0.010576208763313099\n",
      "Test loss: 0.0446519844096016\n",
      "Starting epoch 1367\n",
      "Training loss: 0.01068565444868119\n",
      "Test loss: 0.04501901994700785\n",
      "Starting epoch 1368\n",
      "Training loss: 0.010657041654235026\n",
      "Test loss: 0.04549288252989451\n",
      "Starting epoch 1369\n",
      "Training loss: 0.010640882505256622\n",
      "Test loss: 0.0448713052879881\n",
      "Starting epoch 1370\n",
      "Training loss: 0.010746380482174333\n",
      "Test loss: 0.04447543807327747\n",
      "Starting epoch 1371\n",
      "Training loss: 0.010613337586649129\n",
      "Test loss: 0.04505252162063563\n",
      "Starting epoch 1372\n",
      "Training loss: 0.010601033960453799\n",
      "Test loss: 0.04508462975974436\n",
      "Starting epoch 1373\n",
      "Training loss: 0.010592262092672411\n",
      "Test loss: 0.04499201338600229\n",
      "Starting epoch 1374\n",
      "Training loss: 0.010810721803029052\n",
      "Test loss: 0.04474405226884065\n",
      "Starting epoch 1375\n",
      "Training loss: 0.010747432036966573\n",
      "Test loss: 0.04529248122815733\n",
      "Starting epoch 1376\n",
      "Training loss: 0.010625244667906254\n",
      "Test loss: 0.04544531347023116\n",
      "Starting epoch 1377\n",
      "Training loss: 0.010548550711914163\n",
      "Test loss: 0.045032503773216846\n",
      "Starting epoch 1378\n",
      "Training loss: 0.010594263512519051\n",
      "Test loss: 0.04477231935770423\n",
      "Starting epoch 1379\n",
      "Training loss: 0.010583532500828876\n",
      "Test loss: 0.0444441480493104\n",
      "Starting epoch 1380\n",
      "Training loss: 0.010609756636082148\n",
      "Test loss: 0.04465796025814833\n",
      "Starting epoch 1381\n",
      "Training loss: 0.010585942916327813\n",
      "Test loss: 0.044955353218096274\n",
      "Starting epoch 1382\n",
      "Training loss: 0.010818852905611524\n",
      "Test loss: 0.04503775512178739\n",
      "Starting epoch 1383\n",
      "Training loss: 0.010503426026247565\n",
      "Test loss: 0.044328860386654186\n",
      "Starting epoch 1384\n",
      "Training loss: 0.01054824174183314\n",
      "Test loss: 0.04439105466008186\n",
      "Starting epoch 1385\n",
      "Training loss: 0.010569386275821045\n",
      "Test loss: 0.04459433002328431\n",
      "Starting epoch 1386\n",
      "Training loss: 0.010576858200499268\n",
      "Test loss: 0.04453540683068611\n",
      "Starting epoch 1387\n",
      "Training loss: 0.010559279746452316\n",
      "Test loss: 0.045012269996934466\n",
      "Starting epoch 1388\n",
      "Training loss: 0.010747251161908517\n",
      "Test loss: 0.04477563183064814\n",
      "Starting epoch 1389\n",
      "Training loss: 0.010531219211025317\n",
      "Test loss: 0.04429808921284146\n",
      "Starting epoch 1390\n",
      "Training loss: 0.010598671775250161\n",
      "Test loss: 0.04446678484479586\n",
      "Starting epoch 1391\n",
      "Training loss: 0.010752953268343309\n",
      "Test loss: 0.04474425033010818\n",
      "Starting epoch 1392\n",
      "Training loss: 0.010642593955529517\n",
      "Test loss: 0.04526085827361654\n",
      "Starting epoch 1393\n",
      "Training loss: 0.010539935962831388\n",
      "Test loss: 0.04483005031943321\n",
      "Starting epoch 1394\n",
      "Training loss: 0.01078473091064418\n",
      "Test loss: 0.0449789811477617\n",
      "Starting epoch 1395\n",
      "Training loss: 0.010675838904180487\n",
      "Test loss: 0.04545747681900307\n",
      "Starting epoch 1396\n",
      "Training loss: 0.010674831854393248\n",
      "Test loss: 0.04477683027033453\n",
      "Starting epoch 1397\n",
      "Training loss: 0.010499248250586088\n",
      "Test loss: 0.04465620261099604\n",
      "Starting epoch 1398\n",
      "Training loss: 0.01056147829369932\n",
      "Test loss: 0.0447379262359054\n",
      "Starting epoch 1399\n",
      "Training loss: 0.010631905929719816\n",
      "Test loss: 0.04512339306098444\n",
      "Starting epoch 1400\n",
      "Training loss: 0.010594692676648742\n",
      "Test loss: 0.045252172759285676\n",
      "Starting epoch 1401\n",
      "Training loss: 0.01052385728928398\n",
      "Test loss: 0.04485013253158993\n",
      "Starting epoch 1402\n",
      "Training loss: 0.01055637605060808\n",
      "Test loss: 0.044955453387013185\n",
      "Starting epoch 1403\n",
      "Training loss: 0.010653346960173279\n",
      "Test loss: 0.04481474172185968\n",
      "Starting epoch 1404\n",
      "Training loss: 0.010788974428518873\n",
      "Test loss: 0.045145605173375875\n",
      "Starting epoch 1405\n",
      "Training loss: 0.010770957367342026\n",
      "Test loss: 0.04429716161555714\n",
      "Starting epoch 1406\n",
      "Training loss: 0.010600068484295587\n",
      "Test loss: 0.04417501210614487\n",
      "Starting epoch 1407\n",
      "Training loss: 0.010654310069856097\n",
      "Test loss: 0.04484845711677163\n",
      "Starting epoch 1408\n",
      "Training loss: 0.010555514271874896\n",
      "Test loss: 0.04533953743952292\n",
      "Starting epoch 1409\n",
      "Training loss: 0.010533485271525188\n",
      "Test loss: 0.04524930801104616\n",
      "Starting epoch 1410\n",
      "Training loss: 0.010588501427383696\n",
      "Test loss: 0.04515333045963888\n",
      "Starting epoch 1411\n",
      "Training loss: 0.010576428066878045\n",
      "Test loss: 0.04508447550513126\n",
      "Starting epoch 1412\n",
      "Training loss: 0.010617494644200216\n",
      "Test loss: 0.044962752196523875\n",
      "Starting epoch 1413\n",
      "Training loss: 0.010557799691670254\n",
      "Test loss: 0.044646159918219956\n",
      "Starting epoch 1414\n",
      "Training loss: 0.01052303931324697\n",
      "Test loss: 0.0448601096868515\n",
      "Starting epoch 1415\n",
      "Training loss: 0.01055482637564667\n",
      "Test loss: 0.04481522904502021\n",
      "Starting epoch 1416\n",
      "Training loss: 0.010575508790426567\n",
      "Test loss: 0.04509828970940025\n",
      "Starting epoch 1417\n",
      "Training loss: 0.010544580086821416\n",
      "Test loss: 0.04525232190887133\n",
      "Starting epoch 1418\n",
      "Training loss: 0.010632309818365535\n",
      "Test loss: 0.04500424282418357\n",
      "Starting epoch 1419\n",
      "Training loss: 0.01051304116845131\n",
      "Test loss: 0.04481626522761804\n",
      "Starting epoch 1420\n",
      "Training loss: 0.010521657535897904\n",
      "Test loss: 0.04495185917174375\n",
      "Starting epoch 1421\n",
      "Training loss: 0.010622075224509005\n",
      "Test loss: 0.04500553957014172\n",
      "Starting epoch 1422\n",
      "Training loss: 0.010546673303011988\n",
      "Test loss: 0.04477283013639627\n",
      "Starting epoch 1423\n",
      "Training loss: 0.01050776238629564\n",
      "Test loss: 0.04488551671858187\n",
      "Starting epoch 1424\n",
      "Training loss: 0.010560515260354418\n",
      "Test loss: 0.04519111249181959\n",
      "Starting epoch 1425\n",
      "Training loss: 0.010789602185736914\n",
      "Test loss: 0.04516747290337527\n",
      "Starting epoch 1426\n",
      "Training loss: 0.010540756885511954\n",
      "Test loss: 0.04442903889274156\n",
      "Starting epoch 1427\n",
      "Training loss: 0.010530710082928666\n",
      "Test loss: 0.04479947451640059\n",
      "Starting epoch 1428\n",
      "Training loss: 0.010547628519354297\n",
      "Test loss: 0.04499926445660768\n",
      "Starting epoch 1429\n",
      "Training loss: 0.010542735762772013\n",
      "Test loss: 0.04482868962265827\n",
      "Starting epoch 1430\n",
      "Training loss: 0.01056373368215854\n",
      "Test loss: 0.04498246063788732\n",
      "Starting epoch 1431\n",
      "Training loss: 0.010623246217604543\n",
      "Test loss: 0.04493005242612627\n",
      "Starting epoch 1432\n",
      "Training loss: 0.010616716394414668\n",
      "Test loss: 0.045249661085782225\n",
      "Starting epoch 1433\n",
      "Training loss: 0.010525513974735971\n",
      "Test loss: 0.0446137692089434\n",
      "Starting epoch 1434\n",
      "Training loss: 0.010812401970023991\n",
      "Test loss: 0.04480040059597404\n",
      "Starting epoch 1435\n",
      "Training loss: 0.01047103439808869\n",
      "Test loss: 0.0454115801387363\n",
      "Starting epoch 1436\n",
      "Training loss: 0.010547889136999358\n",
      "Test loss: 0.045169256834520236\n",
      "Starting epoch 1437\n",
      "Training loss: 0.010742551295972262\n",
      "Test loss: 0.044656351622607976\n",
      "Starting epoch 1438\n",
      "Training loss: 0.010614738952307428\n",
      "Test loss: 0.04441810426888643\n",
      "Starting epoch 1439\n",
      "Training loss: 0.01050917630190732\n",
      "Test loss: 0.04485391225251886\n",
      "Starting epoch 1440\n",
      "Training loss: 0.010513777508721\n",
      "Test loss: 0.044962706665198006\n",
      "Starting epoch 1441\n",
      "Training loss: 0.010848939754679555\n",
      "Test loss: 0.04514768664483671\n",
      "Starting epoch 1442\n",
      "Training loss: 0.010635485223754019\n",
      "Test loss: 0.04595179493642516\n",
      "Starting epoch 1443\n",
      "Training loss: 0.010608746807594768\n",
      "Test loss: 0.0450614762182037\n",
      "Starting epoch 1444\n",
      "Training loss: 0.010593625380978232\n",
      "Test loss: 0.0451313442102185\n",
      "Starting epoch 1445\n",
      "Training loss: 0.010637633197131704\n",
      "Test loss: 0.044799362757691634\n",
      "Starting epoch 1446\n",
      "Training loss: 0.01074548606134829\n",
      "Test loss: 0.04539864303337203\n",
      "Starting epoch 1447\n",
      "Training loss: 0.01059495433248946\n",
      "Test loss: 0.046035497828766155\n",
      "Starting epoch 1448\n",
      "Training loss: 0.010679515277142407\n",
      "Test loss: 0.045273303571674556\n",
      "Starting epoch 1449\n",
      "Training loss: 0.010599223316692915\n",
      "Test loss: 0.04556062238083945\n",
      "Starting epoch 1450\n",
      "Training loss: 0.010565301922501111\n",
      "Test loss: 0.04513157021116327\n",
      "Starting epoch 1451\n",
      "Training loss: 0.010602003551225682\n",
      "Test loss: 0.045214549672824365\n",
      "Starting epoch 1452\n",
      "Training loss: 0.010592005154514899\n",
      "Test loss: 0.04483302254919653\n",
      "Starting epoch 1453\n",
      "Training loss: 0.010682685407580899\n",
      "Test loss: 0.04453056871339127\n",
      "Starting epoch 1454\n",
      "Training loss: 0.010456882111850332\n",
      "Test loss: 0.04534976349936591\n",
      "Starting epoch 1455\n",
      "Training loss: 0.010541168301076185\n",
      "Test loss: 0.04510289168468228\n",
      "Starting epoch 1456\n",
      "Training loss: 0.01047035415091964\n",
      "Test loss: 0.04499406326148245\n",
      "Starting epoch 1457\n",
      "Training loss: 0.010506742679681933\n",
      "Test loss: 0.045235825771534885\n",
      "Starting epoch 1458\n",
      "Training loss: 0.010576782335878396\n",
      "Test loss: 0.04503898019040072\n",
      "Starting epoch 1459\n",
      "Training loss: 0.010497924627461394\n",
      "Test loss: 0.045380345511215704\n",
      "Starting epoch 1460\n",
      "Training loss: 0.010505020549734597\n",
      "Test loss: 0.04538412668086864\n",
      "Starting epoch 1461\n",
      "Training loss: 0.010616405454815412\n",
      "Test loss: 0.045264970097276896\n",
      "Starting epoch 1462\n",
      "Training loss: 0.010443047696693998\n",
      "Test loss: 0.04468573643653481\n",
      "Starting epoch 1463\n",
      "Training loss: 0.010838730016448458\n",
      "Test loss: 0.04478291836049822\n",
      "Starting epoch 1464\n",
      "Training loss: 0.010611150734370849\n",
      "Test loss: 0.04437300107545323\n",
      "Starting epoch 1465\n",
      "Training loss: 0.010485233479469527\n",
      "Test loss: 0.04528608863000517\n",
      "Starting epoch 1466\n",
      "Training loss: 0.010472296569191042\n",
      "Test loss: 0.04546525884695627\n",
      "Starting epoch 1467\n",
      "Training loss: 0.01045332665814728\n",
      "Test loss: 0.04513323789945355\n",
      "Starting epoch 1468\n",
      "Training loss: 0.010615527019148967\n",
      "Test loss: 0.045353093908892736\n",
      "Starting epoch 1469\n",
      "Training loss: 0.010472010698963384\n",
      "Test loss: 0.0455317469658675\n",
      "Starting epoch 1470\n",
      "Training loss: 0.010465859695047628\n",
      "Test loss: 0.04554875636542285\n",
      "Starting epoch 1471\n",
      "Training loss: 0.010444174490136201\n",
      "Test loss: 0.04512027595882063\n",
      "Starting epoch 1472\n",
      "Training loss: 0.010620967759827122\n",
      "Test loss: 0.04493365464387117\n",
      "Starting epoch 1473\n",
      "Training loss: 0.010572409471038912\n",
      "Test loss: 0.04469958651396963\n",
      "Starting epoch 1474\n",
      "Training loss: 0.010491566366103829\n",
      "Test loss: 0.045287341431335164\n",
      "Starting epoch 1475\n",
      "Training loss: 0.010529314396811313\n",
      "Test loss: 0.04519302246195299\n",
      "Starting epoch 1476\n",
      "Training loss: 0.010565120467274893\n",
      "Test loss: 0.04525972343981266\n",
      "Starting epoch 1477\n",
      "Training loss: 0.010433529931135842\n",
      "Test loss: 0.04533011452467353\n",
      "Starting epoch 1478\n",
      "Training loss: 0.01048865954040504\n",
      "Test loss: 0.04489118081552011\n",
      "Starting epoch 1479\n",
      "Training loss: 0.010534433235765481\n",
      "Test loss: 0.0448896539983926\n",
      "Starting epoch 1480\n",
      "Training loss: 0.010553174270469635\n",
      "Test loss: 0.045336965885427266\n",
      "Starting epoch 1481\n",
      "Training loss: 0.01058328850958191\n",
      "Test loss: 0.045667123394431894\n",
      "Starting epoch 1482\n",
      "Training loss: 0.010440151909457856\n",
      "Test loss: 0.04558552401485266\n",
      "Starting epoch 1483\n",
      "Training loss: 0.01047229014153852\n",
      "Test loss: 0.04520548383394877\n",
      "Starting epoch 1484\n",
      "Training loss: 0.010704267205151378\n",
      "Test loss: 0.044998768165155696\n",
      "Starting epoch 1485\n",
      "Training loss: 0.010566363843982338\n",
      "Test loss: 0.0443835419913133\n",
      "Starting epoch 1486\n",
      "Training loss: 0.01054826586461458\n",
      "Test loss: 0.04495561178083773\n",
      "Starting epoch 1487\n",
      "Training loss: 0.010478584523328015\n",
      "Test loss: 0.04529718033693455\n",
      "Starting epoch 1488\n",
      "Training loss: 0.010687343593015045\n",
      "Test loss: 0.04495862498879433\n",
      "Starting epoch 1489\n",
      "Training loss: 0.010798405596345175\n",
      "Test loss: 0.04488721641677397\n",
      "Starting epoch 1490\n",
      "Training loss: 0.010433582985987429\n",
      "Test loss: 0.04554902458632434\n",
      "Starting epoch 1491\n",
      "Training loss: 0.010421362644458403\n",
      "Test loss: 0.045397945714217645\n",
      "Starting epoch 1492\n",
      "Training loss: 0.010557657397794918\n",
      "Test loss: 0.045330338524999444\n",
      "Starting epoch 1493\n",
      "Training loss: 0.010949862097985432\n",
      "Test loss: 0.04569110288112252\n",
      "Starting epoch 1494\n",
      "Training loss: 0.010676288870392276\n",
      "Test loss: 0.0461416469403991\n",
      "Starting epoch 1495\n",
      "Training loss: 0.010513734164052322\n",
      "Test loss: 0.04602111048168606\n",
      "Starting epoch 1496\n",
      "Training loss: 0.010493257922715828\n",
      "Test loss: 0.04548672241745172\n",
      "Starting epoch 1497\n",
      "Training loss: 0.010440857882504581\n",
      "Test loss: 0.0454111874655441\n",
      "Starting epoch 1498\n",
      "Training loss: 0.010469619474816517\n",
      "Test loss: 0.04512203664139465\n",
      "Starting epoch 1499\n",
      "Training loss: 0.01049755861768957\n",
      "Test loss: 0.04480730956075368\n",
      "Starting epoch 1500\n",
      "Training loss: 0.010410560737745683\n",
      "Test loss: 0.04488295385683024\n",
      "Starting epoch 1501\n",
      "Training loss: 0.010468442893785531\n",
      "Test loss: 0.04526397199542434\n",
      "Starting epoch 1502\n",
      "Training loss: 0.010456875989549473\n",
      "Test loss: 0.04510553001805588\n",
      "Starting epoch 1503\n",
      "Training loss: 0.010399728784429246\n",
      "Test loss: 0.04507439735311049\n",
      "Starting epoch 1504\n",
      "Training loss: 0.010430309623785194\n",
      "Test loss: 0.045010869563729676\n",
      "Starting epoch 1505\n",
      "Training loss: 0.010408963550065384\n",
      "Test loss: 0.04520514855782191\n",
      "Starting epoch 1506\n",
      "Training loss: 0.010595170719946017\n",
      "Test loss: 0.045332021045464056\n",
      "Starting epoch 1507\n",
      "Training loss: 0.010528501336936091\n",
      "Test loss: 0.04586384748971021\n",
      "Starting epoch 1508\n",
      "Training loss: 0.010460656563766667\n",
      "Test loss: 0.045993998715723\n",
      "Starting epoch 1509\n",
      "Training loss: 0.010936637744918222\n",
      "Test loss: 0.04539622628578433\n",
      "Starting epoch 1510\n",
      "Training loss: 0.010449572130427008\n",
      "Test loss: 0.04447811303867234\n",
      "Starting epoch 1511\n",
      "Training loss: 0.010489178363417015\n",
      "Test loss: 0.04506784801681837\n",
      "Starting epoch 1512\n",
      "Training loss: 0.010499192264358529\n",
      "Test loss: 0.045208581619792514\n",
      "Starting epoch 1513\n",
      "Training loss: 0.01043417811638019\n",
      "Test loss: 0.045481348893156755\n",
      "Starting epoch 1514\n",
      "Training loss: 0.010620524083859608\n",
      "Test loss: 0.04524554188052813\n",
      "Starting epoch 1515\n",
      "Training loss: 0.010405255832755174\n",
      "Test loss: 0.044650888415398424\n",
      "Starting epoch 1516\n",
      "Training loss: 0.010544033675286614\n",
      "Test loss: 0.044904634218525\n",
      "Starting epoch 1517\n",
      "Training loss: 0.01047089561575749\n",
      "Test loss: 0.044928556929032006\n",
      "Starting epoch 1518\n",
      "Training loss: 0.01044873984866455\n",
      "Test loss: 0.044697745254746186\n",
      "Starting epoch 1519\n",
      "Training loss: 0.010448161695824295\n",
      "Test loss: 0.04505942016839981\n",
      "Starting epoch 1520\n",
      "Training loss: 0.010440598241984844\n",
      "Test loss: 0.04495376569253427\n",
      "Starting epoch 1521\n",
      "Training loss: 0.010418001791370696\n",
      "Test loss: 0.044901857497515504\n",
      "Starting epoch 1522\n",
      "Training loss: 0.010540311072082793\n",
      "Test loss: 0.044673816956303736\n",
      "Starting epoch 1523\n",
      "Training loss: 0.010470433695028062\n",
      "Test loss: 0.044837740146451525\n",
      "Starting epoch 1524\n",
      "Training loss: 0.010535798814208781\n",
      "Test loss: 0.044823805491129555\n",
      "Starting epoch 1525\n",
      "Training loss: 0.0105122141998078\n",
      "Test loss: 0.04541528118015439\n",
      "Starting epoch 1526\n",
      "Training loss: 0.010550821413759326\n",
      "Test loss: 0.04520320192117382\n",
      "Starting epoch 1527\n",
      "Training loss: 0.010476278660238767\n",
      "Test loss: 0.04470932842404754\n",
      "Starting epoch 1528\n",
      "Training loss: 0.010429563626769136\n",
      "Test loss: 0.0451700572890264\n",
      "Starting epoch 1529\n",
      "Training loss: 0.01046649310127145\n",
      "Test loss: 0.04501136295773365\n",
      "Starting epoch 1530\n",
      "Training loss: 0.010422672328279644\n",
      "Test loss: 0.04484236185197477\n",
      "Starting epoch 1531\n",
      "Training loss: 0.010415851337010743\n",
      "Test loss: 0.045118377164558125\n",
      "Starting epoch 1532\n",
      "Training loss: 0.01045482478974784\n",
      "Test loss: 0.04559668664027144\n",
      "Starting epoch 1533\n",
      "Training loss: 0.010489970842590098\n",
      "Test loss: 0.04514289219622259\n",
      "Starting epoch 1534\n",
      "Training loss: 0.01044976776923801\n",
      "Test loss: 0.04519572136578737\n",
      "Starting epoch 1535\n",
      "Training loss: 0.010386011578509064\n",
      "Test loss: 0.045397942816769635\n",
      "Starting epoch 1536\n",
      "Training loss: 0.010489788135422057\n",
      "Test loss: 0.045341936181540844\n",
      "Starting epoch 1537\n",
      "Training loss: 0.010493625001218474\n",
      "Test loss: 0.045776082271779026\n",
      "Starting epoch 1538\n",
      "Training loss: 0.010486620050839713\n",
      "Test loss: 0.04510471514529652\n",
      "Starting epoch 1539\n",
      "Training loss: 0.010415133302573298\n",
      "Test loss: 0.045150137333958236\n",
      "Starting epoch 1540\n",
      "Training loss: 0.010427948667622004\n",
      "Test loss: 0.045180251132007\n",
      "Starting epoch 1541\n",
      "Training loss: 0.010401904140217383\n",
      "Test loss: 0.04527310413066988\n",
      "Starting epoch 1542\n",
      "Training loss: 0.010399996333557074\n",
      "Test loss: 0.04523994342458469\n",
      "Starting epoch 1543\n",
      "Training loss: 0.010436904212062965\n",
      "Test loss: 0.04557111155655649\n",
      "Starting epoch 1544\n",
      "Training loss: 0.01046649516239518\n",
      "Test loss: 0.04538107870353593\n",
      "Starting epoch 1545\n",
      "Training loss: 0.010377243397849017\n",
      "Test loss: 0.04564160550082171\n",
      "Starting epoch 1546\n",
      "Training loss: 0.010545098925100976\n",
      "Test loss: 0.04544250929245242\n",
      "Starting epoch 1547\n",
      "Training loss: 0.010402921862045273\n",
      "Test loss: 0.044908145511591877\n",
      "Starting epoch 1548\n",
      "Training loss: 0.010372264997758826\n",
      "Test loss: 0.045074385625344736\n",
      "Starting epoch 1549\n",
      "Training loss: 0.010448681190609932\n",
      "Test loss: 0.04516576713433972\n",
      "Starting epoch 1550\n",
      "Training loss: 0.010431680858868067\n",
      "Test loss: 0.045302196923229426\n",
      "Starting epoch 1551\n",
      "Training loss: 0.01058704264797881\n",
      "Test loss: 0.04523141530376894\n",
      "Starting epoch 1552\n",
      "Training loss: 0.010378306151413526\n",
      "Test loss: 0.044787924736738205\n",
      "Starting epoch 1553\n",
      "Training loss: 0.010449184883446967\n",
      "Test loss: 0.04519229851387165\n",
      "Starting epoch 1554\n",
      "Training loss: 0.010735501932193999\n",
      "Test loss: 0.04534655726618237\n",
      "Starting epoch 1555\n",
      "Training loss: 0.010375246329263586\n",
      "Test loss: 0.04593352097328062\n",
      "Starting epoch 1556\n",
      "Training loss: 0.010403890582564914\n",
      "Test loss: 0.04547854485335173\n",
      "Starting epoch 1557\n",
      "Training loss: 0.01039899543660586\n",
      "Test loss: 0.04492076596728078\n",
      "Starting epoch 1558\n",
      "Training loss: 0.010418040265680337\n",
      "Test loss: 0.04495561743776003\n",
      "Starting epoch 1559\n",
      "Training loss: 0.010432978985128833\n",
      "Test loss: 0.04530580024476404\n",
      "Starting epoch 1560\n",
      "Training loss: 0.010378769553098523\n",
      "Test loss: 0.04531741735559923\n",
      "Starting epoch 1561\n",
      "Training loss: 0.01052137876509643\n",
      "Test loss: 0.04511215192852197\n",
      "Starting epoch 1562\n",
      "Training loss: 0.010495973460864826\n",
      "Test loss: 0.04463167212627552\n",
      "Starting epoch 1563\n",
      "Training loss: 0.01039558786471359\n",
      "Test loss: 0.04530694156333252\n",
      "Starting epoch 1564\n",
      "Training loss: 0.010580724899153241\n",
      "Test loss: 0.04560423062907325\n",
      "Starting epoch 1565\n",
      "Training loss: 0.010482975663464577\n",
      "Test loss: 0.044835353201186215\n",
      "Starting epoch 1566\n",
      "Training loss: 0.010407186235438605\n",
      "Test loss: 0.044709085314362136\n",
      "Starting epoch 1567\n",
      "Training loss: 0.010418512140874003\n",
      "Test loss: 0.04495100166510652\n",
      "Starting epoch 1568\n",
      "Training loss: 0.010444109175415313\n",
      "Test loss: 0.045418438536149484\n",
      "Starting epoch 1569\n",
      "Training loss: 0.01043106278129777\n",
      "Test loss: 0.04576809718101113\n",
      "Starting epoch 1570\n",
      "Training loss: 0.010385868170100157\n",
      "Test loss: 0.04531107346216837\n",
      "Starting epoch 1571\n",
      "Training loss: 0.010571269593277916\n",
      "Test loss: 0.045102476659748286\n",
      "Starting epoch 1572\n",
      "Training loss: 0.010400993726597946\n",
      "Test loss: 0.04568911274826085\n",
      "Starting epoch 1573\n",
      "Training loss: 0.010728576281642328\n",
      "Test loss: 0.04559414268091873\n",
      "Starting epoch 1574\n",
      "Training loss: 0.010427136936026518\n",
      "Test loss: 0.04482970814462061\n",
      "Starting epoch 1575\n",
      "Training loss: 0.010406543210637375\n",
      "Test loss: 0.04498116878999604\n",
      "Starting epoch 1576\n",
      "Training loss: 0.010517778302558133\n",
      "Test loss: 0.04504448147835555\n",
      "Starting epoch 1577\n",
      "Training loss: 0.010410303769053006\n",
      "Test loss: 0.04487334178001792\n",
      "Starting epoch 1578\n",
      "Training loss: 0.0105238230288273\n",
      "Test loss: 0.04482364392390958\n",
      "Starting epoch 1579\n",
      "Training loss: 0.010707370539913412\n",
      "Test loss: 0.045685223683162975\n",
      "Starting epoch 1580\n",
      "Training loss: 0.010824887204121371\n",
      "Test loss: 0.045234456520389626\n",
      "Starting epoch 1581\n",
      "Training loss: 0.010482753886551153\n",
      "Test loss: 0.045859746910907606\n",
      "Starting epoch 1582\n",
      "Training loss: 0.01037728902502138\n",
      "Test loss: 0.04515587124559614\n",
      "Starting epoch 1583\n",
      "Training loss: 0.010454683579870911\n",
      "Test loss: 0.044824956053936924\n",
      "Starting epoch 1584\n",
      "Training loss: 0.010355056752069075\n",
      "Test loss: 0.04536393850489899\n",
      "Starting epoch 1585\n",
      "Training loss: 0.010421428676755702\n",
      "Test loss: 0.04565913533722913\n",
      "Starting epoch 1586\n",
      "Training loss: 0.010438555439353967\n",
      "Test loss: 0.045447480899316293\n",
      "Starting epoch 1587\n",
      "Training loss: 0.010326209294869274\n",
      "Test loss: 0.04514576411909527\n",
      "Starting epoch 1588\n",
      "Training loss: 0.010449389911821632\n",
      "Test loss: 0.04531824678458549\n",
      "Starting epoch 1589\n",
      "Training loss: 0.010891126621453489\n",
      "Test loss: 0.04519835956118725\n",
      "Starting epoch 1590\n",
      "Training loss: 0.010383435570802844\n",
      "Test loss: 0.046128719907116006\n",
      "Starting epoch 1591\n",
      "Training loss: 0.010342032862369155\n",
      "Test loss: 0.04557961763607131\n",
      "Starting epoch 1592\n",
      "Training loss: 0.010535600076078391\n",
      "Test loss: 0.045288588437769145\n",
      "Starting epoch 1593\n",
      "Training loss: 0.010347401860673896\n",
      "Test loss: 0.04481215247263511\n",
      "Starting epoch 1594\n",
      "Training loss: 0.010445553168165879\n",
      "Test loss: 0.045093320586063246\n",
      "Starting epoch 1595\n",
      "Training loss: 0.010481015763810424\n",
      "Test loss: 0.04501520787124281\n",
      "Starting epoch 1596\n",
      "Training loss: 0.010527759912561198\n",
      "Test loss: 0.04506253202756246\n",
      "Starting epoch 1597\n",
      "Training loss: 0.010450017623236923\n",
      "Test loss: 0.04574629912773768\n",
      "Starting epoch 1598\n",
      "Training loss: 0.010533091016724462\n",
      "Test loss: 0.04559155677755674\n",
      "Starting epoch 1599\n",
      "Training loss: 0.010334238135179535\n",
      "Test loss: 0.04595322893173606\n",
      "Starting epoch 1600\n",
      "Training loss: 0.010478554369851213\n",
      "Test loss: 0.045647474488726365\n",
      "Starting epoch 1601\n",
      "Training loss: 0.010429283863574754\n",
      "Test loss: 0.04486019640333123\n",
      "Starting epoch 1602\n",
      "Training loss: 0.01039812232932595\n",
      "Test loss: 0.04496357752079213\n",
      "Starting epoch 1603\n",
      "Training loss: 0.010408971458673477\n",
      "Test loss: 0.04533354779360471\n",
      "Starting epoch 1604\n",
      "Training loss: 0.01034791737062032\n",
      "Test loss: 0.04574252554663905\n",
      "Starting epoch 1605\n",
      "Training loss: 0.010527923352039252\n",
      "Test loss: 0.045802741001049675\n",
      "Starting epoch 1606\n",
      "Training loss: 0.01055509522251907\n",
      "Test loss: 0.046015577735724275\n",
      "Starting epoch 1607\n",
      "Training loss: 0.01037165420282571\n",
      "Test loss: 0.045229017044659016\n",
      "Starting epoch 1608\n",
      "Training loss: 0.010478926822543144\n",
      "Test loss: 0.0455496609210968\n",
      "Starting epoch 1609\n",
      "Training loss: 0.010351569101702972\n",
      "Test loss: 0.04519236929438732\n",
      "Starting epoch 1610\n",
      "Training loss: 0.010476618333429586\n",
      "Test loss: 0.04521807241770956\n",
      "Starting epoch 1611\n",
      "Training loss: 0.01062669847771281\n",
      "Test loss: 0.0452900557882256\n",
      "Starting epoch 1612\n",
      "Training loss: 0.01033164525679389\n",
      "Test loss: 0.04603185201132739\n",
      "Starting epoch 1613\n",
      "Training loss: 0.010359459571906777\n",
      "Test loss: 0.04565210288597478\n",
      "Starting epoch 1614\n",
      "Training loss: 0.010594565314469768\n",
      "Test loss: 0.04552974482929265\n",
      "Starting epoch 1615\n",
      "Training loss: 0.010427582398301265\n",
      "Test loss: 0.045966763187337806\n",
      "Starting epoch 1616\n",
      "Training loss: 0.010342155766413837\n",
      "Test loss: 0.045465741168569634\n",
      "Starting epoch 1617\n",
      "Training loss: 0.010361718166558469\n",
      "Test loss: 0.04508468646694113\n",
      "Starting epoch 1618\n",
      "Training loss: 0.010338143384481063\n",
      "Test loss: 0.0450691030257278\n",
      "Starting epoch 1619\n",
      "Training loss: 0.01060364916004607\n",
      "Test loss: 0.04554007492131657\n",
      "Starting epoch 1620\n",
      "Training loss: 0.01031856551827466\n",
      "Test loss: 0.04614618586169349\n",
      "Starting epoch 1621\n",
      "Training loss: 0.010344383795363982\n",
      "Test loss: 0.045630355538041505\n",
      "Starting epoch 1622\n",
      "Training loss: 0.010285630211478373\n",
      "Test loss: 0.045246774054787775\n",
      "Starting epoch 1623\n",
      "Training loss: 0.010845615543791504\n",
      "Test loss: 0.045373115964509825\n",
      "Starting epoch 1624\n",
      "Training loss: 0.010351557376199081\n",
      "Test loss: 0.04483726158462189\n",
      "Starting epoch 1625\n",
      "Training loss: 0.010355104264787963\n",
      "Test loss: 0.04544034186336729\n",
      "Starting epoch 1626\n",
      "Training loss: 0.010301059905867108\n",
      "Test loss: 0.04561441019177437\n",
      "Starting epoch 1627\n",
      "Training loss: 0.010494918165514703\n",
      "Test loss: 0.04577018844860571\n",
      "Starting epoch 1628\n",
      "Training loss: 0.010917227012945003\n",
      "Test loss: 0.04593948385229817\n",
      "Starting epoch 1629\n",
      "Training loss: 0.010302717385233426\n",
      "Test loss: 0.04505097824666235\n",
      "Starting epoch 1630\n",
      "Training loss: 0.010358878212874054\n",
      "Test loss: 0.045205209956124974\n",
      "Starting epoch 1631\n",
      "Training loss: 0.010368383123127163\n",
      "Test loss: 0.04578281532007235\n",
      "Starting epoch 1632\n",
      "Training loss: 0.010333540940993145\n",
      "Test loss: 0.04554117815913977\n",
      "Starting epoch 1633\n",
      "Training loss: 0.010336708766026576\n",
      "Test loss: 0.04552061359087626\n",
      "Starting epoch 1634\n",
      "Training loss: 0.010463128599231361\n",
      "Test loss: 0.04532008756090094\n",
      "Starting epoch 1635\n",
      "Training loss: 0.010412370450183993\n",
      "Test loss: 0.044888505505190954\n",
      "Starting epoch 1636\n",
      "Training loss: 0.010386212164017021\n",
      "Test loss: 0.045391451843358854\n",
      "Starting epoch 1637\n",
      "Training loss: 0.010477604329219608\n",
      "Test loss: 0.04540456893543402\n",
      "Starting epoch 1638\n",
      "Training loss: 0.010553608556873486\n",
      "Test loss: 0.045842523514120666\n",
      "Starting epoch 1639\n",
      "Training loss: 0.010538263490698377\n",
      "Test loss: 0.045162436448865466\n",
      "Starting epoch 1640\n",
      "Training loss: 0.010404208201731815\n",
      "Test loss: 0.045638394438558154\n",
      "Starting epoch 1641\n",
      "Training loss: 0.010321616240945018\n",
      "Test loss: 0.045326986660559974\n",
      "Starting epoch 1642\n",
      "Training loss: 0.010438789598277359\n",
      "Test loss: 0.04556725139695185\n",
      "Starting epoch 1643\n",
      "Training loss: 0.010302550403676072\n",
      "Test loss: 0.04591799279054006\n",
      "Starting epoch 1644\n",
      "Training loss: 0.010352436185921313\n",
      "Test loss: 0.04553152386237074\n",
      "Starting epoch 1645\n",
      "Training loss: 0.010361130639422135\n",
      "Test loss: 0.045586752256861436\n",
      "Starting epoch 1646\n",
      "Training loss: 0.010314407773682328\n",
      "Test loss: 0.045248279830923784\n",
      "Starting epoch 1647\n",
      "Training loss: 0.010303829537063349\n",
      "Test loss: 0.04523433537946807\n",
      "Starting epoch 1648\n",
      "Training loss: 0.010435405752209366\n",
      "Test loss: 0.04526826615134875\n",
      "Starting epoch 1649\n",
      "Training loss: 0.01032677888259536\n",
      "Test loss: 0.045746858059256164\n",
      "Starting epoch 1650\n",
      "Training loss: 0.010312082520762428\n",
      "Test loss: 0.04552724978162183\n",
      "Starting epoch 1651\n",
      "Training loss: 0.01038648253764774\n",
      "Test loss: 0.04572870913479063\n",
      "Starting epoch 1652\n",
      "Training loss: 0.010320439980533279\n",
      "Test loss: 0.04539574751699412\n",
      "Starting epoch 1653\n",
      "Training loss: 0.01035645072821711\n",
      "Test loss: 0.04515384027251491\n",
      "Starting epoch 1654\n",
      "Training loss: 0.010375299910846793\n",
      "Test loss: 0.04557586316433218\n",
      "Starting epoch 1655\n",
      "Training loss: 0.010385333621477495\n",
      "Test loss: 0.045521317739729526\n",
      "Starting epoch 1656\n",
      "Training loss: 0.010335442724591885\n",
      "Test loss: 0.045529767594955584\n",
      "Starting epoch 1657\n",
      "Training loss: 0.010343040415986639\n",
      "Test loss: 0.045257017568305684\n",
      "Starting epoch 1658\n",
      "Training loss: 0.010308611970089499\n",
      "Test loss: 0.045637532654735774\n",
      "Starting epoch 1659\n",
      "Training loss: 0.01043529187130635\n",
      "Test loss: 0.04575933226280742\n",
      "Starting epoch 1660\n",
      "Training loss: 0.010384370206443012\n",
      "Test loss: 0.04582593576223762\n",
      "Starting epoch 1661\n",
      "Training loss: 0.010459172982172887\n",
      "Test loss: 0.045521944485328814\n",
      "Starting epoch 1662\n",
      "Training loss: 0.01028132649352316\n",
      "Test loss: 0.045664710027200205\n",
      "Starting epoch 1663\n",
      "Training loss: 0.01035240329191333\n",
      "Test loss: 0.04557465679115719\n",
      "Starting epoch 1664\n",
      "Training loss: 0.01053685662871013\n",
      "Test loss: 0.04544270314552166\n",
      "Starting epoch 1665\n",
      "Training loss: 0.010466460993544001\n",
      "Test loss: 0.04604590987717664\n",
      "Starting epoch 1666\n",
      "Training loss: 0.01041949812139644\n",
      "Test loss: 0.045486425084096414\n",
      "Starting epoch 1667\n",
      "Training loss: 0.01041014376478117\n",
      "Test loss: 0.044956748132352474\n",
      "Starting epoch 1668\n",
      "Training loss: 0.010334894488580892\n",
      "Test loss: 0.045664565982642\n",
      "Starting epoch 1669\n",
      "Training loss: 0.010322621244876111\n",
      "Test loss: 0.0458954233262274\n",
      "Starting epoch 1670\n",
      "Training loss: 0.010506021362714103\n",
      "Test loss: 0.04573027230799198\n",
      "Starting epoch 1671\n",
      "Training loss: 0.010435091331601143\n",
      "Test loss: 0.045186727273243445\n",
      "Starting epoch 1672\n",
      "Training loss: 0.010289902157592968\n",
      "Test loss: 0.04482661353217231\n",
      "Starting epoch 1673\n",
      "Training loss: 0.010573624266830624\n",
      "Test loss: 0.0450508973940655\n",
      "Starting epoch 1674\n",
      "Training loss: 0.01031635483329902\n",
      "Test loss: 0.04592467237401892\n",
      "Starting epoch 1675\n",
      "Training loss: 0.010372650931726714\n",
      "Test loss: 0.04560709372162819\n",
      "Starting epoch 1676\n",
      "Training loss: 0.010615624640075887\n",
      "Test loss: 0.04550955044450583\n",
      "Starting epoch 1677\n",
      "Training loss: 0.011041578027557154\n",
      "Test loss: 0.04517769572083597\n",
      "Starting epoch 1678\n",
      "Training loss: 0.010352153422646835\n",
      "Test loss: 0.04630722036516225\n",
      "Starting epoch 1679\n",
      "Training loss: 0.010355748663671682\n",
      "Test loss: 0.04604074345142753\n",
      "Starting epoch 1680\n",
      "Training loss: 0.010316132445682268\n",
      "Test loss: 0.04547446510858006\n",
      "Starting epoch 1681\n",
      "Training loss: 0.010290819281315217\n",
      "Test loss: 0.04537393725304692\n",
      "Starting epoch 1682\n",
      "Training loss: 0.010288247029434462\n",
      "Test loss: 0.04542106748731048\n",
      "Starting epoch 1683\n",
      "Training loss: 0.010373081622614723\n",
      "Test loss: 0.04561099368664953\n",
      "Starting epoch 1684\n",
      "Training loss: 0.010322423331195214\n",
      "Test loss: 0.04521630345671265\n",
      "Starting epoch 1685\n",
      "Training loss: 0.010342228409574657\n",
      "Test loss: 0.045590712792343564\n",
      "Starting epoch 1686\n",
      "Training loss: 0.010324081925095105\n",
      "Test loss: 0.045157125357676436\n",
      "Starting epoch 1687\n",
      "Training loss: 0.010387426975076317\n",
      "Test loss: 0.04562980757543334\n",
      "Starting epoch 1688\n",
      "Training loss: 0.010325343783212001\n",
      "Test loss: 0.04597517075362029\n",
      "Starting epoch 1689\n",
      "Training loss: 0.010398135520517826\n",
      "Test loss: 0.045544580866893135\n",
      "Starting epoch 1690\n",
      "Training loss: 0.010475367826760793\n",
      "Test loss: 0.04584758783932085\n",
      "Starting epoch 1691\n",
      "Training loss: 0.010269386571694593\n",
      "Test loss: 0.04618625770564432\n",
      "Starting epoch 1692\n",
      "Training loss: 0.010446490200816607\n",
      "Test loss: 0.04598233807418081\n",
      "Starting epoch 1693\n",
      "Training loss: 0.010332942909759576\n",
      "Test loss: 0.04524307491050826\n",
      "Starting epoch 1694\n",
      "Training loss: 0.010335748893071393\n",
      "Test loss: 0.045564526209124816\n",
      "Starting epoch 1695\n",
      "Training loss: 0.010299281270594382\n",
      "Test loss: 0.04541573701081453\n",
      "Starting epoch 1696\n",
      "Training loss: 0.010300997171367779\n",
      "Test loss: 0.04585633737345537\n",
      "Starting epoch 1697\n",
      "Training loss: 0.010298046100212902\n",
      "Test loss: 0.045466364464826055\n",
      "Starting epoch 1698\n",
      "Training loss: 0.010315757886063858\n",
      "Test loss: 0.04561377423642962\n",
      "Starting epoch 1699\n",
      "Training loss: 0.010291395098215243\n",
      "Test loss: 0.04586551931721193\n",
      "Starting epoch 1700\n",
      "Training loss: 0.010265536636846965\n",
      "Test loss: 0.04561185878184107\n",
      "Starting epoch 1701\n",
      "Training loss: 0.010592933927403122\n",
      "Test loss: 0.04543902587007593\n",
      "Starting epoch 1702\n",
      "Training loss: 0.010295572934947053\n",
      "Test loss: 0.044948259299552\n",
      "Starting epoch 1703\n",
      "Training loss: 0.010558607591102357\n",
      "Test loss: 0.04555205379923185\n",
      "Starting epoch 1704\n",
      "Training loss: 0.010268621284087173\n",
      "Test loss: 0.04618679290568387\n",
      "Starting epoch 1705\n",
      "Training loss: 0.01050871948055068\n",
      "Test loss: 0.04586877867027565\n",
      "Starting epoch 1706\n",
      "Training loss: 0.01029120352058137\n",
      "Test loss: 0.0461526238531978\n",
      "Starting epoch 1707\n",
      "Training loss: 0.010259550186942835\n",
      "Test loss: 0.045638136151764125\n",
      "Starting epoch 1708\n",
      "Training loss: 0.010268588321375066\n",
      "Test loss: 0.04572271797116156\n",
      "Starting epoch 1709\n",
      "Training loss: 0.010376152826747934\n",
      "Test loss: 0.0456454086082953\n",
      "Starting epoch 1710\n",
      "Training loss: 0.010303980777742432\n",
      "Test loss: 0.04538795821092747\n",
      "Starting epoch 1711\n",
      "Training loss: 0.010414232331954066\n",
      "Test loss: 0.045448323297831744\n",
      "Starting epoch 1712\n",
      "Training loss: 0.010303660601255347\n",
      "Test loss: 0.045836993803580604\n",
      "Starting epoch 1713\n",
      "Training loss: 0.010263962747498613\n",
      "Test loss: 0.04592567516697778\n",
      "Starting epoch 1714\n",
      "Training loss: 0.010343446052770634\n",
      "Test loss: 0.0457117218800165\n",
      "Starting epoch 1715\n",
      "Training loss: 0.010303605164660782\n",
      "Test loss: 0.045222167753511004\n",
      "Starting epoch 1716\n",
      "Training loss: 0.010363112096903755\n",
      "Test loss: 0.04542234001888169\n",
      "Starting epoch 1717\n",
      "Training loss: 0.010480575889471124\n",
      "Test loss: 0.04551844533394884\n",
      "Starting epoch 1718\n",
      "Training loss: 0.010258061910567225\n",
      "Test loss: 0.04525781257285012\n",
      "Starting epoch 1719\n",
      "Training loss: 0.010650158630775624\n",
      "Test loss: 0.04540260384480158\n",
      "Starting epoch 1720\n",
      "Training loss: 0.010477606039188925\n",
      "Test loss: 0.04612239063889892\n",
      "Starting epoch 1721\n",
      "Training loss: 0.010326496615517334\n",
      "Test loss: 0.04549019866519504\n",
      "Starting epoch 1722\n",
      "Training loss: 0.010263769368289924\n",
      "Test loss: 0.04524247431092792\n",
      "Starting epoch 1723\n",
      "Training loss: 0.010258695614508918\n",
      "Test loss: 0.0454087576104535\n",
      "Starting epoch 1724\n",
      "Training loss: 0.010579010868658785\n",
      "Test loss: 0.04529844459008287\n",
      "Starting epoch 1725\n",
      "Training loss: 0.01045614097755952\n",
      "Test loss: 0.04518000712549245\n",
      "Starting epoch 1726\n",
      "Training loss: 0.01045002502801477\n",
      "Test loss: 0.04599866333107153\n",
      "Starting epoch 1727\n",
      "Training loss: 0.010349513703315962\n",
      "Test loss: 0.045454359302918114\n",
      "Starting epoch 1728\n",
      "Training loss: 0.010219093809118037\n",
      "Test loss: 0.0457695797085762\n",
      "Starting epoch 1729\n",
      "Training loss: 0.010339515680660967\n",
      "Test loss: 0.045902285172983455\n",
      "Starting epoch 1730\n",
      "Training loss: 0.01039352112251227\n",
      "Test loss: 0.04608878148374734\n",
      "Starting epoch 1731\n",
      "Training loss: 0.010226193433780163\n",
      "Test loss: 0.04529187110839067\n",
      "Starting epoch 1732\n",
      "Training loss: 0.010456941563819275\n",
      "Test loss: 0.045334865235620074\n",
      "Starting epoch 1733\n",
      "Training loss: 0.010235180314935621\n",
      "Test loss: 0.0460576249493493\n",
      "Starting epoch 1734\n",
      "Training loss: 0.010255017149888102\n",
      "Test loss: 0.0457125946327492\n",
      "Starting epoch 1735\n",
      "Training loss: 0.010299504612435083\n",
      "Test loss: 0.04583051469590929\n",
      "Starting epoch 1736\n",
      "Training loss: 0.01031338177682435\n",
      "Test loss: 0.0460175525535036\n",
      "Starting epoch 1737\n",
      "Training loss: 0.01025686103117759\n",
      "Test loss: 0.046049352114399277\n",
      "Starting epoch 1738\n",
      "Training loss: 0.010295150541990507\n",
      "Test loss: 0.04571675729972345\n",
      "Starting epoch 1739\n",
      "Training loss: 0.010267720221862441\n",
      "Test loss: 0.04590488556358549\n",
      "Starting epoch 1740\n",
      "Training loss: 0.010277059807090974\n",
      "Test loss: 0.04566217365640181\n",
      "Starting epoch 1741\n",
      "Training loss: 0.010240674797506606\n",
      "Test loss: 0.04540702824791273\n",
      "Starting epoch 1742\n",
      "Training loss: 0.010409114576998304\n",
      "Test loss: 0.045472234625507285\n",
      "Starting epoch 1743\n",
      "Training loss: 0.010271907478693078\n",
      "Test loss: 0.04541988539750929\n",
      "Starting epoch 1744\n",
      "Training loss: 0.010541491332601328\n",
      "Test loss: 0.045376593040095434\n",
      "Starting epoch 1745\n",
      "Training loss: 0.010311373356790816\n",
      "Test loss: 0.04497962431223304\n",
      "Starting epoch 1746\n",
      "Training loss: 0.010383471816045339\n",
      "Test loss: 0.04539376649039763\n",
      "Starting epoch 1747\n",
      "Training loss: 0.01028581936156652\n",
      "Test loss: 0.04597253517972098\n",
      "Starting epoch 1748\n",
      "Training loss: 0.010346583151792894\n",
      "Test loss: 0.04600289353617915\n",
      "Starting epoch 1749\n",
      "Training loss: 0.010277196757312193\n",
      "Test loss: 0.04556343235351421\n",
      "Starting epoch 1750\n",
      "Training loss: 0.010488420724868774\n",
      "Test loss: 0.04522430620811604\n",
      "Starting epoch 1751\n",
      "Training loss: 0.01031667345249262\n",
      "Test loss: 0.0460250094808914\n",
      "Starting epoch 1752\n",
      "Training loss: 0.010224461998240869\n",
      "Test loss: 0.04571456634611995\n",
      "Starting epoch 1753\n",
      "Training loss: 0.010232980668422629\n",
      "Test loss: 0.045711827360921435\n",
      "Starting epoch 1754\n",
      "Training loss: 0.010405975836710851\n",
      "Test loss: 0.04571177700051555\n",
      "Starting epoch 1755\n",
      "Training loss: 0.01039753185554606\n",
      "Test loss: 0.04630854781027193\n",
      "Starting epoch 1756\n",
      "Training loss: 0.010372646824746836\n",
      "Test loss: 0.04623705817869416\n",
      "Starting epoch 1757\n",
      "Training loss: 0.01034468255143185\n",
      "Test loss: 0.045579530850604726\n",
      "Starting epoch 1758\n",
      "Training loss: 0.010413776121300752\n",
      "Test loss: 0.0452934177937331\n",
      "Starting epoch 1759\n",
      "Training loss: 0.010396193728217335\n",
      "Test loss: 0.04550259091235973\n",
      "Starting epoch 1760\n",
      "Training loss: 0.010311873507548551\n",
      "Test loss: 0.04530180935506468\n",
      "Starting epoch 1761\n",
      "Training loss: 0.010334006037379875\n",
      "Test loss: 0.04582061177050626\n",
      "Starting epoch 1762\n",
      "Training loss: 0.01023791131914639\n",
      "Test loss: 0.045986826704056176\n",
      "Starting epoch 1763\n",
      "Training loss: 0.010630828572711984\n",
      "Test loss: 0.04593138948634819\n",
      "Starting epoch 1764\n",
      "Training loss: 0.010244792350186188\n",
      "Test loss: 0.04525343273524885\n",
      "Starting epoch 1765\n",
      "Training loss: 0.010263467925127412\n",
      "Test loss: 0.04519115126243344\n",
      "Starting epoch 1766\n",
      "Training loss: 0.010205595196820185\n",
      "Test loss: 0.04567346335561187\n",
      "Starting epoch 1767\n",
      "Training loss: 0.010358885251229903\n",
      "Test loss: 0.04596553080611759\n",
      "Starting epoch 1768\n",
      "Training loss: 0.010456341700475724\n",
      "Test loss: 0.045497045403829324\n",
      "Starting epoch 1769\n",
      "Training loss: 0.010337406372437711\n",
      "Test loss: 0.04556550899589503\n",
      "Starting epoch 1770\n",
      "Training loss: 0.01044441475609287\n",
      "Test loss: 0.04553060371566702\n",
      "Starting epoch 1771\n",
      "Training loss: 0.010234928430348147\n",
      "Test loss: 0.045970864318035265\n",
      "Starting epoch 1772\n",
      "Training loss: 0.010434307081655401\n",
      "Test loss: 0.0457631161229478\n",
      "Starting epoch 1773\n",
      "Training loss: 0.010352738018407196\n",
      "Test loss: 0.04513782973366755\n",
      "Starting epoch 1774\n",
      "Training loss: 0.010316499508917332\n",
      "Test loss: 0.045840308208156516\n",
      "Starting epoch 1775\n",
      "Training loss: 0.010210204838973577\n",
      "Test loss: 0.04629660704759536\n",
      "Starting epoch 1776\n",
      "Training loss: 0.01034308511947022\n",
      "Test loss: 0.04582786408287508\n",
      "Starting epoch 1777\n",
      "Training loss: 0.010200779136942058\n",
      "Test loss: 0.04533907605542077\n",
      "Starting epoch 1778\n",
      "Training loss: 0.010342668024365042\n",
      "Test loss: 0.04558966377819026\n",
      "Starting epoch 1779\n",
      "Training loss: 0.010246679431102315\n",
      "Test loss: 0.04605983894456316\n",
      "Starting epoch 1780\n",
      "Training loss: 0.010254653124902093\n",
      "Test loss: 0.04617848785387145\n",
      "Starting epoch 1781\n",
      "Training loss: 0.010242917574942112\n",
      "Test loss: 0.046099661676972\n",
      "Starting epoch 1782\n",
      "Training loss: 0.010229687649207037\n",
      "Test loss: 0.045764220533547575\n",
      "Starting epoch 1783\n",
      "Training loss: 0.010360297075182687\n",
      "Test loss: 0.045511662408157634\n",
      "Starting epoch 1784\n",
      "Training loss: 0.010266084254520838\n",
      "Test loss: 0.045288647490519064\n",
      "Starting epoch 1785\n",
      "Training loss: 0.010274945605607306\n",
      "Test loss: 0.045784706318819965\n",
      "Starting epoch 1786\n",
      "Training loss: 0.010260452592714887\n",
      "Test loss: 0.04602068566061832\n",
      "Starting epoch 1787\n",
      "Training loss: 0.010235809369898234\n",
      "Test loss: 0.04549527430423984\n",
      "Starting epoch 1788\n",
      "Training loss: 0.010179897724482858\n",
      "Test loss: 0.0453685869773229\n",
      "Starting epoch 1789\n",
      "Training loss: 0.010324820555502275\n",
      "Test loss: 0.045648230584683244\n",
      "Starting epoch 1790\n",
      "Training loss: 0.010205792751712878\n",
      "Test loss: 0.04561926252036183\n",
      "Starting epoch 1791\n",
      "Training loss: 0.010211907113428976\n",
      "Test loss: 0.045714619397013274\n",
      "Starting epoch 1792\n",
      "Training loss: 0.010309969304037875\n",
      "Test loss: 0.04568689068158468\n",
      "Starting epoch 1793\n",
      "Training loss: 0.010240157287506784\n",
      "Test loss: 0.045383959732673784\n",
      "Starting epoch 1794\n",
      "Training loss: 0.0102087834727813\n",
      "Test loss: 0.04547390383150843\n",
      "Starting epoch 1795\n",
      "Training loss: 0.010216500808591725\n",
      "Test loss: 0.04563984012714139\n",
      "Starting epoch 1796\n",
      "Training loss: 0.010381163449072446\n",
      "Test loss: 0.0457531765655235\n",
      "Starting epoch 1797\n",
      "Training loss: 0.010624856733884967\n",
      "Test loss: 0.04545243249999152\n",
      "Starting epoch 1798\n",
      "Training loss: 0.010419392607129012\n",
      "Test loss: 0.04609833726728404\n",
      "Starting epoch 1799\n",
      "Training loss: 0.010292000274677745\n",
      "Test loss: 0.04571561887860298\n",
      "Starting epoch 1800\n",
      "Training loss: 0.010258742211172815\n",
      "Test loss: 0.04544702972526903\n",
      "Starting epoch 1801\n",
      "Training loss: 0.010450085548714537\n",
      "Test loss: 0.04599918011162016\n",
      "Starting epoch 1802\n",
      "Training loss: 0.010270434446999283\n",
      "Test loss: 0.04555374301142163\n",
      "Starting epoch 1803\n",
      "Training loss: 0.010232782540995567\n",
      "Test loss: 0.0458053066222756\n",
      "Starting epoch 1804\n",
      "Training loss: 0.010231614021248505\n",
      "Test loss: 0.04585497922919415\n",
      "Starting epoch 1805\n",
      "Training loss: 0.010194115325442104\n",
      "Test loss: 0.04557082636488809\n",
      "Starting epoch 1806\n",
      "Training loss: 0.010414871089465793\n",
      "Test loss: 0.046043956790257384\n",
      "Starting epoch 1807\n",
      "Training loss: 0.010265220231452927\n",
      "Test loss: 0.04549622280454194\n",
      "Starting epoch 1808\n",
      "Training loss: 0.010222314444721723\n",
      "Test loss: 0.04584180273943477\n",
      "Starting epoch 1809\n",
      "Training loss: 0.010386816256481116\n",
      "Test loss: 0.045510737190919894\n",
      "Starting epoch 1810\n",
      "Training loss: 0.010526516719064752\n",
      "Test loss: 0.04598453965176035\n",
      "Starting epoch 1811\n",
      "Training loss: 0.01028635212388195\n",
      "Test loss: 0.0454492030872239\n",
      "Starting epoch 1812\n",
      "Training loss: 0.010213741620422387\n",
      "Test loss: 0.04587446511895568\n",
      "Starting epoch 1813\n",
      "Training loss: 0.010161425147327732\n",
      "Test loss: 0.04607956056241636\n",
      "Starting epoch 1814\n",
      "Training loss: 0.010669713565072075\n",
      "Test loss: 0.04603021095196406\n",
      "Starting epoch 1815\n",
      "Training loss: 0.010841272924034322\n",
      "Test loss: 0.04654449514216847\n",
      "Starting epoch 1816\n",
      "Training loss: 0.010218628743266473\n",
      "Test loss: 0.045050181448459625\n",
      "Starting epoch 1817\n",
      "Training loss: 0.010194378584379056\n",
      "Test loss: 0.045390708234023164\n",
      "Starting epoch 1818\n",
      "Training loss: 0.010196118294948438\n",
      "Test loss: 0.04571384626130263\n",
      "Starting epoch 1819\n",
      "Training loss: 0.010207806377992278\n",
      "Test loss: 0.045723949593526346\n",
      "Starting epoch 1820\n",
      "Training loss: 0.010152101150301636\n",
      "Test loss: 0.045936215186008704\n",
      "Starting epoch 1821\n",
      "Training loss: 0.010186078423847918\n",
      "Test loss: 0.045648489630332696\n",
      "Starting epoch 1822\n",
      "Training loss: 0.010254449546947831\n",
      "Test loss: 0.04573554559438317\n",
      "Starting epoch 1823\n",
      "Training loss: 0.010294456935686166\n",
      "Test loss: 0.045317967180852535\n",
      "Starting epoch 1824\n",
      "Training loss: 0.010193699176927081\n",
      "Test loss: 0.04575374142991172\n",
      "Starting epoch 1825\n",
      "Training loss: 0.010266958781685985\n",
      "Test loss: 0.04606102738115522\n",
      "Starting epoch 1826\n",
      "Training loss: 0.010293867439031601\n",
      "Test loss: 0.045945079928195035\n",
      "Starting epoch 1827\n",
      "Training loss: 0.010188987051121524\n",
      "Test loss: 0.045780834914357575\n",
      "Starting epoch 1828\n",
      "Training loss: 0.010145746843248118\n",
      "Test loss: 0.0458730834501761\n",
      "Starting epoch 1829\n",
      "Training loss: 0.010160179839271014\n",
      "Test loss: 0.04581345093471033\n",
      "Starting epoch 1830\n",
      "Training loss: 0.010210220778330427\n",
      "Test loss: 0.04589287577955811\n",
      "Starting epoch 1831\n",
      "Training loss: 0.010565231508407437\n",
      "Test loss: 0.046100737871947114\n",
      "Starting epoch 1832\n",
      "Training loss: 0.010445108820424705\n",
      "Test loss: 0.045298002384327074\n",
      "Starting epoch 1833\n",
      "Training loss: 0.010202539046524002\n",
      "Test loss: 0.04572267788979742\n",
      "Starting epoch 1834\n",
      "Training loss: 0.010278891222398789\n",
      "Test loss: 0.04552824229553894\n",
      "Starting epoch 1835\n",
      "Training loss: 0.010228117286670403\n",
      "Test loss: 0.045497677116482345\n",
      "Starting epoch 1836\n",
      "Training loss: 0.010263365159024958\n",
      "Test loss: 0.04615010597087719\n",
      "Starting epoch 1837\n",
      "Training loss: 0.01013950308876448\n",
      "Test loss: 0.045785359762333055\n",
      "Starting epoch 1838\n",
      "Training loss: 0.010174102424720272\n",
      "Test loss: 0.04563633373214139\n",
      "Starting epoch 1839\n",
      "Training loss: 0.010282476615832477\n",
      "Test loss: 0.045277162144581475\n",
      "Starting epoch 1840\n",
      "Training loss: 0.010207905556212683\n",
      "Test loss: 0.045126951541061756\n",
      "Starting epoch 1841\n",
      "Training loss: 0.010257075571256583\n",
      "Test loss: 0.045587904199405956\n",
      "Starting epoch 1842\n",
      "Training loss: 0.01023271058487599\n",
      "Test loss: 0.04617476601291586\n",
      "Starting epoch 1843\n",
      "Training loss: 0.010248400629727079\n",
      "Test loss: 0.046427578975756965\n",
      "Starting epoch 1844\n",
      "Training loss: 0.010187021334518175\n",
      "Test loss: 0.045701655110827195\n",
      "Starting epoch 1845\n",
      "Training loss: 0.010154341439121082\n",
      "Test loss: 0.04545931014473791\n",
      "Starting epoch 1846\n",
      "Training loss: 0.010213156879619986\n",
      "Test loss: 0.04582860031061702\n",
      "Starting epoch 1847\n",
      "Training loss: 0.010395309674080278\n",
      "Test loss: 0.045489259616092396\n",
      "Starting epoch 1848\n",
      "Training loss: 0.010393652301587042\n",
      "Test loss: 0.046122664171788424\n",
      "Starting epoch 1849\n",
      "Training loss: 0.01026685884008642\n",
      "Test loss: 0.046376225573045236\n",
      "Starting epoch 1850\n",
      "Training loss: 0.010200627835192641\n",
      "Test loss: 0.045705662971293484\n",
      "Starting epoch 1851\n",
      "Training loss: 0.010345940172794412\n",
      "Test loss: 0.045697955069718535\n",
      "Starting epoch 1852\n",
      "Training loss: 0.010239644953217662\n",
      "Test loss: 0.04536013198257596\n",
      "Starting epoch 1853\n",
      "Training loss: 0.01029323981922181\n",
      "Test loss: 0.0458719821439849\n",
      "Starting epoch 1854\n",
      "Training loss: 0.010327057164834171\n",
      "Test loss: 0.04635958635696658\n",
      "Starting epoch 1855\n",
      "Training loss: 0.01034303088901473\n",
      "Test loss: 0.04554131916827626\n",
      "Starting epoch 1856\n",
      "Training loss: 0.01034240994113879\n",
      "Test loss: 0.04598576879059827\n",
      "Starting epoch 1857\n",
      "Training loss: 0.01014505990887763\n",
      "Test loss: 0.046331680207340804\n",
      "Starting epoch 1858\n",
      "Training loss: 0.010279599164963746\n",
      "Test loss: 0.046091996133327484\n",
      "Starting epoch 1859\n",
      "Training loss: 0.010268328512911915\n",
      "Test loss: 0.045706344595937816\n",
      "Starting epoch 1860\n",
      "Training loss: 0.010228721012712503\n",
      "Test loss: 0.04602512544779866\n",
      "Starting epoch 1861\n",
      "Training loss: 0.01017856738362156\n",
      "Test loss: 0.04569098877686041\n",
      "Starting epoch 1862\n",
      "Training loss: 0.010165212523253237\n",
      "Test loss: 0.04553155987351029\n",
      "Starting epoch 1863\n",
      "Training loss: 0.010238428401653885\n",
      "Test loss: 0.04573659564333933\n",
      "Starting epoch 1864\n",
      "Training loss: 0.010193266646295298\n",
      "Test loss: 0.04555565739671389\n",
      "Starting epoch 1865\n",
      "Training loss: 0.010250755585730076\n",
      "Test loss: 0.045941671908453656\n",
      "Starting epoch 1866\n",
      "Training loss: 0.010254581993232008\n",
      "Test loss: 0.04602558083004422\n",
      "Starting epoch 1867\n",
      "Training loss: 0.010199575730767406\n",
      "Test loss: 0.045882117831044726\n",
      "Starting epoch 1868\n",
      "Training loss: 0.010235929968537854\n",
      "Test loss: 0.04570476434848927\n",
      "Starting epoch 1869\n",
      "Training loss: 0.010293907470634727\n",
      "Test loss: 0.046135619972591045\n",
      "Starting epoch 1870\n",
      "Training loss: 0.010260462165489549\n",
      "Test loss: 0.04545604120250101\n",
      "Starting epoch 1871\n",
      "Training loss: 0.010229077068020086\n",
      "Test loss: 0.045901343778327656\n",
      "Starting epoch 1872\n",
      "Training loss: 0.01017445515169472\n",
      "Test loss: 0.046244651079177856\n",
      "Starting epoch 1873\n",
      "Training loss: 0.010229682412426003\n",
      "Test loss: 0.045895442435586895\n",
      "Starting epoch 1874\n",
      "Training loss: 0.010203371404624377\n",
      "Test loss: 0.04614421325149359\n",
      "Starting epoch 1875\n",
      "Training loss: 0.010209413657545066\n",
      "Test loss: 0.04603926009602017\n",
      "Starting epoch 1876\n",
      "Training loss: 0.010268877260386944\n",
      "Test loss: 0.04561020640863313\n",
      "Starting epoch 1877\n",
      "Training loss: 0.010136481786726928\n",
      "Test loss: 0.04600948081524284\n",
      "Starting epoch 1878\n",
      "Training loss: 0.010235484521530691\n",
      "Test loss: 0.04571246259190418\n",
      "Starting epoch 1879\n",
      "Training loss: 0.010311219062595094\n",
      "Test loss: 0.04542856525491785\n",
      "Starting epoch 1880\n",
      "Training loss: 0.010312902176233589\n",
      "Test loss: 0.0458243181584058\n",
      "Starting epoch 1881\n",
      "Training loss: 0.010142409899195686\n",
      "Test loss: 0.045526093147971014\n",
      "Starting epoch 1882\n",
      "Training loss: 0.010205207133024443\n",
      "Test loss: 0.045389019366767674\n",
      "Starting epoch 1883\n",
      "Training loss: 0.010106882233111585\n",
      "Test loss: 0.04534262846465464\n",
      "Starting epoch 1884\n",
      "Training loss: 0.010147666802904645\n",
      "Test loss: 0.04552335305898278\n",
      "Starting epoch 1885\n",
      "Training loss: 0.010266944231679205\n",
      "Test loss: 0.04575439321774023\n",
      "Starting epoch 1886\n",
      "Training loss: 0.010146446098558238\n",
      "Test loss: 0.0453031932314237\n",
      "Starting epoch 1887\n",
      "Training loss: 0.010189202400382425\n",
      "Test loss: 0.04551195794785464\n",
      "Starting epoch 1888\n",
      "Training loss: 0.010187912518616582\n",
      "Test loss: 0.045642483013647574\n",
      "Starting epoch 1889\n",
      "Training loss: 0.010291411312388593\n",
      "Test loss: 0.04550641871712826\n",
      "Starting epoch 1890\n",
      "Training loss: 0.010302234227295782\n",
      "Test loss: 0.04612279931704203\n",
      "Starting epoch 1891\n",
      "Training loss: 0.010136429144100088\n",
      "Test loss: 0.045523880808441726\n",
      "Starting epoch 1892\n",
      "Training loss: 0.010140171396683474\n",
      "Test loss: 0.045216026750427706\n",
      "Starting epoch 1893\n",
      "Training loss: 0.010447221258502514\n",
      "Test loss: 0.045432469428137494\n",
      "Starting epoch 1894\n",
      "Training loss: 0.010492645387278229\n",
      "Test loss: 0.04521283038236477\n",
      "Starting epoch 1895\n",
      "Training loss: 0.010141110505725517\n",
      "Test loss: 0.0459245553723088\n",
      "Starting epoch 1896\n",
      "Training loss: 0.010293339592877959\n",
      "Test loss: 0.04579851141682378\n",
      "Starting epoch 1897\n",
      "Training loss: 0.010105567373579642\n",
      "Test loss: 0.045446413534658926\n",
      "Starting epoch 1898\n",
      "Training loss: 0.010198990753317465\n",
      "Test loss: 0.045603961787290044\n",
      "Starting epoch 1899\n",
      "Training loss: 0.010157173484197406\n",
      "Test loss: 0.04593899004437305\n",
      "Starting epoch 1900\n",
      "Training loss: 0.010306803921817756\n",
      "Test loss: 0.04563078601603155\n",
      "Starting epoch 1901\n",
      "Training loss: 0.010160357431798685\n",
      "Test loss: 0.04601213825797593\n",
      "Starting epoch 1902\n",
      "Training loss: 0.010144003346318105\n",
      "Test loss: 0.045972308764855065\n",
      "Starting epoch 1903\n",
      "Training loss: 0.010164824802978117\n",
      "Test loss: 0.046012389232163074\n",
      "Starting epoch 1904\n",
      "Training loss: 0.010136136113375913\n",
      "Test loss: 0.045979580376297235\n",
      "Starting epoch 1905\n",
      "Training loss: 0.010157020243464923\n",
      "Test loss: 0.04580037288919643\n",
      "Starting epoch 1906\n",
      "Training loss: 0.010208215595024531\n",
      "Test loss: 0.04587743445127099\n",
      "Starting epoch 1907\n",
      "Training loss: 0.010136349401513084\n",
      "Test loss: 0.04609899181458685\n",
      "Starting epoch 1908\n",
      "Training loss: 0.010522010522421266\n",
      "Test loss: 0.04576830772889985\n",
      "Starting epoch 1909\n",
      "Training loss: 0.010353282643634765\n",
      "Test loss: 0.045093652412847234\n",
      "Starting epoch 1910\n",
      "Training loss: 0.010229750857001445\n",
      "Test loss: 0.04496771110980599\n",
      "Starting epoch 1911\n",
      "Training loss: 0.010435602994116604\n",
      "Test loss: 0.045713538510931864\n",
      "Starting epoch 1912\n",
      "Training loss: 0.010196527374572441\n",
      "Test loss: 0.0451994352042675\n",
      "Starting epoch 1913\n",
      "Training loss: 0.010180218572743604\n",
      "Test loss: 0.045225372055062545\n",
      "Starting epoch 1914\n",
      "Training loss: 0.010218958133740014\n",
      "Test loss: 0.045940003599281666\n",
      "Starting epoch 1915\n",
      "Training loss: 0.010256331688800796\n",
      "Test loss: 0.045693022233468515\n",
      "Starting epoch 1916\n",
      "Training loss: 0.01011452069658725\n",
      "Test loss: 0.045196180818257506\n",
      "Starting epoch 1917\n",
      "Training loss: 0.010214381179482233\n",
      "Test loss: 0.04511488856816733\n",
      "Starting epoch 1918\n",
      "Training loss: 0.010153437277576963\n",
      "Test loss: 0.04568893869441969\n",
      "Starting epoch 1919\n",
      "Training loss: 0.010205404573410261\n",
      "Test loss: 0.0459995796834981\n",
      "Starting epoch 1920\n",
      "Training loss: 0.010134554086405723\n",
      "Test loss: 0.04626050384508239\n",
      "Starting epoch 1921\n",
      "Training loss: 0.01013948140879635\n",
      "Test loss: 0.045873819263996904\n",
      "Starting epoch 1922\n",
      "Training loss: 0.010173135253860325\n",
      "Test loss: 0.0459885190896414\n",
      "Starting epoch 1923\n",
      "Training loss: 0.010238146821617103\n",
      "Test loss: 0.04613767177970321\n",
      "Starting epoch 1924\n",
      "Training loss: 0.010092770604447264\n",
      "Test loss: 0.045493943685734714\n",
      "Starting epoch 1925\n",
      "Training loss: 0.010124721472747013\n",
      "Test loss: 0.045366855131255254\n",
      "Starting epoch 1926\n",
      "Training loss: 0.01012497638031596\n",
      "Test loss: 0.04529296924118643\n",
      "Starting epoch 1927\n",
      "Training loss: 0.010159452873297403\n",
      "Test loss: 0.04557965137064457\n",
      "Starting epoch 1928\n",
      "Training loss: 0.010258748898374253\n",
      "Test loss: 0.045649482420197236\n",
      "Starting epoch 1929\n",
      "Training loss: 0.010244079346417403\n",
      "Test loss: 0.0459762721977852\n",
      "Starting epoch 1930\n",
      "Training loss: 0.010129854739567295\n",
      "Test loss: 0.04603999311587325\n",
      "Starting epoch 1931\n",
      "Training loss: 0.010163084420635075\n",
      "Test loss: 0.045929113747896974\n",
      "Starting epoch 1932\n",
      "Training loss: 0.010155982292089307\n",
      "Test loss: 0.045858081361209904\n",
      "Starting epoch 1933\n",
      "Training loss: 0.010099098162695032\n",
      "Test loss: 0.045709914079418885\n",
      "Starting epoch 1934\n",
      "Training loss: 0.01017035186657163\n",
      "Test loss: 0.045619887196355395\n",
      "Starting epoch 1935\n",
      "Training loss: 0.010319326622564285\n",
      "Test loss: 0.0458011153947424\n",
      "Starting epoch 1936\n",
      "Training loss: 0.010234949064486827\n",
      "Test loss: 0.04521445074567088\n",
      "Starting epoch 1937\n",
      "Training loss: 0.010090895867372145\n",
      "Test loss: 0.04553849367355859\n",
      "Starting epoch 1938\n",
      "Training loss: 0.010203833020002138\n",
      "Test loss: 0.04559318914457604\n",
      "Starting epoch 1939\n",
      "Training loss: 0.010210270703327461\n",
      "Test loss: 0.04525329503748152\n",
      "Starting epoch 1940\n",
      "Training loss: 0.010477456371070908\n",
      "Test loss: 0.04519122880366114\n",
      "Starting epoch 1941\n",
      "Training loss: 0.010148905232915135\n",
      "Test loss: 0.04614466939259459\n",
      "Starting epoch 1942\n",
      "Training loss: 0.010225943030148257\n",
      "Test loss: 0.04611669108271599\n",
      "Starting epoch 1943\n",
      "Training loss: 0.010185305792532984\n",
      "Test loss: 0.045944535069995456\n",
      "Starting epoch 1944\n",
      "Training loss: 0.010240874863916734\n",
      "Test loss: 0.04575875774025917\n",
      "Starting epoch 1945\n",
      "Training loss: 0.010095205325938638\n",
      "Test loss: 0.04599022892890153\n",
      "Starting epoch 1946\n",
      "Training loss: 0.010122625233574968\n",
      "Test loss: 0.04589730970285557\n",
      "Starting epoch 1947\n",
      "Training loss: 0.010141572060032945\n",
      "Test loss: 0.0459332262614259\n",
      "Starting epoch 1948\n",
      "Training loss: 0.01013700896110691\n",
      "Test loss: 0.04593634633002458\n",
      "Starting epoch 1949\n",
      "Training loss: 0.01009886185104241\n",
      "Test loss: 0.04596534909473525\n",
      "Starting epoch 1950\n",
      "Training loss: 0.010088703396623253\n",
      "Test loss: 0.045811933361821704\n",
      "Starting epoch 1951\n",
      "Training loss: 0.010269760512975885\n",
      "Test loss: 0.04599437227955571\n",
      "Starting epoch 1952\n",
      "Training loss: 0.010213783713149243\n",
      "Test loss: 0.04549448688824972\n",
      "Starting epoch 1953\n",
      "Training loss: 0.010458808010596721\n",
      "Test loss: 0.04589717573037854\n",
      "Starting epoch 1954\n",
      "Training loss: 0.01009699106827134\n",
      "Test loss: 0.04658918524229968\n",
      "Starting epoch 1955\n",
      "Training loss: 0.010148996487259865\n",
      "Test loss: 0.04601717147010344\n",
      "Starting epoch 1956\n",
      "Training loss: 0.010158988463951916\n",
      "Test loss: 0.0458208070722995\n",
      "Starting epoch 1957\n",
      "Training loss: 0.01017696066896935\n",
      "Test loss: 0.04588038626092451\n",
      "Starting epoch 1958\n",
      "Training loss: 0.010161538974794208\n",
      "Test loss: 0.04581676878862911\n",
      "Starting epoch 1959\n",
      "Training loss: 0.010159659626908967\n",
      "Test loss: 0.045628467091807616\n",
      "Starting epoch 1960\n",
      "Training loss: 0.010131104177505266\n",
      "Test loss: 0.045841859032710396\n",
      "Starting epoch 1961\n",
      "Training loss: 0.010134160533913823\n",
      "Test loss: 0.04605723427677596\n",
      "Starting epoch 1962\n",
      "Training loss: 0.010116882897058472\n",
      "Test loss: 0.04611507775606932\n",
      "Starting epoch 1963\n",
      "Training loss: 0.01015161519839627\n",
      "Test loss: 0.04604096793466144\n",
      "Starting epoch 1964\n",
      "Training loss: 0.010405251817380796\n",
      "Test loss: 0.045600794393707206\n",
      "Starting epoch 1965\n",
      "Training loss: 0.010073646460278113\n",
      "Test loss: 0.04605623596796283\n",
      "Starting epoch 1966\n",
      "Training loss: 0.01024844787526326\n",
      "Test loss: 0.045900928477446236\n",
      "Starting epoch 1967\n",
      "Training loss: 0.010165597968658463\n",
      "Test loss: 0.04585770807332463\n",
      "Starting epoch 1968\n",
      "Training loss: 0.010100198405810067\n",
      "Test loss: 0.04541447096400791\n",
      "Starting epoch 1969\n",
      "Training loss: 0.010202995241909731\n",
      "Test loss: 0.045203067293321644\n",
      "Starting epoch 1970\n",
      "Training loss: 0.010121252418297236\n",
      "Test loss: 0.04563379743032985\n",
      "Starting epoch 1971\n",
      "Training loss: 0.010256562519391051\n",
      "Test loss: 0.04606749890027223\n",
      "Starting epoch 1972\n",
      "Training loss: 0.010208614475902964\n",
      "Test loss: 0.04646332382603928\n",
      "Starting epoch 1973\n",
      "Training loss: 0.010159900717315127\n",
      "Test loss: 0.046552253818070446\n",
      "Starting epoch 1974\n",
      "Training loss: 0.010284744187823085\n",
      "Test loss: 0.045772571944528155\n",
      "Starting epoch 1975\n",
      "Training loss: 0.010088908180716585\n",
      "Test loss: 0.045303408746366146\n",
      "Starting epoch 1976\n",
      "Training loss: 0.010451605848845889\n",
      "Test loss: 0.04580470202145753\n",
      "Starting epoch 1977\n",
      "Training loss: 0.010148029438540583\n",
      "Test loss: 0.046558158817114656\n",
      "Starting epoch 1978\n",
      "Training loss: 0.010145332648983745\n",
      "Test loss: 0.04644612416073128\n",
      "Starting epoch 1979\n",
      "Training loss: 0.01011968481919316\n",
      "Test loss: 0.04630191168851323\n",
      "Starting epoch 1980\n",
      "Training loss: 0.010122066027805453\n",
      "Test loss: 0.04600274728404151\n",
      "Starting epoch 1981\n",
      "Training loss: 0.010275960396059224\n",
      "Test loss: 0.045785389461175156\n",
      "Starting epoch 1982\n",
      "Training loss: 0.010080802689504917\n",
      "Test loss: 0.04566706827393285\n",
      "Starting epoch 1983\n",
      "Training loss: 0.010109161454268167\n",
      "Test loss: 0.04572067492538028\n",
      "Starting epoch 1984\n",
      "Training loss: 0.010514265094257768\n",
      "Test loss: 0.04591629419613768\n",
      "Starting epoch 1985\n",
      "Training loss: 0.01021308349598138\n",
      "Test loss: 0.04658159930948858\n",
      "Starting epoch 1986\n",
      "Training loss: 0.010126683769411728\n",
      "Test loss: 0.04567125329264888\n",
      "Starting epoch 1987\n",
      "Training loss: 0.01011414781640299\n",
      "Test loss: 0.045674089756276876\n",
      "Starting epoch 1988\n",
      "Training loss: 0.01023670921071631\n",
      "Test loss: 0.046096111475317565\n",
      "Starting epoch 1989\n",
      "Training loss: 0.010082290019290369\n",
      "Test loss: 0.045554515388276845\n",
      "Starting epoch 1990\n",
      "Training loss: 0.010129625466270525\n",
      "Test loss: 0.045551572270967344\n",
      "Starting epoch 1991\n",
      "Training loss: 0.01013154527325122\n",
      "Test loss: 0.04575188085436821\n",
      "Starting epoch 1992\n",
      "Training loss: 0.01022350720939089\n",
      "Test loss: 0.045782405607126375\n",
      "Starting epoch 1993\n",
      "Training loss: 0.010131332580549796\n",
      "Test loss: 0.046230495941859705\n",
      "Starting epoch 1994\n",
      "Training loss: 0.01009435452459777\n",
      "Test loss: 0.046176287725015926\n",
      "Starting epoch 1995\n",
      "Training loss: 0.010259605226580237\n",
      "Test loss: 0.045760286764966115\n",
      "Starting epoch 1996\n",
      "Training loss: 0.010146374982155737\n",
      "Test loss: 0.04553208175908636\n",
      "Starting epoch 1997\n",
      "Training loss: 0.010129542349547636\n",
      "Test loss: 0.04574883687827322\n",
      "Starting epoch 1998\n",
      "Training loss: 0.010152904744275281\n",
      "Test loss: 0.04554201731527293\n",
      "Starting epoch 1999\n",
      "Training loss: 0.010079877954892447\n",
      "Test loss: 0.04597597845174648\n",
      "Starting epoch 2000\n",
      "Training loss: 0.010110060608045\n",
      "Test loss: 0.04584546069856043\n",
      "Starting epoch 2001\n",
      "Training loss: 0.010093084093732912\n",
      "Test loss: 0.04555924278166559\n",
      "Starting epoch 2002\n",
      "Training loss: 0.010093310496724043\n",
      "Test loss: 0.045431082171422464\n",
      "Starting epoch 2003\n",
      "Training loss: 0.01015435707312627\n",
      "Test loss: 0.045628717790047325\n",
      "Starting epoch 2004\n",
      "Training loss: 0.010086529705001682\n",
      "Test loss: 0.04537212641702758\n",
      "Starting epoch 2005\n",
      "Training loss: 0.010074786796066606\n",
      "Test loss: 0.04551380693360611\n",
      "Starting epoch 2006\n",
      "Training loss: 0.010184860177582404\n",
      "Test loss: 0.04569109681027907\n",
      "Starting epoch 2007\n",
      "Training loss: 0.01021674455555736\n",
      "Test loss: 0.045452942312867554\n",
      "Starting epoch 2008\n",
      "Training loss: 0.010100020798014813\n",
      "Test loss: 0.04523861580700786\n",
      "Starting epoch 2009\n",
      "Training loss: 0.010182026781324969\n",
      "Test loss: 0.04546335919035806\n",
      "Starting epoch 2010\n",
      "Training loss: 0.01009451655945817\n",
      "Test loss: 0.04543752457808565\n",
      "Starting epoch 2011\n",
      "Training loss: 0.010086159511912064\n",
      "Test loss: 0.04584710341360834\n",
      "Starting epoch 2012\n",
      "Training loss: 0.010122047172340214\n",
      "Test loss: 0.04578235248724619\n",
      "Starting epoch 2013\n",
      "Training loss: 0.010065966484243752\n",
      "Test loss: 0.04583771168081849\n",
      "Starting epoch 2014\n",
      "Training loss: 0.010048444138565024\n",
      "Test loss: 0.045897724382855276\n",
      "Starting epoch 2015\n",
      "Training loss: 0.010068318409631486\n",
      "Test loss: 0.045586933140401485\n",
      "Starting epoch 2016\n",
      "Training loss: 0.01009090976087285\n",
      "Test loss: 0.045598374058802925\n",
      "Starting epoch 2017\n",
      "Training loss: 0.010393941011585172\n",
      "Test loss: 0.0455718586842219\n",
      "Starting epoch 2018\n",
      "Training loss: 0.01006603543264944\n",
      "Test loss: 0.045109370516406164\n",
      "Starting epoch 2019\n",
      "Training loss: 0.010188532718380943\n",
      "Test loss: 0.04540618509054184\n",
      "Starting epoch 2020\n",
      "Training loss: 0.010316527860819316\n",
      "Test loss: 0.04547030796055441\n",
      "Starting epoch 2021\n",
      "Training loss: 0.01015659479699174\n",
      "Test loss: 0.045106390974035966\n",
      "Starting epoch 2022\n",
      "Training loss: 0.010353958050979942\n",
      "Test loss: 0.04590991042830326\n",
      "Starting epoch 2023\n",
      "Training loss: 0.010280610184322615\n",
      "Test loss: 0.04644995320726324\n",
      "Starting epoch 2024\n",
      "Training loss: 0.010317778153741946\n",
      "Test loss: 0.045606380259549176\n",
      "Starting epoch 2025\n",
      "Training loss: 0.010143511791209706\n",
      "Test loss: 0.04569980060612714\n",
      "Starting epoch 2026\n",
      "Training loss: 0.010161751881241798\n",
      "Test loss: 0.045351625592620286\n",
      "Starting epoch 2027\n",
      "Training loss: 0.010173672703323795\n",
      "Test loss: 0.04534341753632934\n",
      "Starting epoch 2028\n",
      "Training loss: 0.01019461170510679\n",
      "Test loss: 0.04559426775409116\n",
      "Starting epoch 2029\n",
      "Training loss: 0.010110041096073682\n",
      "Test loss: 0.045326991075718845\n",
      "Starting epoch 2030\n",
      "Training loss: 0.010141557311547584\n",
      "Test loss: 0.0458678820480903\n",
      "Starting epoch 2031\n",
      "Training loss: 0.010215809614565528\n",
      "Test loss: 0.04558940355976423\n",
      "Starting epoch 2032\n",
      "Training loss: 0.010107721255512023\n",
      "Test loss: 0.045363838887876935\n",
      "Starting epoch 2033\n",
      "Training loss: 0.010088840896477465\n",
      "Test loss: 0.045669762072739775\n",
      "Starting epoch 2034\n",
      "Training loss: 0.01037887477728187\n",
      "Test loss: 0.04578016036086612\n",
      "Starting epoch 2035\n",
      "Training loss: 0.01008063128798223\n",
      "Test loss: 0.04647921688026852\n",
      "Starting epoch 2036\n",
      "Training loss: 0.010078833530061558\n",
      "Test loss: 0.04610038286557904\n",
      "Starting epoch 2037\n",
      "Training loss: 0.01009576886770178\n",
      "Test loss: 0.04570819113265585\n",
      "Starting epoch 2038\n",
      "Training loss: 0.010168982333824282\n",
      "Test loss: 0.045680028282933764\n",
      "Starting epoch 2039\n",
      "Training loss: 0.010163109787724545\n",
      "Test loss: 0.04620546240497519\n",
      "Starting epoch 2040\n",
      "Training loss: 0.010108563056612601\n",
      "Test loss: 0.04572637489548436\n",
      "Starting epoch 2041\n",
      "Training loss: 0.010312437736352936\n",
      "Test loss: 0.0457746929799517\n",
      "Starting epoch 2042\n",
      "Training loss: 0.010100520857167049\n",
      "Test loss: 0.04619174781772825\n",
      "Starting epoch 2043\n",
      "Training loss: 0.010162546345200694\n",
      "Test loss: 0.04600981664326456\n",
      "Starting epoch 2044\n",
      "Training loss: 0.010329146197706949\n",
      "Test loss: 0.04578216477400727\n",
      "Starting epoch 2045\n",
      "Training loss: 0.010082138312949997\n",
      "Test loss: 0.04623414489820048\n",
      "Starting epoch 2046\n",
      "Training loss: 0.010059574298316339\n",
      "Test loss: 0.04577000908277653\n",
      "Starting epoch 2047\n",
      "Training loss: 0.01016535121397894\n",
      "Test loss: 0.04569776031982015\n",
      "Starting epoch 2048\n",
      "Training loss: 0.010097963048419992\n",
      "Test loss: 0.04611202391485373\n",
      "Starting epoch 2049\n",
      "Training loss: 0.010174569322681818\n",
      "Test loss: 0.04590843797281936\n",
      "Starting epoch 2050\n",
      "Training loss: 0.010295390899552673\n",
      "Test loss: 0.04612695536127797\n",
      "Starting epoch 2051\n",
      "Training loss: 0.010060840408455153\n",
      "Test loss: 0.04533435845816577\n",
      "Starting epoch 2052\n",
      "Training loss: 0.010310503638914375\n",
      "Test loss: 0.04546886648016947\n",
      "Starting epoch 2053\n",
      "Training loss: 0.01005410592331261\n",
      "Test loss: 0.04617924698525005\n",
      "Starting epoch 2054\n",
      "Training loss: 0.010069236296732894\n",
      "Test loss: 0.045798648286748816\n",
      "Starting epoch 2055\n",
      "Training loss: 0.010073956285343796\n",
      "Test loss: 0.04577745642099115\n",
      "Starting epoch 2056\n",
      "Training loss: 0.01010660857695048\n",
      "Test loss: 0.045632516896283185\n",
      "Starting epoch 2057\n",
      "Training loss: 0.010188113943841613\n",
      "Test loss: 0.04585960879921913\n",
      "Starting epoch 2058\n",
      "Training loss: 0.010082747283406922\n",
      "Test loss: 0.04596616051815174\n",
      "Starting epoch 2059\n",
      "Training loss: 0.01014735897789236\n",
      "Test loss: 0.04597237347452729\n",
      "Starting epoch 2060\n",
      "Training loss: 0.01011696309767297\n",
      "Test loss: 0.04615640971395704\n",
      "Starting epoch 2061\n",
      "Training loss: 0.010137867426774541\n",
      "Test loss: 0.04575708218746715\n",
      "Starting epoch 2062\n",
      "Training loss: 0.010050561905029367\n",
      "Test loss: 0.045694100084128206\n",
      "Starting epoch 2063\n",
      "Training loss: 0.010096530368948569\n",
      "Test loss: 0.04590930237814232\n",
      "Starting epoch 2064\n",
      "Training loss: 0.010108030782859833\n",
      "Test loss: 0.045897989361374465\n",
      "Starting epoch 2065\n",
      "Training loss: 0.0101308637283376\n",
      "Test loss: 0.04556674213597068\n",
      "Starting epoch 2066\n",
      "Training loss: 0.010068994122328328\n",
      "Test loss: 0.04554959414181886\n",
      "Starting epoch 2067\n",
      "Training loss: 0.010144839093821948\n",
      "Test loss: 0.045600419105203065\n",
      "Starting epoch 2068\n",
      "Training loss: 0.010152093104285295\n",
      "Test loss: 0.04589025828021544\n",
      "Starting epoch 2069\n",
      "Training loss: 0.010149169621653244\n",
      "Test loss: 0.04562777915486583\n",
      "Starting epoch 2070\n",
      "Training loss: 0.01009109385739096\n",
      "Test loss: 0.04584408841199345\n",
      "Starting epoch 2071\n",
      "Training loss: 0.010061227395886281\n",
      "Test loss: 0.045597952135183195\n",
      "Starting epoch 2072\n",
      "Training loss: 0.010139826112655832\n",
      "Test loss: 0.04571823913741995\n",
      "Starting epoch 2073\n",
      "Training loss: 0.010139453408048778\n",
      "Test loss: 0.04532076259730039\n",
      "Starting epoch 2074\n",
      "Training loss: 0.010163712483204778\n",
      "Test loss: 0.04588176254872923\n",
      "Starting epoch 2075\n",
      "Training loss: 0.010107148545565174\n",
      "Test loss: 0.04614053280265243\n",
      "Starting epoch 2076\n",
      "Training loss: 0.01014903416765518\n",
      "Test loss: 0.04583679905368222\n",
      "Starting epoch 2077\n",
      "Training loss: 0.010108794421568269\n",
      "Test loss: 0.045673457284768425\n",
      "Starting epoch 2078\n",
      "Training loss: 0.010045379675069794\n",
      "Test loss: 0.045456800195905894\n",
      "Starting epoch 2079\n",
      "Training loss: 0.010099852121755725\n",
      "Test loss: 0.04556086231712942\n",
      "Starting epoch 2080\n",
      "Training loss: 0.010267114144612531\n",
      "Test loss: 0.04612829301644255\n",
      "Starting epoch 2081\n",
      "Training loss: 0.0100887106181901\n",
      "Test loss: 0.04645474006732305\n",
      "Starting epoch 2082\n",
      "Training loss: 0.010127739599127262\n",
      "Test loss: 0.046116188996367984\n",
      "Starting epoch 2083\n",
      "Training loss: 0.010060273920048456\n",
      "Test loss: 0.04575641894782031\n",
      "Starting epoch 2084\n",
      "Training loss: 0.010180111241633774\n",
      "Test loss: 0.04568819611988686\n",
      "Starting epoch 2085\n",
      "Training loss: 0.010066045013057892\n",
      "Test loss: 0.045918209961167085\n",
      "Starting epoch 2086\n",
      "Training loss: 0.010091503776732038\n",
      "Test loss: 0.04595379572775629\n",
      "Starting epoch 2087\n",
      "Training loss: 0.010447694767327582\n",
      "Test loss: 0.04574008837894157\n",
      "Starting epoch 2088\n",
      "Training loss: 0.01010060560752134\n",
      "Test loss: 0.04643233230820409\n",
      "Starting epoch 2089\n",
      "Training loss: 0.010100153946607817\n",
      "Test loss: 0.046002591097796405\n",
      "Starting epoch 2090\n",
      "Training loss: 0.010058469474926347\n",
      "Test loss: 0.04570944352006471\n",
      "Starting epoch 2091\n",
      "Training loss: 0.010062602791385572\n",
      "Test loss: 0.04555334233575397\n",
      "Starting epoch 2092\n",
      "Training loss: 0.010154077180157432\n",
      "Test loss: 0.045749260733524956\n",
      "Starting epoch 2093\n",
      "Training loss: 0.010094159099532932\n",
      "Test loss: 0.045388728104255815\n",
      "Starting epoch 2094\n",
      "Training loss: 0.01011131000018022\n",
      "Test loss: 0.04535591215999038\n",
      "Starting epoch 2095\n",
      "Training loss: 0.010092836026041234\n",
      "Test loss: 0.04520530874530474\n",
      "Starting epoch 2096\n",
      "Training loss: 0.010148422166582991\n",
      "Test loss: 0.045626875702981594\n",
      "Starting epoch 2097\n",
      "Training loss: 0.010067870046515933\n",
      "Test loss: 0.04589430118600527\n",
      "Starting epoch 2098\n",
      "Training loss: 0.010222986523733765\n",
      "Test loss: 0.045569806877109736\n",
      "Starting epoch 2099\n",
      "Training loss: 0.010114808742445512\n",
      "Test loss: 0.04542952279249827\n",
      "Starting epoch 2100\n",
      "Training loss: 0.010073123133329094\n",
      "Test loss: 0.04549004717005624\n",
      "Starting epoch 2101\n",
      "Training loss: 0.01013177822603554\n",
      "Test loss: 0.04573991411813983\n",
      "Starting epoch 2102\n",
      "Training loss: 0.010084665639967214\n",
      "Test loss: 0.04606850866090368\n",
      "Starting epoch 2103\n",
      "Training loss: 0.010088290744384781\n",
      "Test loss: 0.04582015176614126\n",
      "Starting epoch 2104\n",
      "Training loss: 0.0100844624742377\n",
      "Test loss: 0.04587716167723691\n",
      "Starting epoch 2105\n",
      "Training loss: 0.01003946838747771\n",
      "Test loss: 0.04581458673433021\n",
      "Starting epoch 2106\n",
      "Training loss: 0.010043631689470322\n",
      "Test loss: 0.04592153692135104\n",
      "Starting epoch 2107\n",
      "Training loss: 0.010288353184940384\n",
      "Test loss: 0.04614920072533466\n",
      "Starting epoch 2108\n",
      "Training loss: 0.010551876739644613\n",
      "Test loss: 0.04643351474293956\n",
      "Starting epoch 2109\n",
      "Training loss: 0.010125200172550365\n",
      "Test loss: 0.04538849451475673\n",
      "Starting epoch 2110\n",
      "Training loss: 0.010101590351369536\n",
      "Test loss: 0.04586818400356504\n",
      "Starting epoch 2111\n",
      "Training loss: 0.010092106693592227\n",
      "Test loss: 0.045970386653034774\n",
      "Starting epoch 2112\n",
      "Training loss: 0.010109045893930997\n",
      "Test loss: 0.04613847589051282\n",
      "Starting epoch 2113\n",
      "Training loss: 0.010137734644603534\n",
      "Test loss: 0.045702171270494106\n",
      "Starting epoch 2114\n",
      "Training loss: 0.010116944074264316\n",
      "Test loss: 0.04586836750860567\n",
      "Starting epoch 2115\n",
      "Training loss: 0.010209089587824266\n",
      "Test loss: 0.04581891897099989\n",
      "Starting epoch 2116\n",
      "Training loss: 0.010053853046332226\n",
      "Test loss: 0.045468714364148954\n",
      "Starting epoch 2117\n",
      "Training loss: 0.01026735815112708\n",
      "Test loss: 0.04572525261728852\n",
      "Starting epoch 2118\n",
      "Training loss: 0.010052650311931234\n",
      "Test loss: 0.04549394327181357\n",
      "Starting epoch 2119\n",
      "Training loss: 0.010123419361647035\n",
      "Test loss: 0.045949813392427236\n",
      "Starting epoch 2120\n",
      "Training loss: 0.010247513209087927\n",
      "Test loss: 0.04580449161154253\n",
      "Starting epoch 2121\n",
      "Training loss: 0.010019247862892072\n",
      "Test loss: 0.04623683155686767\n",
      "Starting epoch 2122\n",
      "Training loss: 0.010043916445164407\n",
      "Test loss: 0.045999616108558794\n",
      "Starting epoch 2123\n",
      "Training loss: 0.010075273389210467\n",
      "Test loss: 0.045849987754115355\n",
      "Starting epoch 2124\n",
      "Training loss: 0.010331765443208765\n",
      "Test loss: 0.04583468743496471\n",
      "Starting epoch 2125\n",
      "Training loss: 0.01026232175712214\n",
      "Test loss: 0.04527784069931066\n",
      "Starting epoch 2126\n",
      "Training loss: 0.010201089007810492\n",
      "Test loss: 0.04598129940805612\n",
      "Starting epoch 2127\n",
      "Training loss: 0.01007349523486661\n",
      "Test loss: 0.046319500301723125\n",
      "Starting epoch 2128\n",
      "Training loss: 0.010146076928396693\n",
      "Test loss: 0.045994455063784564\n",
      "Starting epoch 2129\n",
      "Training loss: 0.010091796242555634\n",
      "Test loss: 0.04610865149233076\n",
      "Starting epoch 2130\n",
      "Training loss: 0.010116990579322714\n",
      "Test loss: 0.04619390545067964\n",
      "Starting epoch 2131\n",
      "Training loss: 0.010017478517943719\n",
      "Test loss: 0.04586716341199698\n",
      "Starting epoch 2132\n",
      "Training loss: 0.010290769753275348\n",
      "Test loss: 0.04601169377565384\n",
      "Starting epoch 2133\n",
      "Training loss: 0.01005111717176242\n",
      "Test loss: 0.046392284609653334\n",
      "Starting epoch 2134\n",
      "Training loss: 0.010124603530666867\n",
      "Test loss: 0.04615506791958102\n",
      "Starting epoch 2135\n",
      "Training loss: 0.010144065576987188\n",
      "Test loss: 0.04623088958086791\n",
      "Starting epoch 2136\n",
      "Training loss: 0.010143502890208706\n",
      "Test loss: 0.046315476298332214\n",
      "Starting epoch 2137\n",
      "Training loss: 0.010065480226986721\n",
      "Test loss: 0.04556771436775172\n",
      "Starting epoch 2138\n",
      "Training loss: 0.010066653220135658\n",
      "Test loss: 0.04535674138201608\n",
      "Starting epoch 2139\n",
      "Training loss: 0.010113892679820295\n",
      "Test loss: 0.04540794770474787\n",
      "Starting epoch 2140\n",
      "Training loss: 0.010137005197647654\n",
      "Test loss: 0.04565929759431769\n",
      "Starting epoch 2141\n",
      "Training loss: 0.010085346543642341\n",
      "Test loss: 0.04598385026609456\n",
      "Starting epoch 2142\n",
      "Training loss: 0.010134174595357941\n",
      "Test loss: 0.04581740236392728\n",
      "Starting epoch 2143\n",
      "Training loss: 0.0100326733816354\n",
      "Test loss: 0.04555021474758784\n",
      "Starting epoch 2144\n",
      "Training loss: 0.010157405078166822\n",
      "Test loss: 0.04538726696261653\n",
      "Starting epoch 2145\n",
      "Training loss: 0.010063367895781994\n",
      "Test loss: 0.04537520461060383\n",
      "Starting epoch 2146\n",
      "Training loss: 0.01007235644110402\n",
      "Test loss: 0.04559092292631114\n",
      "Starting epoch 2147\n",
      "Training loss: 0.010055080025655324\n",
      "Test loss: 0.04602798391823416\n",
      "Starting epoch 2148\n",
      "Training loss: 0.010070471077791004\n",
      "Test loss: 0.04561899726589521\n",
      "Starting epoch 2149\n",
      "Training loss: 0.010299059058554837\n",
      "Test loss: 0.0456591154690142\n",
      "Starting epoch 2150\n",
      "Training loss: 0.01004474611617014\n",
      "Test loss: 0.04622795777740302\n",
      "Starting epoch 2151\n",
      "Training loss: 0.010003796534337958\n",
      "Test loss: 0.04631735191301063\n",
      "Starting epoch 2152\n",
      "Training loss: 0.010041002489504267\n",
      "Test loss: 0.04580675645007028\n",
      "Starting epoch 2153\n",
      "Training loss: 0.010140111730968366\n",
      "Test loss: 0.045696623899318556\n",
      "Starting epoch 2154\n",
      "Training loss: 0.010068562507751535\n",
      "Test loss: 0.04608959621853299\n",
      "Starting epoch 2155\n",
      "Training loss: 0.010245296814036174\n",
      "Test loss: 0.046005149406415445\n",
      "Starting epoch 2156\n",
      "Training loss: 0.010107011137316461\n",
      "Test loss: 0.045648543647042024\n",
      "Starting epoch 2157\n",
      "Training loss: 0.010268370239216774\n",
      "Test loss: 0.04630512116407907\n",
      "Starting epoch 2158\n",
      "Training loss: 0.01017189048780281\n",
      "Test loss: 0.045772558768038395\n",
      "Starting epoch 2159\n",
      "Training loss: 0.010042513384804374\n",
      "Test loss: 0.045291810813877315\n",
      "Starting epoch 2160\n",
      "Training loss: 0.010178132226965467\n",
      "Test loss: 0.045404121486677065\n",
      "Starting epoch 2161\n",
      "Training loss: 0.0101172411814332\n",
      "Test loss: 0.04602030388734959\n",
      "Starting epoch 2162\n",
      "Training loss: 0.010165245287486763\n",
      "Test loss: 0.04575860776283123\n",
      "Starting epoch 2163\n",
      "Training loss: 0.01006856411084777\n",
      "Test loss: 0.04611482029711759\n",
      "Starting epoch 2164\n",
      "Training loss: 0.010046210888101429\n",
      "Test loss: 0.04587444870008363\n",
      "Starting epoch 2165\n",
      "Training loss: 0.01005462538756308\n",
      "Test loss: 0.04594041517487279\n",
      "Starting epoch 2166\n",
      "Training loss: 0.010012488109899348\n",
      "Test loss: 0.046134867532937614\n",
      "Starting epoch 2167\n",
      "Training loss: 0.010205382649160799\n",
      "Test loss: 0.04606723288695017\n",
      "Starting epoch 2168\n",
      "Training loss: 0.010009642385068487\n",
      "Test loss: 0.0461905726266128\n",
      "Starting epoch 2169\n",
      "Training loss: 0.010006909625085651\n",
      "Test loss: 0.045927219161832775\n",
      "Starting epoch 2170\n",
      "Training loss: 0.010104635226555535\n",
      "Test loss: 0.045726149860355586\n",
      "Starting epoch 2171\n",
      "Training loss: 0.010020815935291227\n",
      "Test loss: 0.04601233093826859\n",
      "Starting epoch 2172\n",
      "Training loss: 0.010021473494831656\n",
      "Test loss: 0.04605107778614318\n",
      "Starting epoch 2173\n",
      "Training loss: 0.009992815691550246\n",
      "Test loss: 0.0459221881572847\n",
      "Starting epoch 2174\n",
      "Training loss: 0.010256600810489694\n",
      "Test loss: 0.045785183707873024\n",
      "Starting epoch 2175\n",
      "Training loss: 0.010067646071070531\n",
      "Test loss: 0.04630349438499521\n",
      "Starting epoch 2176\n",
      "Training loss: 0.009998421993900518\n",
      "Test loss: 0.04596818348875752\n",
      "Starting epoch 2177\n",
      "Training loss: 0.010064115076035749\n",
      "Test loss: 0.04581991362350958\n",
      "Starting epoch 2178\n",
      "Training loss: 0.010172515832742706\n",
      "Test loss: 0.04616393335163593\n",
      "Starting epoch 2179\n",
      "Training loss: 0.010049149821527669\n",
      "Test loss: 0.04571293873919381\n",
      "Starting epoch 2180\n",
      "Training loss: 0.010090401075536111\n",
      "Test loss: 0.04549994092020723\n",
      "Starting epoch 2181\n",
      "Training loss: 0.010086982358308112\n",
      "Test loss: 0.04601980014531701\n",
      "Starting epoch 2182\n",
      "Training loss: 0.010118806185048134\n",
      "Test loss: 0.04571824948544855\n",
      "Starting epoch 2183\n",
      "Training loss: 0.010159847540322875\n",
      "Test loss: 0.045477139108158926\n",
      "Starting epoch 2184\n",
      "Training loss: 0.010138717999101663\n",
      "Test loss: 0.04609246814140567\n",
      "Starting epoch 2185\n",
      "Training loss: 0.010113004984364647\n",
      "Test loss: 0.04578425721437843\n",
      "Starting epoch 2186\n",
      "Training loss: 0.010018595601203011\n",
      "Test loss: 0.045314658847120076\n",
      "Starting epoch 2187\n",
      "Training loss: 0.01004746252457138\n",
      "Test loss: 0.04538214420554815\n",
      "Starting epoch 2188\n",
      "Training loss: 0.010091349314592902\n",
      "Test loss: 0.04561915641857518\n",
      "Starting epoch 2189\n",
      "Training loss: 0.010000945007825484\n",
      "Test loss: 0.04604998824221117\n",
      "Starting epoch 2190\n",
      "Training loss: 0.010091772516731356\n",
      "Test loss: 0.045832418733172946\n",
      "Starting epoch 2191\n",
      "Training loss: 0.010201083175593713\n",
      "Test loss: 0.04543953209563538\n",
      "Starting epoch 2192\n",
      "Training loss: 0.009989616980196023\n",
      "Test loss: 0.04599824706437411\n",
      "Starting epoch 2193\n",
      "Training loss: 0.01032805670297048\n",
      "Test loss: 0.046063756225285704\n",
      "Starting epoch 2194\n",
      "Training loss: 0.010210757647625735\n",
      "Test loss: 0.045519708897228596\n",
      "Starting epoch 2195\n",
      "Training loss: 0.010010418054632476\n",
      "Test loss: 0.045662779085062166\n",
      "Starting epoch 2196\n",
      "Training loss: 0.010003763510555517\n",
      "Test loss: 0.04597811014563949\n",
      "Starting epoch 2197\n",
      "Training loss: 0.01002262279269148\n",
      "Test loss: 0.045927855703565806\n",
      "Starting epoch 2198\n",
      "Training loss: 0.010289264018418358\n",
      "Test loss: 0.04595349052989924\n",
      "Starting epoch 2199\n",
      "Training loss: 0.010074620043522999\n",
      "Test loss: 0.046295525437151944\n",
      "Starting epoch 2200\n",
      "Training loss: 0.010149748476802325\n",
      "Test loss: 0.045695839932671296\n",
      "Starting epoch 2201\n",
      "Training loss: 0.010014966939438562\n",
      "Test loss: 0.046050342075802664\n",
      "Starting epoch 2202\n",
      "Training loss: 0.010179884747037144\n",
      "Test loss: 0.04569108342682874\n",
      "Starting epoch 2203\n",
      "Training loss: 0.010070728794595257\n",
      "Test loss: 0.04541061446070671\n",
      "Starting epoch 2204\n",
      "Training loss: 0.010019561230037057\n",
      "Test loss: 0.04527534185736268\n",
      "Starting epoch 2205\n",
      "Training loss: 0.010074008966139594\n",
      "Test loss: 0.045620969186226525\n",
      "Starting epoch 2206\n",
      "Training loss: 0.010090418129426534\n",
      "Test loss: 0.0454962584707472\n",
      "Starting epoch 2207\n",
      "Training loss: 0.010073586886168504\n",
      "Test loss: 0.045474401502697555\n",
      "Starting epoch 2208\n",
      "Training loss: 0.010005948836075479\n",
      "Test loss: 0.04523629115687476\n",
      "Starting epoch 2209\n",
      "Training loss: 0.010096926135240032\n",
      "Test loss: 0.045461548078391284\n",
      "Starting epoch 2210\n",
      "Training loss: 0.010029578276100706\n",
      "Test loss: 0.04596222205846398\n",
      "Starting epoch 2211\n",
      "Training loss: 0.009999407913352623\n",
      "Test loss: 0.045900878255014065\n",
      "Starting epoch 2212\n",
      "Training loss: 0.010140048194920918\n",
      "Test loss: 0.04592073384534429\n",
      "Starting epoch 2213\n",
      "Training loss: 0.010033534335919091\n",
      "Test loss: 0.045470071887528454\n",
      "Starting epoch 2214\n",
      "Training loss: 0.01008620194052575\n",
      "Test loss: 0.04582695593988454\n",
      "Starting epoch 2215\n",
      "Training loss: 0.010014064258850012\n",
      "Test loss: 0.04600467864010069\n",
      "Starting epoch 2216\n",
      "Training loss: 0.010318181859176666\n",
      "Test loss: 0.045578484457952005\n",
      "Starting epoch 2217\n",
      "Training loss: 0.010018373518937924\n",
      "Test loss: 0.04498483185414915\n",
      "Starting epoch 2218\n",
      "Training loss: 0.009988961092455954\n",
      "Test loss: 0.04547333965698878\n",
      "Starting epoch 2219\n",
      "Training loss: 0.010039556557770635\n",
      "Test loss: 0.04572614710088129\n",
      "Starting epoch 2220\n",
      "Training loss: 0.01015732099958619\n",
      "Test loss: 0.04574713593831769\n",
      "Starting epoch 2221\n",
      "Training loss: 0.010218709233965053\n",
      "Test loss: 0.046116356565444556\n",
      "Starting epoch 2222\n",
      "Training loss: 0.009979380111469597\n",
      "Test loss: 0.045612549478257144\n",
      "Starting epoch 2223\n",
      "Training loss: 0.010080362151025748\n",
      "Test loss: 0.045677843745108006\n",
      "Starting epoch 2224\n",
      "Training loss: 0.00998176457207711\n",
      "Test loss: 0.04616456340860437\n",
      "Starting epoch 2225\n",
      "Training loss: 0.009992102038909177\n",
      "Test loss: 0.04619988764601725\n",
      "Starting epoch 2226\n",
      "Training loss: 0.010031348445498552\n",
      "Test loss: 0.04596406690500401\n",
      "Starting epoch 2227\n",
      "Training loss: 0.010013908590571802\n",
      "Test loss: 0.04569070565479773\n",
      "Starting epoch 2228\n",
      "Training loss: 0.0099992269161539\n",
      "Test loss: 0.04585991896412991\n",
      "Starting epoch 2229\n",
      "Training loss: 0.009985135746051053\n",
      "Test loss: 0.04586049776386331\n",
      "Starting epoch 2230\n",
      "Training loss: 0.00999500657447049\n",
      "Test loss: 0.0456701390169285\n",
      "Starting epoch 2231\n",
      "Training loss: 0.010063929941321983\n",
      "Test loss: 0.045468792457271506\n",
      "Starting epoch 2232\n",
      "Training loss: 0.010043366125128309\n",
      "Test loss: 0.045396895320327195\n",
      "Starting epoch 2233\n",
      "Training loss: 0.010103863511295592\n",
      "Test loss: 0.04548453443028309\n",
      "Starting epoch 2234\n",
      "Training loss: 0.010066025875142364\n",
      "Test loss: 0.045954755058994996\n",
      "Starting epoch 2235\n",
      "Training loss: 0.010004199980223765\n",
      "Test loss: 0.04618909789456262\n",
      "Starting epoch 2236\n",
      "Training loss: 0.0099864234445525\n",
      "Test loss: 0.04590454476851004\n",
      "Starting epoch 2237\n",
      "Training loss: 0.010082346646756422\n",
      "Test loss: 0.045579604942489554\n",
      "Starting epoch 2238\n",
      "Training loss: 0.010168104539396332\n",
      "Test loss: 0.04572128359642294\n",
      "Starting epoch 2239\n",
      "Training loss: 0.010224495861740386\n",
      "Test loss: 0.04583404364961165\n",
      "Starting epoch 2240\n",
      "Training loss: 0.010048082892279157\n",
      "Test loss: 0.045449266417158976\n",
      "Starting epoch 2241\n",
      "Training loss: 0.010079680926731376\n",
      "Test loss: 0.04592817759624234\n",
      "Starting epoch 2242\n",
      "Training loss: 0.010107440553361276\n",
      "Test loss: 0.045708665831221476\n",
      "Starting epoch 2243\n",
      "Training loss: 0.009966653420544062\n",
      "Test loss: 0.04612534148273645\n",
      "Starting epoch 2244\n",
      "Training loss: 0.010055364292786747\n",
      "Test loss: 0.0460313706210366\n",
      "Starting epoch 2245\n",
      "Training loss: 0.010050451321924319\n",
      "Test loss: 0.04611746911649351\n",
      "Starting epoch 2246\n",
      "Training loss: 0.01016169432245317\n",
      "Test loss: 0.0456412173807621\n",
      "Starting epoch 2247\n",
      "Training loss: 0.010003198854258804\n",
      "Test loss: 0.045911428415113024\n",
      "Starting epoch 2248\n",
      "Training loss: 0.009983483701944351\n",
      "Test loss: 0.0457376083014188\n",
      "Starting epoch 2249\n",
      "Training loss: 0.010063940537024717\n",
      "Test loss: 0.045522991153928966\n",
      "Starting epoch 2250\n",
      "Training loss: 0.009990264875356291\n",
      "Test loss: 0.04558237145344416\n",
      "Starting epoch 2251\n",
      "Training loss: 0.010098316248689518\n",
      "Test loss: 0.04562286715264673\n",
      "Starting epoch 2252\n",
      "Training loss: 0.010017971920429683\n",
      "Test loss: 0.046035314047778095\n",
      "Starting epoch 2253\n",
      "Training loss: 0.010089499860635547\n",
      "Test loss: 0.045826037448865396\n",
      "Starting epoch 2254\n",
      "Training loss: 0.009991702119835088\n",
      "Test loss: 0.046104447847163235\n",
      "Starting epoch 2255\n",
      "Training loss: 0.010082637097259036\n",
      "Test loss: 0.04579181851888144\n",
      "Starting epoch 2256\n",
      "Training loss: 0.010034530683130514\n",
      "Test loss: 0.04544984921813011\n",
      "Starting epoch 2257\n",
      "Training loss: 0.009988661641713048\n",
      "Test loss: 0.04561579393015967\n",
      "Starting epoch 2258\n",
      "Training loss: 0.010212351903930062\n",
      "Test loss: 0.04568968133793937\n",
      "Starting epoch 2259\n",
      "Training loss: 0.010074381548605982\n",
      "Test loss: 0.04614024429961487\n",
      "Starting epoch 2260\n",
      "Training loss: 0.010064184329793101\n",
      "Test loss: 0.04605334707432323\n",
      "Starting epoch 2261\n",
      "Training loss: 0.010090521689443315\n",
      "Test loss: 0.04574783781060466\n",
      "Starting epoch 2262\n",
      "Training loss: 0.010178445471969784\n",
      "Test loss: 0.04592177658169358\n",
      "Starting epoch 2263\n",
      "Training loss: 0.010031199983519609\n",
      "Test loss: 0.045328935125359786\n",
      "Starting epoch 2264\n",
      "Training loss: 0.010107047703178202\n",
      "Test loss: 0.04572209922803773\n",
      "Starting epoch 2265\n",
      "Training loss: 0.010007803618419365\n",
      "Test loss: 0.04584258946555632\n",
      "Starting epoch 2266\n",
      "Training loss: 0.009983673447468241\n",
      "Test loss: 0.045941377265585795\n",
      "Starting epoch 2267\n",
      "Training loss: 0.01006581577793008\n",
      "Test loss: 0.045920916039634635\n",
      "Starting epoch 2268\n",
      "Training loss: 0.010080574049812848\n",
      "Test loss: 0.046147049853095305\n",
      "Starting epoch 2269\n",
      "Training loss: 0.010041515724580796\n",
      "Test loss: 0.04621209162804815\n",
      "Starting epoch 2270\n",
      "Training loss: 0.010074303195369049\n",
      "Test loss: 0.04619579913991469\n",
      "Starting epoch 2271\n",
      "Training loss: 0.010082495872114525\n",
      "Test loss: 0.04621433956479585\n",
      "Starting epoch 2272\n",
      "Training loss: 0.009966479263222609\n",
      "Test loss: 0.04553701483679039\n",
      "Starting epoch 2273\n",
      "Training loss: 0.010049781319303591\n",
      "Test loss: 0.04557165041289948\n",
      "Starting epoch 2274\n",
      "Training loss: 0.009991714822464302\n",
      "Test loss: 0.045481639396813184\n",
      "Starting epoch 2275\n",
      "Training loss: 0.010014462987052613\n",
      "Test loss: 0.04555642390968623\n",
      "Starting epoch 2276\n",
      "Training loss: 0.01010592229908607\n",
      "Test loss: 0.04586091251284988\n",
      "Starting epoch 2277\n",
      "Training loss: 0.010599670534739728\n",
      "Test loss: 0.0457525155334561\n",
      "Starting epoch 2278\n",
      "Training loss: 0.009963189625776693\n",
      "Test loss: 0.046575583793498854\n",
      "Starting epoch 2279\n",
      "Training loss: 0.010170193831818025\n",
      "Test loss: 0.046178748486218626\n",
      "Starting epoch 2280\n",
      "Training loss: 0.009949071180136477\n",
      "Test loss: 0.045484116231953656\n",
      "Starting epoch 2281\n",
      "Training loss: 0.010205883105270198\n",
      "Test loss: 0.04566555849655911\n",
      "Starting epoch 2282\n",
      "Training loss: 0.01002315741765206\n",
      "Test loss: 0.046200896992727565\n",
      "Starting epoch 2283\n",
      "Training loss: 0.009980394611837433\n",
      "Test loss: 0.04604189649776176\n",
      "Starting epoch 2284\n",
      "Training loss: 0.010008461315368042\n",
      "Test loss: 0.04580087849387416\n",
      "Starting epoch 2285\n",
      "Training loss: 0.010476102380723249\n",
      "Test loss: 0.045920731844725435\n",
      "Starting epoch 2286\n",
      "Training loss: 0.010142653295006908\n",
      "Test loss: 0.04529152300070833\n",
      "Starting epoch 2287\n",
      "Training loss: 0.009970941321283091\n",
      "Test loss: 0.04590978887346056\n",
      "Starting epoch 2288\n",
      "Training loss: 0.010052613684999162\n",
      "Test loss: 0.04601888717324645\n",
      "Starting epoch 2289\n",
      "Training loss: 0.009950866113554259\n",
      "Test loss: 0.045660226157418\n",
      "Starting epoch 2290\n",
      "Training loss: 0.010025602490564839\n",
      "Test loss: 0.045772080620129905\n",
      "Starting epoch 2291\n",
      "Training loss: 0.010203741109151333\n",
      "Test loss: 0.04552697542089003\n",
      "Starting epoch 2292\n",
      "Training loss: 0.009957464121770664\n",
      "Test loss: 0.04608063192831145\n",
      "Starting epoch 2293\n",
      "Training loss: 0.00998450931711275\n",
      "Test loss: 0.04593948661177247\n",
      "Starting epoch 2294\n",
      "Training loss: 0.01005473823027044\n",
      "Test loss: 0.04581849525372187\n",
      "Starting epoch 2295\n",
      "Training loss: 0.009979738563787742\n",
      "Test loss: 0.045856873194376625\n",
      "Starting epoch 2296\n",
      "Training loss: 0.010242239343094045\n",
      "Test loss: 0.04599128428984572\n",
      "Starting epoch 2297\n",
      "Training loss: 0.00995142596819606\n",
      "Test loss: 0.04532836281039097\n",
      "Starting epoch 2298\n",
      "Training loss: 0.009984761209334017\n",
      "Test loss: 0.045416134789034175\n",
      "Starting epoch 2299\n",
      "Training loss: 0.010019081919530376\n",
      "Test loss: 0.04566595358429132\n",
      "Starting epoch 2300\n",
      "Training loss: 0.01004939077452558\n",
      "Test loss: 0.04589697911783501\n",
      "Starting epoch 2301\n",
      "Training loss: 0.010059769066875099\n",
      "Test loss: 0.045783332652515836\n",
      "Starting epoch 2302\n",
      "Training loss: 0.010210964614983465\n",
      "Test loss: 0.045342709593198915\n",
      "Starting epoch 2303\n",
      "Training loss: 0.009997828069646827\n",
      "Test loss: 0.045964878949302214\n",
      "Starting epoch 2304\n",
      "Training loss: 0.010051278244765078\n",
      "Test loss: 0.045791111058659024\n",
      "Starting epoch 2305\n",
      "Training loss: 0.01032602594646274\n",
      "Test loss: 0.04551896878897592\n",
      "Starting epoch 2306\n",
      "Training loss: 0.010013721868029384\n",
      "Test loss: 0.044981185553802386\n",
      "Starting epoch 2307\n",
      "Training loss: 0.010083372261924822\n",
      "Test loss: 0.04516212145487467\n",
      "Starting epoch 2308\n",
      "Training loss: 0.010110224994113211\n",
      "Test loss: 0.04533270084195667\n",
      "Starting epoch 2309\n",
      "Training loss: 0.01008482248384933\n",
      "Test loss: 0.045510187883068015\n",
      "Starting epoch 2310\n",
      "Training loss: 0.010000537836649379\n",
      "Test loss: 0.045767798053997534\n",
      "Starting epoch 2311\n",
      "Training loss: 0.010171668909367968\n",
      "Test loss: 0.04595447166098489\n",
      "Starting epoch 2312\n",
      "Training loss: 0.01007509036142318\n",
      "Test loss: 0.04586946991858659\n",
      "Starting epoch 2313\n",
      "Training loss: 0.010139898412296029\n",
      "Test loss: 0.045904410658059294\n",
      "Starting epoch 2314\n",
      "Training loss: 0.010106438480806156\n",
      "Test loss: 0.04622945782762987\n",
      "Starting epoch 2315\n",
      "Training loss: 0.010081109789307\n",
      "Test loss: 0.046284339080254235\n",
      "Starting epoch 2316\n",
      "Training loss: 0.010012635961174965\n",
      "Test loss: 0.045567232839487215\n",
      "Starting epoch 2317\n",
      "Training loss: 0.010024026188938344\n",
      "Test loss: 0.04558979623295643\n",
      "Starting epoch 2318\n",
      "Training loss: 0.010007366278498877\n",
      "Test loss: 0.045919185090396136\n",
      "Starting epoch 2319\n",
      "Training loss: 0.010061254297367862\n",
      "Test loss: 0.04621522804653203\n",
      "Starting epoch 2320\n",
      "Training loss: 0.00998660481580701\n",
      "Test loss: 0.04617391554293809\n",
      "Starting epoch 2321\n",
      "Training loss: 0.010002179849953925\n",
      "Test loss: 0.04571457083026568\n",
      "Starting epoch 2322\n",
      "Training loss: 0.010025853214815993\n",
      "Test loss: 0.045736666975749865\n",
      "Starting epoch 2323\n",
      "Training loss: 0.010207607013890977\n",
      "Test loss: 0.046115496230346185\n",
      "Starting epoch 2324\n",
      "Training loss: 0.010092387647658099\n",
      "Test loss: 0.04642772826331633\n",
      "Starting epoch 2325\n",
      "Training loss: 0.010013218831698426\n",
      "Test loss: 0.046399139557723644\n",
      "Starting epoch 2326\n",
      "Training loss: 0.009974165712712242\n",
      "Test loss: 0.046026167908200515\n",
      "Starting epoch 2327\n",
      "Training loss: 0.01004837027399755\n",
      "Test loss: 0.04582550597411615\n",
      "Starting epoch 2328\n",
      "Training loss: 0.010060207139639581\n",
      "Test loss: 0.04608959470082213\n",
      "Starting epoch 2329\n",
      "Training loss: 0.009949326988492832\n",
      "Test loss: 0.0461058789105327\n",
      "Starting epoch 2330\n",
      "Training loss: 0.009980915144818728\n",
      "Test loss: 0.04577303181091944\n",
      "Starting epoch 2331\n",
      "Training loss: 0.010065435370827307\n",
      "Test loss: 0.04545301212756722\n",
      "Starting epoch 2332\n",
      "Training loss: 0.010520962005878081\n",
      "Test loss: 0.045253667428537654\n",
      "Starting epoch 2333\n",
      "Training loss: 0.009974742552540342\n",
      "Test loss: 0.04485000104263977\n",
      "Starting epoch 2334\n",
      "Training loss: 0.010301881851475746\n",
      "Test loss: 0.04554019447554041\n",
      "Starting epoch 2335\n",
      "Training loss: 0.010123054008381288\n",
      "Test loss: 0.04653273026148478\n",
      "Starting epoch 2336\n",
      "Training loss: 0.009996695833311219\n",
      "Test loss: 0.04679818920515202\n",
      "Starting epoch 2337\n",
      "Training loss: 0.01026409423192505\n",
      "Test loss: 0.045952117553463685\n",
      "Starting epoch 2338\n",
      "Training loss: 0.010019412722255363\n",
      "Test loss: 0.04622415591169287\n",
      "Starting epoch 2339\n",
      "Training loss: 0.010012696481874733\n",
      "Test loss: 0.046196708662642375\n",
      "Starting epoch 2340\n",
      "Training loss: 0.010050513384649988\n",
      "Test loss: 0.04577903973835486\n",
      "Starting epoch 2341\n",
      "Training loss: 0.01009142272113288\n",
      "Test loss: 0.04585339391121158\n",
      "Starting epoch 2342\n",
      "Training loss: 0.01001487537974217\n",
      "Test loss: 0.04618350209461318\n",
      "Starting epoch 2343\n",
      "Training loss: 0.010021043513886264\n",
      "Test loss: 0.045756470412015915\n",
      "Starting epoch 2344\n",
      "Training loss: 0.00999399881474063\n",
      "Test loss: 0.04570026074846586\n",
      "Starting epoch 2345\n",
      "Training loss: 0.010214099614713036\n",
      "Test loss: 0.04553271333376566\n",
      "Starting epoch 2346\n",
      "Training loss: 0.009971324369677754\n",
      "Test loss: 0.04508301712296627\n",
      "Starting epoch 2347\n",
      "Training loss: 0.009995460403380825\n",
      "Test loss: 0.04539123122338896\n",
      "Starting epoch 2348\n",
      "Training loss: 0.010246242946166485\n",
      "Test loss: 0.04543063334292836\n",
      "Starting epoch 2349\n",
      "Training loss: 0.010071223754374707\n",
      "Test loss: 0.04530399044354757\n",
      "Starting epoch 2350\n",
      "Training loss: 0.010441813159917221\n",
      "Test loss: 0.04579640828348972\n",
      "Starting epoch 2351\n",
      "Training loss: 0.010031245343509267\n",
      "Test loss: 0.045344208332675474\n",
      "Starting epoch 2352\n",
      "Training loss: 0.01008179597556591\n",
      "Test loss: 0.045366858304650697\n",
      "Starting epoch 2353\n",
      "Training loss: 0.009970770942688476\n",
      "Test loss: 0.04579760617128125\n",
      "Starting epoch 2354\n",
      "Training loss: 0.010064471619905995\n",
      "Test loss: 0.04564631723419384\n",
      "Starting epoch 2355\n",
      "Training loss: 0.010007663248259514\n",
      "Test loss: 0.04593866663398566\n",
      "Starting epoch 2356\n",
      "Training loss: 0.010173903213294803\n",
      "Test loss: 0.046300386940991436\n",
      "Starting epoch 2357\n",
      "Training loss: 0.01002506752971743\n",
      "Test loss: 0.0455732189670757\n",
      "Starting epoch 2358\n",
      "Training loss: 0.010090283286131796\n",
      "Test loss: 0.045889709834699276\n",
      "Starting epoch 2359\n",
      "Training loss: 0.010318615748623356\n",
      "Test loss: 0.04581835686608597\n",
      "Starting epoch 2360\n",
      "Training loss: 0.010049543572498149\n",
      "Test loss: 0.045178978945370075\n",
      "Starting epoch 2361\n",
      "Training loss: 0.00995789933949709\n",
      "Test loss: 0.045501423033851164\n",
      "Starting epoch 2362\n",
      "Training loss: 0.009992229126271655\n",
      "Test loss: 0.045552045520808965\n",
      "Starting epoch 2363\n",
      "Training loss: 0.010016694497011725\n",
      "Test loss: 0.04574110552116677\n",
      "Starting epoch 2364\n",
      "Training loss: 0.010071767524617617\n",
      "Test loss: 0.04598284595542484\n",
      "Starting epoch 2365\n",
      "Training loss: 0.010005798358775552\n",
      "Test loss: 0.04549307762472718\n",
      "Starting epoch 2366\n",
      "Training loss: 0.010006772964948514\n",
      "Test loss: 0.04554402083158493\n",
      "Starting epoch 2367\n",
      "Training loss: 0.009953486763673728\n",
      "Test loss: 0.045815627470060634\n",
      "Starting epoch 2368\n",
      "Training loss: 0.010181195255307878\n",
      "Test loss: 0.04569251835346222\n",
      "Starting epoch 2369\n",
      "Training loss: 0.010531707407265414\n",
      "Test loss: 0.04532547833191024\n",
      "Starting epoch 2370\n",
      "Training loss: 0.010198713738288059\n",
      "Test loss: 0.046492577978858245\n",
      "Starting epoch 2371\n",
      "Training loss: 0.010057044398711353\n",
      "Test loss: 0.04583156274424659\n",
      "Starting epoch 2372\n",
      "Training loss: 0.009974713635737778\n",
      "Test loss: 0.045826039794418544\n",
      "Starting epoch 2373\n",
      "Training loss: 0.009966627083963057\n",
      "Test loss: 0.04599105056237291\n",
      "Starting epoch 2374\n",
      "Training loss: 0.009973354210130504\n",
      "Test loss: 0.045806109767269204\n",
      "Starting epoch 2375\n",
      "Training loss: 0.010046831484822953\n",
      "Test loss: 0.04564190959488904\n",
      "Starting epoch 2376\n",
      "Training loss: 0.010037374728527225\n",
      "Test loss: 0.04551708698272705\n",
      "Starting epoch 2377\n",
      "Training loss: 0.010071938918506513\n",
      "Test loss: 0.04578342716451044\n",
      "Starting epoch 2378\n",
      "Training loss: 0.009961859460492602\n",
      "Test loss: 0.04612762067053053\n",
      "Starting epoch 2379\n",
      "Training loss: 0.010390644282346866\n",
      "Test loss: 0.04559670885403951\n",
      "Starting epoch 2380\n",
      "Training loss: 0.009974090504597445\n",
      "Test loss: 0.04491003257808862\n",
      "Starting epoch 2381\n",
      "Training loss: 0.00998266160365988\n",
      "Test loss: 0.045117146576996205\n",
      "Starting epoch 2382\n",
      "Training loss: 0.009970272875955848\n",
      "Test loss: 0.04558335755158354\n",
      "Starting epoch 2383\n",
      "Training loss: 0.010002266096531367\n",
      "Test loss: 0.04563086824836554\n",
      "Starting epoch 2384\n",
      "Training loss: 0.009966141055719774\n",
      "Test loss: 0.04545675563039603\n",
      "Starting epoch 2385\n",
      "Training loss: 0.010053223220356663\n",
      "Test loss: 0.04573680846779435\n",
      "Starting epoch 2386\n",
      "Training loss: 0.009970391642485485\n",
      "Test loss: 0.0456637437282889\n",
      "Starting epoch 2387\n",
      "Training loss: 0.010056832683135251\n",
      "Test loss: 0.0455981298453278\n",
      "Starting epoch 2388\n",
      "Training loss: 0.010090445901160358\n",
      "Test loss: 0.045802189175177505\n",
      "Starting epoch 2389\n",
      "Training loss: 0.010020279081263503\n",
      "Test loss: 0.04555664708216985\n",
      "Starting epoch 2390\n",
      "Training loss: 0.009927944096996159\n",
      "Test loss: 0.04584876365131802\n",
      "Starting epoch 2391\n",
      "Training loss: 0.010010801362576054\n",
      "Test loss: 0.04592001493330355\n",
      "Starting epoch 2392\n",
      "Training loss: 0.010000245386093367\n",
      "Test loss: 0.04557037174149796\n",
      "Starting epoch 2393\n",
      "Training loss: 0.010029819122225534\n",
      "Test loss: 0.04546690539077476\n",
      "Starting epoch 2394\n",
      "Training loss: 0.00997554683355523\n",
      "Test loss: 0.04587344604509848\n",
      "Starting epoch 2395\n",
      "Training loss: 0.009956810669210113\n",
      "Test loss: 0.04571507139890282\n",
      "Starting epoch 2396\n",
      "Training loss: 0.01019635294243449\n",
      "Test loss: 0.04592372615028311\n",
      "Starting epoch 2397\n",
      "Training loss: 0.010029489754653368\n",
      "Test loss: 0.045202237105479944\n",
      "Starting epoch 2398\n",
      "Training loss: 0.010065134126143377\n",
      "Test loss: 0.04566624567464546\n",
      "Starting epoch 2399\n",
      "Training loss: 0.010208674675983484\n",
      "Test loss: 0.04575170397206589\n",
      "Starting epoch 2400\n",
      "Training loss: 0.010037776067486553\n",
      "Test loss: 0.04633098944193787\n",
      "Starting epoch 2401\n",
      "Training loss: 0.010207557837005521\n",
      "Test loss: 0.046239044103357524\n",
      "Starting epoch 2402\n",
      "Training loss: 0.01004865319758165\n",
      "Test loss: 0.04538821435912892\n",
      "Starting epoch 2403\n",
      "Training loss: 0.01013778142447843\n",
      "Test loss: 0.04580696741188014\n",
      "Starting epoch 2404\n",
      "Training loss: 0.00998018245350142\n",
      "Test loss: 0.04637894337927854\n",
      "Starting epoch 2405\n",
      "Training loss: 0.010007624651809207\n",
      "Test loss: 0.04576945732589121\n",
      "Starting epoch 2406\n",
      "Training loss: 0.010009391294395337\n",
      "Test loss: 0.04580823966750392\n",
      "Starting epoch 2407\n",
      "Training loss: 0.010002040914946893\n",
      "Test loss: 0.045713368941236426\n",
      "Starting epoch 2408\n",
      "Training loss: 0.009966356847740587\n",
      "Test loss: 0.04561818736018958\n",
      "Starting epoch 2409\n",
      "Training loss: 0.010023922674724312\n",
      "Test loss: 0.04567151847812864\n",
      "Starting epoch 2410\n",
      "Training loss: 0.009945695700704074\n",
      "Test loss: 0.04548436637829851\n",
      "Starting epoch 2411\n",
      "Training loss: 0.009975259703751958\n",
      "Test loss: 0.0456165243630056\n",
      "Starting epoch 2412\n",
      "Training loss: 0.009979977531999838\n",
      "Test loss: 0.045565803017881185\n",
      "Starting epoch 2413\n",
      "Training loss: 0.00996763495819979\n",
      "Test loss: 0.045511380941779526\n",
      "Starting epoch 2414\n",
      "Training loss: 0.010133392666084845\n",
      "Test loss: 0.04539591529303127\n",
      "Starting epoch 2415\n",
      "Training loss: 0.010039937270225072\n",
      "Test loss: 0.045038036105257494\n",
      "Starting epoch 2416\n",
      "Training loss: 0.010070575355384194\n",
      "Test loss: 0.04521902347052539\n",
      "Starting epoch 2417\n",
      "Training loss: 0.010082719908630262\n",
      "Test loss: 0.045757213400469884\n",
      "Starting epoch 2418\n",
      "Training loss: 0.010010704871450291\n",
      "Test loss: 0.04623850559194883\n",
      "Starting epoch 2419\n",
      "Training loss: 0.00997367602024899\n",
      "Test loss: 0.045931444951781523\n",
      "Starting epoch 2420\n",
      "Training loss: 0.009975323331404905\n",
      "Test loss: 0.045614442615597335\n",
      "Starting epoch 2421\n",
      "Training loss: 0.009965341263374344\n",
      "Test loss: 0.045430632377112354\n",
      "Starting epoch 2422\n",
      "Training loss: 0.009942827105033593\n",
      "Test loss: 0.045503414477463124\n",
      "Starting epoch 2423\n",
      "Training loss: 0.009982670764209793\n",
      "Test loss: 0.04568165375126733\n",
      "Starting epoch 2424\n",
      "Training loss: 0.010071872275505886\n",
      "Test loss: 0.045416314706758214\n",
      "Starting epoch 2425\n",
      "Training loss: 0.010042289607837552\n",
      "Test loss: 0.04580553759027411\n",
      "Starting epoch 2426\n",
      "Training loss: 0.009986955550361852\n",
      "Test loss: 0.045980371396850656\n",
      "Starting epoch 2427\n",
      "Training loss: 0.010011118790898168\n",
      "Test loss: 0.04575187975057849\n",
      "Starting epoch 2428\n",
      "Training loss: 0.010154500283056596\n",
      "Test loss: 0.0454065315425396\n",
      "Starting epoch 2429\n",
      "Training loss: 0.01004586394754101\n",
      "Test loss: 0.045984416786167354\n",
      "Starting epoch 2430\n",
      "Training loss: 0.01012782773888502\n",
      "Test loss: 0.04558777684966723\n",
      "Starting epoch 2431\n",
      "Training loss: 0.010123595763303217\n",
      "Test loss: 0.04534524568804988\n",
      "Starting epoch 2432\n",
      "Training loss: 0.010000992093052044\n",
      "Test loss: 0.04575686950098585\n",
      "Starting epoch 2433\n",
      "Training loss: 0.010016534202655808\n",
      "Test loss: 0.04570002012230732\n",
      "Starting epoch 2434\n",
      "Training loss: 0.010005260336777715\n",
      "Test loss: 0.04571234517627292\n",
      "Starting epoch 2435\n",
      "Training loss: 0.010094490070201334\n",
      "Test loss: 0.04572916651765505\n",
      "Starting epoch 2436\n",
      "Training loss: 0.009988785141193475\n",
      "Test loss: 0.04610671875653444\n",
      "Starting epoch 2437\n",
      "Training loss: 0.01008522034179969\n",
      "Test loss: 0.04588799364864826\n",
      "Starting epoch 2438\n",
      "Training loss: 0.010169759698090006\n",
      "Test loss: 0.04550558687360198\n",
      "Starting epoch 2439\n",
      "Training loss: 0.01000666058026865\n",
      "Test loss: 0.04606657851211451\n",
      "Starting epoch 2440\n",
      "Training loss: 0.01006030978360137\n",
      "Test loss: 0.04611232725006563\n",
      "Starting epoch 2441\n",
      "Training loss: 0.01020534771693046\n",
      "Test loss: 0.04562169644567701\n",
      "Starting epoch 2442\n",
      "Training loss: 0.009918539410792902\n",
      "Test loss: 0.046122726052999496\n",
      "Starting epoch 2443\n",
      "Training loss: 0.010128181030760046\n",
      "Test loss: 0.0458786649008592\n",
      "Starting epoch 2444\n",
      "Training loss: 0.010089245090474848\n",
      "Test loss: 0.046028919242046495\n",
      "Starting epoch 2445\n",
      "Training loss: 0.009969224618961577\n",
      "Test loss: 0.04565615814041208\n",
      "Starting epoch 2446\n",
      "Training loss: 0.009988950840273842\n",
      "Test loss: 0.045785445926917925\n",
      "Starting epoch 2447\n",
      "Training loss: 0.010116928409723962\n",
      "Test loss: 0.04575475236331975\n",
      "Starting epoch 2448\n",
      "Training loss: 0.010087025565568541\n",
      "Test loss: 0.04538132443472191\n",
      "Starting epoch 2449\n",
      "Training loss: 0.010098709846984168\n",
      "Test loss: 0.04583210373918215\n",
      "Starting epoch 2450\n",
      "Training loss: 0.010011050178379308\n",
      "Test loss: 0.04537803348567751\n",
      "Starting epoch 2451\n",
      "Training loss: 0.009996169552084853\n",
      "Test loss: 0.04521600432969906\n",
      "Starting epoch 2452\n",
      "Training loss: 0.009998270753221433\n",
      "Test loss: 0.0454102674568141\n",
      "Starting epoch 2453\n",
      "Training loss: 0.010041289306322082\n",
      "Test loss: 0.04572774814786734\n",
      "Starting epoch 2454\n",
      "Training loss: 0.010018888754067852\n",
      "Test loss: 0.045552948006877196\n",
      "Starting epoch 2455\n",
      "Training loss: 0.00995552628377422\n",
      "Test loss: 0.04594485999809371\n",
      "Starting epoch 2456\n",
      "Training loss: 0.010003083492400216\n",
      "Test loss: 0.0459215112582401\n",
      "Starting epoch 2457\n",
      "Training loss: 0.009947526940434683\n",
      "Test loss: 0.04575447655386395\n",
      "Starting epoch 2458\n",
      "Training loss: 0.009989431998158087\n",
      "Test loss: 0.04558772786899849\n",
      "Starting epoch 2459\n",
      "Training loss: 0.009962985521090812\n",
      "Test loss: 0.045659831552593795\n",
      "Starting epoch 2460\n",
      "Training loss: 0.010067433744791101\n",
      "Test loss: 0.04541498008701536\n",
      "Starting epoch 2461\n",
      "Training loss: 0.010025453540023233\n",
      "Test loss: 0.04506633451415433\n",
      "Starting epoch 2462\n",
      "Training loss: 0.010193973840748678\n",
      "Test loss: 0.04570389994316631\n",
      "Starting epoch 2463\n",
      "Training loss: 0.010038544927708438\n",
      "Test loss: 0.04540090697507063\n",
      "Starting epoch 2464\n",
      "Training loss: 0.010016165994352004\n",
      "Test loss: 0.04586476680857164\n",
      "Starting epoch 2465\n",
      "Training loss: 0.010123190744856342\n",
      "Test loss: 0.045975527001751795\n",
      "Starting epoch 2466\n",
      "Training loss: 0.009951282155196198\n",
      "Test loss: 0.046197436336014\n",
      "Starting epoch 2467\n",
      "Training loss: 0.010073868481472868\n",
      "Test loss: 0.045810744925229636\n",
      "Starting epoch 2468\n",
      "Training loss: 0.010594084706218516\n",
      "Test loss: 0.04575979502664672\n",
      "Starting epoch 2469\n",
      "Training loss: 0.009954937474160899\n",
      "Test loss: 0.04499226656776888\n",
      "Starting epoch 2470\n",
      "Training loss: 0.010108840300655756\n",
      "Test loss: 0.04506092173633752\n",
      "Starting epoch 2471\n",
      "Training loss: 0.010179033090711618\n",
      "Test loss: 0.04528039638642912\n",
      "Starting epoch 2472\n",
      "Training loss: 0.01005472444364282\n",
      "Test loss: 0.04621648664275805\n",
      "Starting epoch 2473\n",
      "Training loss: 0.010122181023242043\n",
      "Test loss: 0.0457942231937691\n",
      "Starting epoch 2474\n",
      "Training loss: 0.01022361086101317\n",
      "Test loss: 0.04604447039741057\n",
      "Starting epoch 2475\n",
      "Training loss: 0.010154082852064586\n",
      "Test loss: 0.046293416922843014\n",
      "Starting epoch 2476\n",
      "Training loss: 0.010053934529423714\n",
      "Test loss: 0.045518507836041625\n",
      "Starting epoch 2477\n",
      "Training loss: 0.0099662309512496\n",
      "Test loss: 0.04575615403828798\n",
      "Starting epoch 2478\n",
      "Training loss: 0.009935778535169656\n",
      "Test loss: 0.046019503363856566\n",
      "Starting epoch 2479\n",
      "Training loss: 0.009966812508760905\n",
      "Test loss: 0.04581314380522127\n",
      "Starting epoch 2480\n",
      "Training loss: 0.009989101470249598\n",
      "Test loss: 0.04576518924699889\n",
      "Starting epoch 2481\n",
      "Training loss: 0.010000895113363618\n",
      "Test loss: 0.04584990303825449\n",
      "Starting epoch 2482\n",
      "Training loss: 0.009978307716426302\n",
      "Test loss: 0.04550379252544156\n",
      "Starting epoch 2483\n",
      "Training loss: 0.009958338038232482\n",
      "Test loss: 0.04565425175759527\n",
      "Starting epoch 2484\n",
      "Training loss: 0.010142868781676058\n",
      "Test loss: 0.045704932400473845\n",
      "Starting epoch 2485\n",
      "Training loss: 0.009996937007689085\n",
      "Test loss: 0.04548681416997203\n",
      "Starting epoch 2486\n",
      "Training loss: 0.010040559683788995\n",
      "Test loss: 0.04554141968212746\n",
      "Starting epoch 2487\n",
      "Training loss: 0.010024348029591998\n",
      "Test loss: 0.04613944860520186\n",
      "Starting epoch 2488\n",
      "Training loss: 0.010019312902796463\n",
      "Test loss: 0.04626409737048326\n",
      "Starting epoch 2489\n",
      "Training loss: 0.010123776974248105\n",
      "Test loss: 0.04605891403776628\n",
      "Starting epoch 2490\n",
      "Training loss: 0.009932822975345323\n",
      "Test loss: 0.045369941396293814\n",
      "Starting epoch 2491\n",
      "Training loss: 0.010193950924106309\n",
      "Test loss: 0.04550176927888835\n",
      "Starting epoch 2492\n",
      "Training loss: 0.010003980768264318\n",
      "Test loss: 0.04525685103403197\n",
      "Starting epoch 2493\n",
      "Training loss: 0.00997406578638026\n",
      "Test loss: 0.04559379002010381\n",
      "Starting epoch 2494\n",
      "Training loss: 0.010086471963002056\n",
      "Test loss: 0.045650349722968206\n",
      "Starting epoch 2495\n",
      "Training loss: 0.00998970956282049\n",
      "Test loss: 0.045393176238845895\n",
      "Starting epoch 2496\n",
      "Training loss: 0.010420755238928756\n",
      "Test loss: 0.04561420060970165\n",
      "Starting epoch 2497\n",
      "Training loss: 0.009984997314874266\n",
      "Test loss: 0.04521995306842857\n",
      "Starting epoch 2498\n",
      "Training loss: 0.01015514643771238\n",
      "Test loss: 0.04525671830331838\n",
      "Starting epoch 2499\n",
      "Training loss: 0.010001238206493073\n",
      "Test loss: 0.045517750912242465\n",
      "Starting epoch 2500\n",
      "Training loss: 0.010310077154245532\n",
      "Test loss: 0.04600655949778027\n",
      "Starting epoch 2501\n",
      "Training loss: 0.009953539070413738\n",
      "Test loss: 0.04530298144177154\n",
      "Starting epoch 2502\n",
      "Training loss: 0.009956437850096186\n",
      "Test loss: 0.0454308840756615\n",
      "Starting epoch 2503\n",
      "Training loss: 0.00996074419407571\n",
      "Test loss: 0.04558710091643863\n",
      "Starting epoch 2504\n",
      "Training loss: 0.010157061725488453\n",
      "Test loss: 0.04591207716751982\n",
      "Starting epoch 2505\n",
      "Training loss: 0.010250138256271354\n",
      "Test loss: 0.046234617906588095\n",
      "Starting epoch 2506\n",
      "Training loss: 0.010331386394920896\n",
      "Test loss: 0.045401355182683026\n",
      "Starting epoch 2507\n",
      "Training loss: 0.0102082633978275\n",
      "Test loss: 0.04498795820055185\n",
      "Starting epoch 2508\n",
      "Training loss: 0.010028663595191768\n",
      "Test loss: 0.045576491151694896\n",
      "Starting epoch 2509\n",
      "Training loss: 0.010109583175451051\n",
      "Test loss: 0.045643089408123935\n",
      "Starting epoch 2510\n",
      "Training loss: 0.010074486879662413\n",
      "Test loss: 0.04532378028940271\n",
      "Starting epoch 2511\n",
      "Training loss: 0.010189042182364424\n",
      "Test loss: 0.045235996307046326\n",
      "Starting epoch 2512\n",
      "Training loss: 0.010057172325790906\n",
      "Test loss: 0.04518413143577399\n",
      "Starting epoch 2513\n",
      "Training loss: 0.010021381309164351\n",
      "Test loss: 0.04593457060831564\n",
      "Starting epoch 2514\n",
      "Training loss: 0.009912973086609215\n",
      "Test loss: 0.04606884835218942\n",
      "Starting epoch 2515\n",
      "Training loss: 0.00993911917397722\n",
      "Test loss: 0.045768200109402336\n",
      "Starting epoch 2516\n",
      "Training loss: 0.010100496627512525\n",
      "Test loss: 0.04555793755032398\n",
      "Starting epoch 2517\n",
      "Training loss: 0.009964067946936264\n",
      "Test loss: 0.04501888707832054\n",
      "Starting epoch 2518\n",
      "Training loss: 0.010012915907580345\n",
      "Test loss: 0.0451827403847818\n",
      "Starting epoch 2519\n",
      "Training loss: 0.009964296441586291\n",
      "Test loss: 0.04521894606727141\n",
      "Starting epoch 2520\n",
      "Training loss: 0.009986839959489518\n",
      "Test loss: 0.04531128704547882\n",
      "Starting epoch 2521\n",
      "Training loss: 0.010036917464410672\n",
      "Test loss: 0.045450661055467745\n",
      "Starting epoch 2522\n",
      "Training loss: 0.009990139360554883\n",
      "Test loss: 0.04545455025853934\n",
      "Starting epoch 2523\n",
      "Training loss: 0.009968144620661853\n",
      "Test loss: 0.04575593121073864\n",
      "Starting epoch 2524\n",
      "Training loss: 0.009980289845681582\n",
      "Test loss: 0.04597304347488615\n",
      "Starting epoch 2525\n",
      "Training loss: 0.01001505972054161\n",
      "Test loss: 0.04564186640911632\n",
      "Starting epoch 2526\n",
      "Training loss: 0.009953200710235073\n",
      "Test loss: 0.04586686683749711\n",
      "Starting epoch 2527\n",
      "Training loss: 0.010151186790011946\n",
      "Test loss: 0.04577935473234565\n",
      "Starting epoch 2528\n",
      "Training loss: 0.009913272407577663\n",
      "Test loss: 0.04614038558469878\n",
      "Starting epoch 2529\n",
      "Training loss: 0.010008985947695424\n",
      "Test loss: 0.045884113275894416\n",
      "Starting epoch 2530\n",
      "Training loss: 0.009917674998401618\n",
      "Test loss: 0.045355732242266335\n",
      "Starting epoch 2531\n",
      "Training loss: 0.010081278862523252\n",
      "Test loss: 0.0455913872078613\n",
      "Starting epoch 2532\n",
      "Training loss: 0.010036236331721798\n",
      "Test loss: 0.04604379956920942\n",
      "Starting epoch 2533\n",
      "Training loss: 0.009963210611069789\n",
      "Test loss: 0.04575483970068119\n",
      "Starting epoch 2534\n",
      "Training loss: 0.010197853822200025\n",
      "Test loss: 0.04604986303106502\n",
      "Starting epoch 2535\n",
      "Training loss: 0.009921410991275897\n",
      "Test loss: 0.046375086462056195\n",
      "Starting epoch 2536\n",
      "Training loss: 0.009969991112708068\n",
      "Test loss: 0.04588794866921725\n",
      "Starting epoch 2537\n",
      "Training loss: 0.00993286885443281\n",
      "Test loss: 0.045418415770486546\n",
      "Starting epoch 2538\n",
      "Training loss: 0.00992640361311983\n",
      "Test loss: 0.04522800017838125\n",
      "Starting epoch 2539\n",
      "Training loss: 0.009984432399028638\n",
      "Test loss: 0.045520800131338614\n",
      "Starting epoch 2540\n",
      "Training loss: 0.01000960282676044\n",
      "Test loss: 0.04591360964156963\n",
      "Starting epoch 2541\n",
      "Training loss: 0.009940403330399365\n",
      "Test loss: 0.0462134186592367\n",
      "Starting epoch 2542\n",
      "Training loss: 0.009941518444140426\n",
      "Test loss: 0.045833002775907516\n",
      "Starting epoch 2543\n",
      "Training loss: 0.009923174168120642\n",
      "Test loss: 0.045433604882823095\n",
      "Starting epoch 2544\n",
      "Training loss: 0.010040211384413673\n",
      "Test loss: 0.04550118992726008\n",
      "Starting epoch 2545\n",
      "Training loss: 0.009949438808272119\n",
      "Test loss: 0.04525494899738718\n",
      "Starting epoch 2546\n",
      "Training loss: 0.009983103600193242\n",
      "Test loss: 0.04547935503500479\n",
      "Starting epoch 2547\n",
      "Training loss: 0.009908613077074777\n",
      "Test loss: 0.04576598307876675\n",
      "Starting epoch 2548\n",
      "Training loss: 0.00996531611766483\n",
      "Test loss: 0.04570486196489246\n",
      "Starting epoch 2549\n",
      "Training loss: 0.010008414962985476\n",
      "Test loss: 0.04543459732775335\n",
      "Starting epoch 2550\n",
      "Training loss: 0.010018537431711057\n",
      "Test loss: 0.045764353747169174\n",
      "Starting epoch 2551\n",
      "Training loss: 0.00999752789369372\n",
      "Test loss: 0.045554017165192855\n",
      "Starting epoch 2552\n",
      "Training loss: 0.010240200616907879\n",
      "Test loss: 0.045499548591949324\n",
      "Starting epoch 2553\n",
      "Training loss: 0.009983164655258421\n",
      "Test loss: 0.046166731113636936\n",
      "Starting epoch 2554\n",
      "Training loss: 0.009933515573989173\n",
      "Test loss: 0.046374481861238125\n",
      "Starting epoch 2555\n",
      "Training loss: 0.010076318531617766\n",
      "Test loss: 0.046086514437640155\n",
      "Starting epoch 2556\n",
      "Training loss: 0.010185981947989737\n",
      "Test loss: 0.04556015361514357\n",
      "Starting epoch 2557\n",
      "Training loss: 0.010384728597690825\n",
      "Test loss: 0.045893856358748895\n",
      "Starting epoch 2558\n",
      "Training loss: 0.01009389622228556\n",
      "Test loss: 0.04532197821471426\n",
      "Starting epoch 2559\n",
      "Training loss: 0.009938419697287142\n",
      "Test loss: 0.04492658431883211\n",
      "Starting epoch 2560\n",
      "Training loss: 0.010006404283349632\n",
      "Test loss: 0.04522559895283646\n",
      "Starting epoch 2561\n",
      "Training loss: 0.009974858173947842\n",
      "Test loss: 0.04569919310786106\n",
      "Starting epoch 2562\n",
      "Training loss: 0.009945203321146184\n",
      "Test loss: 0.0459819120803365\n",
      "Starting epoch 2563\n",
      "Training loss: 0.010103384445070243\n",
      "Test loss: 0.04578927725001618\n",
      "Starting epoch 2564\n",
      "Training loss: 0.009952184957925414\n",
      "Test loss: 0.04551909229269734\n",
      "Starting epoch 2565\n",
      "Training loss: 0.010038219712918899\n",
      "Test loss: 0.045605398507581815\n",
      "Starting epoch 2566\n",
      "Training loss: 0.010002558852439045\n",
      "Test loss: 0.04609709757345694\n",
      "Starting epoch 2567\n",
      "Training loss: 0.009974694917680786\n",
      "Test loss: 0.04617789484284542\n",
      "Starting epoch 2568\n",
      "Training loss: 0.010031957026632106\n",
      "Test loss: 0.04585343885614916\n",
      "Starting epoch 2569\n",
      "Training loss: 0.010020130634552142\n",
      "Test loss: 0.04585066341139652\n",
      "Starting epoch 2570\n",
      "Training loss: 0.009958436712622643\n",
      "Test loss: 0.046051251943464634\n",
      "Starting epoch 2571\n",
      "Training loss: 0.010231887784282693\n",
      "Test loss: 0.046225686730058106\n",
      "Starting epoch 2572\n",
      "Training loss: 0.009973375493141472\n",
      "Test loss: 0.04644390374973968\n",
      "Starting epoch 2573\n",
      "Training loss: 0.009952017014510319\n",
      "Test loss: 0.04605107423332003\n",
      "Starting epoch 2574\n",
      "Training loss: 0.01011288828659253\n",
      "Test loss: 0.04562435713079241\n",
      "Starting epoch 2575\n",
      "Training loss: 0.009905535788809667\n",
      "Test loss: 0.04610331384120164\n",
      "Starting epoch 2576\n",
      "Training loss: 0.009947005918890726\n",
      "Test loss: 0.046073302074714946\n",
      "Starting epoch 2577\n",
      "Training loss: 0.009927975242865867\n",
      "Test loss: 0.04587823663044859\n",
      "Starting epoch 2578\n",
      "Training loss: 0.010089095880384327\n",
      "Test loss: 0.04583555860099969\n",
      "Starting epoch 2579\n",
      "Training loss: 0.009937989922454123\n",
      "Test loss: 0.046255048364400864\n",
      "Starting epoch 2580\n",
      "Training loss: 0.010028561241314059\n",
      "Test loss: 0.046166927036311894\n",
      "Starting epoch 2581\n",
      "Training loss: 0.010085379903311611\n",
      "Test loss: 0.0456032849572323\n",
      "Starting epoch 2582\n",
      "Training loss: 0.010004511454188432\n",
      "Test loss: 0.04587355628609657\n",
      "Starting epoch 2583\n",
      "Training loss: 0.009925909309846456\n",
      "Test loss: 0.0459800417776461\n",
      "Starting epoch 2584\n",
      "Training loss: 0.009936143659421654\n",
      "Test loss: 0.0459696707074289\n",
      "Starting epoch 2585\n",
      "Training loss: 0.009979553948171804\n",
      "Test loss: 0.04602414314393644\n",
      "Starting epoch 2586\n",
      "Training loss: 0.009990281898711548\n",
      "Test loss: 0.04619882476550562\n",
      "Starting epoch 2587\n",
      "Training loss: 0.009925915607724522\n",
      "Test loss: 0.04606944081132059\n",
      "Starting epoch 2588\n",
      "Training loss: 0.010013830405278284\n",
      "Test loss: 0.04596453863713476\n",
      "Starting epoch 2589\n",
      "Training loss: 0.009923520452174985\n",
      "Test loss: 0.045542985752776814\n",
      "Starting epoch 2590\n",
      "Training loss: 0.009953633011853109\n",
      "Test loss: 0.04550833931123769\n",
      "Starting epoch 2591\n",
      "Training loss: 0.009972406490171542\n",
      "Test loss: 0.045538007385200925\n",
      "Starting epoch 2592\n",
      "Training loss: 0.010039615979204412\n",
      "Test loss: 0.04592989812846537\n",
      "Starting epoch 2593\n",
      "Training loss: 0.010060387999430055\n",
      "Test loss: 0.04611774244242244\n",
      "Starting epoch 2594\n",
      "Training loss: 0.009970536623455461\n",
      "Test loss: 0.04573421045723888\n",
      "Starting epoch 2595\n",
      "Training loss: 0.010157806783548145\n",
      "Test loss: 0.045492809610786264\n",
      "Starting epoch 2596\n",
      "Training loss: 0.010070864920366983\n",
      "Test loss: 0.046031474653217525\n",
      "Starting epoch 2597\n",
      "Training loss: 0.0099157451300836\n",
      "Test loss: 0.0458518515030543\n",
      "Starting epoch 2598\n",
      "Training loss: 0.010186864582241559\n",
      "Test loss: 0.04587136567742736\n",
      "Starting epoch 2599\n",
      "Training loss: 0.0099785999990389\n",
      "Test loss: 0.046354656694112\n",
      "Starting epoch 2600\n",
      "Training loss: 0.010016024204306915\n",
      "Test loss: 0.045785576519038945\n",
      "Starting epoch 2601\n",
      "Training loss: 0.009959164869467744\n",
      "Test loss: 0.04602056534753905\n",
      "Starting epoch 2602\n",
      "Training loss: 0.0099773928217712\n",
      "Test loss: 0.04568776564189681\n",
      "Starting epoch 2603\n",
      "Training loss: 0.009941704555979518\n",
      "Test loss: 0.045613746952127526\n",
      "Starting epoch 2604\n",
      "Training loss: 0.010069501357244664\n",
      "Test loss: 0.04544242830188186\n",
      "Starting epoch 2605\n",
      "Training loss: 0.009965824833536734\n",
      "Test loss: 0.04531550503991268\n",
      "Starting epoch 2606\n",
      "Training loss: 0.0100167521932086\n",
      "Test loss: 0.045876264572143555\n",
      "Starting epoch 2607\n",
      "Training loss: 0.00991788413375616\n",
      "Test loss: 0.046034956695856874\n",
      "Starting epoch 2608\n",
      "Training loss: 0.010038426100108468\n",
      "Test loss: 0.04606486801747923\n",
      "Starting epoch 2609\n",
      "Training loss: 0.009967540070170262\n",
      "Test loss: 0.046262900034586586\n",
      "Starting epoch 2610\n",
      "Training loss: 0.010074142572760094\n",
      "Test loss: 0.04599877978088679\n",
      "Starting epoch 2611\n",
      "Training loss: 0.00993087136598884\n",
      "Test loss: 0.04617990035977629\n",
      "Starting epoch 2612\n",
      "Training loss: 0.009956464919521183\n",
      "Test loss: 0.04598367738502997\n",
      "Starting epoch 2613\n",
      "Training loss: 0.010055616925485799\n",
      "Test loss: 0.04591841705971294\n",
      "Starting epoch 2614\n",
      "Training loss: 0.009946209462512224\n",
      "Test loss: 0.0460994242242089\n",
      "Starting epoch 2615\n",
      "Training loss: 0.009938624092057103\n",
      "Test loss: 0.04588693904655951\n",
      "Starting epoch 2616\n",
      "Training loss: 0.009978046243796583\n",
      "Test loss: 0.04580762734015783\n",
      "Starting epoch 2617\n",
      "Training loss: 0.009963870659226277\n",
      "Test loss: 0.04596017232095754\n",
      "Starting epoch 2618\n",
      "Training loss: 0.009901176344175807\n",
      "Test loss: 0.04583494668757474\n",
      "Starting epoch 2619\n",
      "Training loss: 0.009986424032354453\n",
      "Test loss: 0.04584688665690245\n",
      "Starting epoch 2620\n",
      "Training loss: 0.010051120973390634\n",
      "Test loss: 0.04592557934423288\n",
      "Starting epoch 2621\n",
      "Training loss: 0.009936021442418216\n",
      "Test loss: 0.04636270194141953\n",
      "Starting epoch 2622\n",
      "Training loss: 0.00997239513872344\n",
      "Test loss: 0.04621631569332547\n",
      "Starting epoch 2623\n",
      "Training loss: 0.010073163622959715\n",
      "Test loss: 0.04613176443510585\n",
      "Starting epoch 2624\n",
      "Training loss: 0.009894009358936647\n",
      "Test loss: 0.04549199936014635\n",
      "Starting epoch 2625\n",
      "Training loss: 0.009903218429230276\n",
      "Test loss: 0.04547348225282298\n",
      "Starting epoch 2626\n",
      "Training loss: 0.010015989043062827\n",
      "Test loss: 0.04573862475377542\n",
      "Starting epoch 2627\n",
      "Training loss: 0.010007288795514185\n",
      "Test loss: 0.04536372271400911\n",
      "Starting epoch 2628\n",
      "Training loss: 0.009958347778950558\n",
      "Test loss: 0.04561731660807574\n",
      "Starting epoch 2629\n",
      "Training loss: 0.009928235326145516\n",
      "Test loss: 0.04584081470966339\n",
      "Starting epoch 2630\n",
      "Training loss: 0.010087946796270668\n",
      "Test loss: 0.04559274073000307\n",
      "Starting epoch 2631\n",
      "Training loss: 0.009974782965833047\n",
      "Test loss: 0.04602430133080041\n",
      "Starting epoch 2632\n",
      "Training loss: 0.01016395132927621\n",
      "Test loss: 0.04605993380149206\n",
      "Starting epoch 2633\n",
      "Training loss: 0.009895643051408354\n",
      "Test loss: 0.04536591428849432\n",
      "Starting epoch 2634\n",
      "Training loss: 0.009939217588818465\n",
      "Test loss: 0.045269794624160836\n",
      "Starting epoch 2635\n",
      "Training loss: 0.00991032987100179\n",
      "Test loss: 0.045441420679842984\n",
      "Starting epoch 2636\n",
      "Training loss: 0.00996335600426451\n",
      "Test loss: 0.045603290338207175\n",
      "Starting epoch 2637\n",
      "Training loss: 0.0100210200476109\n",
      "Test loss: 0.04584588193231159\n",
      "Starting epoch 2638\n",
      "Training loss: 0.009922072092895626\n",
      "Test loss: 0.04619334720902973\n",
      "Starting epoch 2639\n",
      "Training loss: 0.00993493875702385\n",
      "Test loss: 0.04595443840931963\n",
      "Starting epoch 2640\n",
      "Training loss: 0.00995502036187004\n",
      "Test loss: 0.04565073991263354\n",
      "Starting epoch 2641\n",
      "Training loss: 0.010073032260673945\n",
      "Test loss: 0.045605873896016016\n",
      "Starting epoch 2642\n",
      "Training loss: 0.009937890667895802\n",
      "Test loss: 0.046251475948978354\n",
      "Starting epoch 2643\n",
      "Training loss: 0.010054752887150303\n",
      "Test loss: 0.04599702316853735\n",
      "Starting epoch 2644\n",
      "Training loss: 0.0099618241313051\n",
      "Test loss: 0.046147719577506734\n",
      "Starting epoch 2645\n",
      "Training loss: 0.010054044746106765\n",
      "Test loss: 0.04569648992684153\n",
      "Starting epoch 2646\n",
      "Training loss: 0.009920994247325132\n",
      "Test loss: 0.04542597493639699\n",
      "Starting epoch 2647\n",
      "Training loss: 0.00992429041166286\n",
      "Test loss: 0.04559114809941362\n",
      "Starting epoch 2648\n",
      "Training loss: 0.009943640729809394\n",
      "Test loss: 0.045650943009941665\n",
      "Starting epoch 2649\n",
      "Training loss: 0.009974135650840939\n",
      "Test loss: 0.04580654134904897\n",
      "Starting epoch 2650\n",
      "Training loss: 0.009939162167491482\n",
      "Test loss: 0.04567471381138872\n",
      "Starting epoch 2651\n",
      "Training loss: 0.010066359609243323\n",
      "Test loss: 0.04546645566545151\n",
      "Starting epoch 2652\n",
      "Training loss: 0.010076807857659019\n",
      "Test loss: 0.045999832865264684\n",
      "Starting epoch 2653\n",
      "Training loss: 0.009975602667106957\n",
      "Test loss: 0.04602799785357935\n",
      "Starting epoch 2654\n",
      "Training loss: 0.00994257700675335\n",
      "Test loss: 0.04587729454592422\n",
      "Starting epoch 2655\n",
      "Training loss: 0.009896734515663053\n",
      "Test loss: 0.0456119105219841\n",
      "Starting epoch 2656\n",
      "Training loss: 0.010407032185524214\n",
      "Test loss: 0.04590081754657957\n",
      "Starting epoch 2657\n",
      "Training loss: 0.010050833271052993\n",
      "Test loss: 0.046618260305236886\n",
      "Starting epoch 2658\n",
      "Training loss: 0.01000626018789948\n",
      "Test loss: 0.046102905852927104\n",
      "Starting epoch 2659\n",
      "Training loss: 0.010057258205946351\n",
      "Test loss: 0.04577710810634825\n",
      "Starting epoch 2660\n",
      "Training loss: 0.009995789083911747\n",
      "Test loss: 0.045463102559248604\n",
      "Starting epoch 2661\n",
      "Training loss: 0.010011784213243937\n",
      "Test loss: 0.04602638852817041\n",
      "Starting epoch 2662\n",
      "Training loss: 0.009976662023634206\n",
      "Test loss: 0.04593158168373285\n",
      "Starting epoch 2663\n",
      "Training loss: 0.009971610041426831\n",
      "Test loss: 0.04561168714253991\n",
      "Starting epoch 2664\n",
      "Training loss: 0.010010948511542843\n",
      "Test loss: 0.045720099850937175\n",
      "Starting epoch 2665\n",
      "Training loss: 0.009983412616077016\n",
      "Test loss: 0.04585881227696383\n",
      "Starting epoch 2666\n",
      "Training loss: 0.009975277345444336\n",
      "Test loss: 0.04564022741935871\n",
      "Starting epoch 2667\n",
      "Training loss: 0.01011875142022723\n",
      "Test loss: 0.0454824533727434\n",
      "Starting epoch 2668\n",
      "Training loss: 0.010095334046932518\n",
      "Test loss: 0.04539888158992485\n",
      "Starting epoch 2669\n",
      "Training loss: 0.0099872870248605\n",
      "Test loss: 0.045192051885856524\n",
      "Starting epoch 2670\n",
      "Training loss: 0.009916839495179106\n",
      "Test loss: 0.04578073722896753\n",
      "Starting epoch 2671\n",
      "Training loss: 0.010008696573557423\n",
      "Test loss: 0.04588007154288115\n",
      "Starting epoch 2672\n",
      "Training loss: 0.0099000021448878\n",
      "Test loss: 0.04582042729964963\n",
      "Starting epoch 2673\n",
      "Training loss: 0.010120608309497599\n",
      "Test loss: 0.04589970741007063\n",
      "Starting epoch 2674\n",
      "Training loss: 0.009968638603315979\n",
      "Test loss: 0.046143010120700906\n",
      "Starting epoch 2675\n",
      "Training loss: 0.009986337747608051\n",
      "Test loss: 0.04615214549832874\n",
      "Starting epoch 2676\n",
      "Training loss: 0.009941368256924583\n",
      "Test loss: 0.04579788170478962\n",
      "Starting epoch 2677\n",
      "Training loss: 0.01015212481505558\n",
      "Test loss: 0.04589204779929585\n",
      "Starting epoch 2678\n",
      "Training loss: 0.010058478940827925\n",
      "Test loss: 0.04553408341275321\n",
      "Starting epoch 2679\n",
      "Training loss: 0.009920411346266504\n",
      "Test loss: 0.04613236282710676\n",
      "Starting epoch 2680\n",
      "Training loss: 0.009951512749138915\n",
      "Test loss: 0.04634477846600391\n",
      "Starting epoch 2681\n",
      "Training loss: 0.010184014979444567\n",
      "Test loss: 0.04610157095723682\n",
      "Starting epoch 2682\n",
      "Training loss: 0.010011265023809965\n",
      "Test loss: 0.04639272709135656\n",
      "Starting epoch 2683\n",
      "Training loss: 0.010044013379050082\n",
      "Test loss: 0.04652798451759197\n",
      "Starting epoch 2684\n",
      "Training loss: 0.010161861578827022\n",
      "Test loss: 0.04562134178424323\n",
      "Starting epoch 2685\n",
      "Training loss: 0.009925666433133062\n",
      "Test loss: 0.044994771687520876\n",
      "Starting epoch 2686\n",
      "Training loss: 0.010144340882047277\n",
      "Test loss: 0.04557188103596369\n",
      "Starting epoch 2687\n",
      "Training loss: 0.010055104117901599\n",
      "Test loss: 0.0461835564562568\n",
      "Starting epoch 2688\n",
      "Training loss: 0.009975080126438473\n",
      "Test loss: 0.04567737629016241\n",
      "Starting epoch 2689\n",
      "Training loss: 0.009961418425816982\n",
      "Test loss: 0.04519142362254637\n",
      "Starting epoch 2690\n",
      "Training loss: 0.010024182696933628\n",
      "Test loss: 0.04553818633710897\n",
      "Starting epoch 2691\n",
      "Training loss: 0.00992198305998425\n",
      "Test loss: 0.04541589471477049\n",
      "Starting epoch 2692\n",
      "Training loss: 0.009964844166133248\n",
      "Test loss: 0.045500770763114644\n",
      "Starting epoch 2693\n",
      "Training loss: 0.009950262990581696\n",
      "Test loss: 0.04555340652802476\n",
      "Starting epoch 2694\n",
      "Training loss: 0.0098859771444905\n",
      "Test loss: 0.0453547062697234\n",
      "Starting epoch 2695\n",
      "Training loss: 0.009953103898490061\n",
      "Test loss: 0.045573255944031256\n",
      "Starting epoch 2696\n",
      "Training loss: 0.009945376959369808\n",
      "Test loss: 0.045789120994784216\n",
      "Starting epoch 2697\n",
      "Training loss: 0.010061968743923257\n",
      "Test loss: 0.045846603258892345\n",
      "Starting epoch 2698\n",
      "Training loss: 0.009921279628990127\n",
      "Test loss: 0.04566857784434601\n",
      "Starting epoch 2699\n",
      "Training loss: 0.009991718608824933\n",
      "Test loss: 0.04575446703367763\n",
      "Starting epoch 2700\n",
      "Training loss: 0.009938860342639392\n",
      "Test loss: 0.04580702825828835\n",
      "Starting epoch 2701\n",
      "Training loss: 0.009922712323729132\n",
      "Test loss: 0.04599960037955531\n",
      "Starting epoch 2702\n",
      "Training loss: 0.009960915076500569\n",
      "Test loss: 0.045895131649794405\n",
      "Starting epoch 2703\n",
      "Training loss: 0.009953713731565436\n",
      "Test loss: 0.04550679931762042\n",
      "Starting epoch 2704\n",
      "Training loss: 0.010010782842997645\n",
      "Test loss: 0.045360421003014954\n",
      "Starting epoch 2705\n",
      "Training loss: 0.00990640077373532\n",
      "Test loss: 0.045668695880858985\n",
      "Starting epoch 2706\n",
      "Training loss: 0.009937730007117888\n",
      "Test loss: 0.04581650291328077\n",
      "Starting epoch 2707\n",
      "Training loss: 0.01004127054246234\n",
      "Test loss: 0.04602495953440666\n",
      "Starting epoch 2708\n",
      "Training loss: 0.00995098680379938\n",
      "Test loss: 0.045515459306814054\n",
      "Starting epoch 2709\n",
      "Training loss: 0.009913987022076473\n",
      "Test loss: 0.04546379353161211\n",
      "Starting epoch 2710\n",
      "Training loss: 0.009923676433438649\n",
      "Test loss: 0.045752554993938516\n",
      "Starting epoch 2711\n",
      "Training loss: 0.009994153024964645\n",
      "Test loss: 0.04588630712694592\n",
      "Starting epoch 2712\n",
      "Training loss: 0.009964636206382611\n",
      "Test loss: 0.04611166794266966\n",
      "Starting epoch 2713\n",
      "Training loss: 0.009928684544245728\n",
      "Test loss: 0.046179699539034454\n",
      "Starting epoch 2714\n",
      "Training loss: 0.00994077221047683\n",
      "Test loss: 0.04582633533411556\n",
      "Starting epoch 2715\n",
      "Training loss: 0.009918529402892121\n",
      "Test loss: 0.045720902306062204\n",
      "Starting epoch 2716\n",
      "Training loss: 0.010165468911777754\n",
      "Test loss: 0.045611386083894305\n",
      "Starting epoch 2717\n",
      "Training loss: 0.010175290990804062\n",
      "Test loss: 0.04530721426837974\n",
      "Starting epoch 2718\n",
      "Training loss: 0.00991426532721666\n",
      "Test loss: 0.045881886311151365\n",
      "Starting epoch 2719\n",
      "Training loss: 0.010102188428405856\n",
      "Test loss: 0.045987477595055545\n",
      "Starting epoch 2720\n",
      "Training loss: 0.009941777168605172\n",
      "Test loss: 0.04628379084169865\n",
      "Starting epoch 2721\n",
      "Training loss: 0.01002631844861097\n",
      "Test loss: 0.046196250038014516\n",
      "Starting epoch 2722\n",
      "Training loss: 0.010032960977099959\n",
      "Test loss: 0.04596912529733446\n",
      "Starting epoch 2723\n",
      "Training loss: 0.01003630415032633\n",
      "Test loss: 0.04574063406498344\n",
      "Starting epoch 2724\n",
      "Training loss: 0.009965148830755811\n",
      "Test loss: 0.045951808906263776\n",
      "Starting epoch 2725\n",
      "Training loss: 0.009923257880279274\n",
      "Test loss: 0.045975515687907184\n",
      "Starting epoch 2726\n",
      "Training loss: 0.010072789872523214\n",
      "Test loss: 0.045923283530606165\n",
      "Starting epoch 2727\n",
      "Training loss: 0.010125704811977559\n",
      "Test loss: 0.04635127895960101\n",
      "Starting epoch 2728\n",
      "Training loss: 0.009980499836020782\n",
      "Test loss: 0.04679972264501783\n",
      "Starting epoch 2729\n",
      "Training loss: 0.010086766108259803\n",
      "Test loss: 0.046315170962501456\n",
      "Starting epoch 2730\n",
      "Training loss: 0.009978518424341913\n",
      "Test loss: 0.04611826826024939\n",
      "Starting epoch 2731\n",
      "Training loss: 0.010190393897842188\n",
      "Test loss: 0.04561031816734208\n",
      "Starting epoch 2732\n",
      "Training loss: 0.010093269862051382\n",
      "Test loss: 0.0462291738777249\n",
      "Starting epoch 2733\n",
      "Training loss: 0.009925741580177526\n",
      "Test loss: 0.04550467486734743\n",
      "Starting epoch 2734\n",
      "Training loss: 0.010082383991264906\n",
      "Test loss: 0.04556922421411232\n",
      "Starting epoch 2735\n",
      "Training loss: 0.009982692123558677\n",
      "Test loss: 0.045211487346225314\n",
      "Starting epoch 2736\n",
      "Training loss: 0.009942436957212746\n",
      "Test loss: 0.04572133719921112\n",
      "Starting epoch 2737\n",
      "Training loss: 0.009907960693245052\n",
      "Test loss: 0.04565655288321001\n",
      "Starting epoch 2738\n",
      "Training loss: 0.009933258208339332\n",
      "Test loss: 0.045655105607929056\n",
      "Starting epoch 2739\n",
      "Training loss: 0.009950162827602176\n",
      "Test loss: 0.04574147680843318\n",
      "Starting epoch 2740\n",
      "Training loss: 0.01001405166309388\n",
      "Test loss: 0.04582300499357559\n",
      "Starting epoch 2741\n",
      "Training loss: 0.009917173626237228\n",
      "Test loss: 0.04563251123936088\n",
      "Starting epoch 2742\n",
      "Training loss: 0.00990252021211581\n",
      "Test loss: 0.045656787162577664\n",
      "Starting epoch 2743\n",
      "Training loss: 0.009918434369820551\n",
      "Test loss: 0.04572109836671087\n",
      "Starting epoch 2744\n",
      "Training loss: 0.0102491682761761\n",
      "Test loss: 0.04589966739769335\n",
      "Starting epoch 2745\n",
      "Training loss: 0.01005535392609776\n",
      "Test loss: 0.045222971864320616\n",
      "Starting epoch 2746\n",
      "Training loss: 0.00999645930027864\n",
      "Test loss: 0.04563338757941016\n",
      "Starting epoch 2747\n",
      "Training loss: 0.009931242887358197\n",
      "Test loss: 0.04580764279321388\n",
      "Starting epoch 2748\n",
      "Training loss: 0.009916543624684459\n",
      "Test loss: 0.04597743586809547\n",
      "Starting epoch 2749\n",
      "Training loss: 0.01025607917824241\n",
      "Test loss: 0.04615472491692613\n",
      "Starting epoch 2750\n",
      "Training loss: 0.010109316496575465\n",
      "Test loss: 0.04535595727739511\n",
      "Starting epoch 2751\n",
      "Training loss: 0.009998994161848162\n",
      "Test loss: 0.045988343518089364\n",
      "Starting epoch 2752\n",
      "Training loss: 0.009967457686291366\n",
      "Test loss: 0.04615965513167558\n",
      "Starting epoch 2753\n",
      "Training loss: 0.009963688181071986\n",
      "Test loss: 0.04593896369139353\n",
      "Starting epoch 2754\n",
      "Training loss: 0.010110030286624784\n",
      "Test loss: 0.04568444164814772\n",
      "Starting epoch 2755\n",
      "Training loss: 0.00996974849554359\n",
      "Test loss: 0.046182775800978695\n",
      "Starting epoch 2756\n",
      "Training loss: 0.010115918917123412\n",
      "Test loss: 0.04623926223980056\n",
      "Starting epoch 2757\n",
      "Training loss: 0.009976297128395956\n",
      "Test loss: 0.04647804727708852\n",
      "Starting epoch 2758\n",
      "Training loss: 0.010046103129499271\n",
      "Test loss: 0.04587752841137074\n",
      "Starting epoch 2759\n",
      "Training loss: 0.009897586568945745\n",
      "Test loss: 0.045331000729843425\n",
      "Starting epoch 2760\n",
      "Training loss: 0.009905961403226266\n",
      "Test loss: 0.045567165232366987\n",
      "Starting epoch 2761\n",
      "Training loss: 0.009967644973734363\n",
      "Test loss: 0.04585415842356505\n",
      "Starting epoch 2762\n",
      "Training loss: 0.009976178789358647\n",
      "Test loss: 0.04586330635680093\n",
      "Starting epoch 2763\n",
      "Training loss: 0.00991176674905859\n",
      "Test loss: 0.04586241180422129\n",
      "Starting epoch 2764\n",
      "Training loss: 0.009972014800324792\n",
      "Test loss: 0.045772715023270356\n",
      "Starting epoch 2765\n",
      "Training loss: 0.010110665845577835\n",
      "Test loss: 0.046185411374878\n",
      "Starting epoch 2766\n",
      "Training loss: 0.009916537593989099\n",
      "Test loss: 0.046535757956681426\n",
      "Starting epoch 2767\n",
      "Training loss: 0.010012688267914975\n",
      "Test loss: 0.046223285159579026\n",
      "Starting epoch 2768\n",
      "Training loss: 0.00995980442852759\n",
      "Test loss: 0.046276357024908066\n",
      "Starting epoch 2769\n",
      "Training loss: 0.01017654467313016\n",
      "Test loss: 0.0461481174937001\n",
      "Starting epoch 2770\n",
      "Training loss: 0.009927293362065416\n",
      "Test loss: 0.04638646584418085\n",
      "Starting epoch 2771\n",
      "Training loss: 0.00996422292817323\n",
      "Test loss: 0.046391989207930036\n",
      "Starting epoch 2772\n",
      "Training loss: 0.00993060696198315\n",
      "Test loss: 0.045764643836904456\n",
      "Starting epoch 2773\n",
      "Training loss: 0.009892852228806644\n",
      "Test loss: 0.04582630663558289\n",
      "Starting epoch 2774\n",
      "Training loss: 0.010036632800322087\n",
      "Test loss: 0.04575628442344842\n",
      "Starting epoch 2775\n",
      "Training loss: 0.009882569984822977\n",
      "Test loss: 0.04584826735986604\n",
      "Starting epoch 2776\n",
      "Training loss: 0.009951423609354457\n",
      "Test loss: 0.04578859883326071\n",
      "Starting epoch 2777\n",
      "Training loss: 0.01015535555779934\n",
      "Test loss: 0.046051405646182875\n",
      "Starting epoch 2778\n",
      "Training loss: 0.00994111389898863\n",
      "Test loss: 0.045691760187899624\n",
      "Starting epoch 2779\n",
      "Training loss: 0.010009963279131983\n",
      "Test loss: 0.0456228528033804\n",
      "Starting epoch 2780\n",
      "Training loss: 0.009981481007254515\n",
      "Test loss: 0.046146520861872924\n",
      "Starting epoch 2781\n",
      "Training loss: 0.010227583806778564\n",
      "Test loss: 0.04633474584530901\n",
      "Starting epoch 2782\n",
      "Training loss: 0.0099336389818641\n",
      "Test loss: 0.04572935250622255\n",
      "Starting epoch 2783\n",
      "Training loss: 0.009980246165126074\n",
      "Test loss: 0.04550312514658327\n",
      "Starting epoch 2784\n",
      "Training loss: 0.009959418326616287\n",
      "Test loss: 0.04539576683331419\n",
      "Starting epoch 2785\n",
      "Training loss: 0.009958885098639571\n",
      "Test loss: 0.04603059658849681\n",
      "Starting epoch 2786\n",
      "Training loss: 0.00992120752782851\n",
      "Test loss: 0.046137448883167016\n",
      "Starting epoch 2787\n",
      "Training loss: 0.009962710719860967\n",
      "Test loss: 0.04572742156408451\n",
      "Starting epoch 2788\n",
      "Training loss: 0.010165353641524667\n",
      "Test loss: 0.046012187721552675\n",
      "Starting epoch 2789\n",
      "Training loss: 0.009931229009125077\n",
      "Test loss: 0.04543676765428649\n",
      "Starting epoch 2790\n",
      "Training loss: 0.009919807192732076\n",
      "Test loss: 0.04567926501234373\n",
      "Starting epoch 2791\n",
      "Training loss: 0.009963416998259356\n",
      "Test loss: 0.045923457515460474\n",
      "Starting epoch 2792\n",
      "Training loss: 0.009959174747594068\n",
      "Test loss: 0.0459426582135536\n",
      "Starting epoch 2793\n",
      "Training loss: 0.010184864025013368\n",
      "Test loss: 0.04617958695248321\n",
      "Starting epoch 2794\n",
      "Training loss: 0.009913449084050343\n",
      "Test loss: 0.04570459505474126\n",
      "Starting epoch 2795\n",
      "Training loss: 0.009923285438266934\n",
      "Test loss: 0.045938455396228366\n",
      "Starting epoch 2796\n",
      "Training loss: 0.009887159710414097\n",
      "Test loss: 0.04621719003275589\n",
      "Starting epoch 2797\n",
      "Training loss: 0.009883730626497113\n",
      "Test loss: 0.04606065029899279\n",
      "Starting epoch 2798\n",
      "Training loss: 0.01000469721487311\n",
      "Test loss: 0.04597829275385097\n",
      "Starting epoch 2799\n",
      "Training loss: 0.009916062848489792\n",
      "Test loss: 0.046306773330326435\n",
      "Starting epoch 2800\n",
      "Training loss: 0.009962408269037966\n",
      "Test loss: 0.046051136735412806\n",
      "Starting epoch 2801\n",
      "Training loss: 0.009945636111326882\n",
      "Test loss: 0.046159514122539096\n",
      "Starting epoch 2802\n",
      "Training loss: 0.00993331390448281\n",
      "Test loss: 0.04620993951404536\n",
      "Starting epoch 2803\n",
      "Training loss: 0.009942319251780138\n",
      "Test loss: 0.04596514737716428\n",
      "Starting epoch 2804\n",
      "Training loss: 0.00988461109275212\n",
      "Test loss: 0.046053863371963856\n",
      "Starting epoch 2805\n",
      "Training loss: 0.009920826090163872\n",
      "Test loss: 0.045969576333408\n",
      "Starting epoch 2806\n",
      "Training loss: 0.009918096322627341\n",
      "Test loss: 0.04584567504072631\n",
      "Starting epoch 2807\n",
      "Training loss: 0.010001043392131563\n",
      "Test loss: 0.045706147948900856\n",
      "Starting epoch 2808\n",
      "Training loss: 0.01006985464911969\n",
      "Test loss: 0.04606539000653558\n",
      "Starting epoch 2809\n",
      "Training loss: 0.010059098438283459\n",
      "Test loss: 0.04542652420975544\n",
      "Starting epoch 2810\n",
      "Training loss: 0.009979332629285876\n",
      "Test loss: 0.045140539606412254\n",
      "Starting epoch 2811\n",
      "Training loss: 0.009964850807531934\n",
      "Test loss: 0.045610945464836225\n",
      "Starting epoch 2812\n",
      "Training loss: 0.00998782619956087\n",
      "Test loss: 0.04604327416530362\n",
      "Starting epoch 2813\n",
      "Training loss: 0.009912481989528312\n",
      "Test loss: 0.04579577650184984\n",
      "Starting epoch 2814\n",
      "Training loss: 0.009946498989326055\n",
      "Test loss: 0.045736081484291286\n",
      "Starting epoch 2815\n",
      "Training loss: 0.009934955108605448\n",
      "Test loss: 0.04594382009020558\n",
      "Starting epoch 2816\n",
      "Training loss: 0.009974816936205645\n",
      "Test loss: 0.045803526485407794\n",
      "Starting epoch 2817\n",
      "Training loss: 0.01010717359966919\n",
      "Test loss: 0.04604950657597295\n",
      "Starting epoch 2818\n",
      "Training loss: 0.009916494783685833\n",
      "Test loss: 0.04645933431607706\n",
      "Starting epoch 2819\n",
      "Training loss: 0.010044664785754486\n",
      "Test loss: 0.04603000951034052\n",
      "Starting epoch 2820\n",
      "Training loss: 0.009963832933028213\n",
      "Test loss: 0.046246326631969877\n",
      "Starting epoch 2821\n",
      "Training loss: 0.00992925283422724\n",
      "Test loss: 0.04629281039039294\n",
      "Starting epoch 2822\n",
      "Training loss: 0.009923666379735118\n",
      "Test loss: 0.045917881583725964\n",
      "Starting epoch 2823\n",
      "Training loss: 0.009944419971987849\n",
      "Test loss: 0.045971288863155574\n",
      "Starting epoch 2824\n",
      "Training loss: 0.009905184787072118\n",
      "Test loss: 0.04594306261451156\n",
      "Starting epoch 2825\n",
      "Training loss: 0.010009029971771553\n",
      "Test loss: 0.04558197615875138\n",
      "Starting epoch 2826\n",
      "Training loss: 0.010076948884324949\n",
      "Test loss: 0.045648977574374944\n",
      "Starting epoch 2827\n",
      "Training loss: 0.009963976936872865\n",
      "Test loss: 0.046096991678630864\n",
      "Starting epoch 2828\n",
      "Training loss: 0.010003578101025253\n",
      "Test loss: 0.04570826074039495\n",
      "Starting epoch 2829\n",
      "Training loss: 0.009930604702380837\n",
      "Test loss: 0.045503511748932024\n",
      "Starting epoch 2830\n",
      "Training loss: 0.009960194576348438\n",
      "Test loss: 0.0455101260708438\n",
      "Starting epoch 2831\n",
      "Training loss: 0.00995898233024312\n",
      "Test loss: 0.04580084607005119\n",
      "Starting epoch 2832\n",
      "Training loss: 0.010005321964377263\n",
      "Test loss: 0.04614093416818866\n",
      "Starting epoch 2833\n",
      "Training loss: 0.010002954557660173\n",
      "Test loss: 0.04627692664938944\n",
      "Starting epoch 2834\n",
      "Training loss: 0.010001443830303481\n",
      "Test loss: 0.046362619295164394\n",
      "Starting epoch 2835\n",
      "Training loss: 0.01006028265310604\n",
      "Test loss: 0.045865696475461674\n",
      "Starting epoch 2836\n",
      "Training loss: 0.009893698312464307\n",
      "Test loss: 0.04619703938563665\n",
      "Starting epoch 2837\n",
      "Training loss: 0.009942328740583092\n",
      "Test loss: 0.04608432755426124\n",
      "Starting epoch 2838\n",
      "Training loss: 0.010097780356519535\n",
      "Test loss: 0.04606778257422977\n",
      "Starting epoch 2839\n",
      "Training loss: 0.009935103127824479\n",
      "Test loss: 0.04653940432601505\n",
      "Starting epoch 2840\n",
      "Training loss: 0.009943121280826505\n",
      "Test loss: 0.04614220566495701\n",
      "Starting epoch 2841\n",
      "Training loss: 0.00988521042173026\n",
      "Test loss: 0.04584632248238281\n",
      "Starting epoch 2842\n",
      "Training loss: 0.009958697666154533\n",
      "Test loss: 0.04558539776890366\n",
      "Starting epoch 2843\n",
      "Training loss: 0.009967333316558698\n",
      "Test loss: 0.0455812171653465\n",
      "Starting epoch 2844\n",
      "Training loss: 0.009945241764920657\n",
      "Test loss: 0.045459365403210675\n",
      "Starting epoch 2845\n",
      "Training loss: 0.009914485661343474\n",
      "Test loss: 0.04567425035768085\n",
      "Starting epoch 2846\n",
      "Training loss: 0.010077197898606786\n",
      "Test loss: 0.045879733093358854\n",
      "Starting epoch 2847\n",
      "Training loss: 0.009960810470654339\n",
      "Test loss: 0.04624961902973829\n",
      "Starting epoch 2848\n",
      "Training loss: 0.01003298132878835\n",
      "Test loss: 0.0458861554938334\n",
      "Starting epoch 2849\n",
      "Training loss: 0.009995152761579537\n",
      "Test loss: 0.04582681779370264\n",
      "Starting epoch 2850\n",
      "Training loss: 0.009903487550919174\n",
      "Test loss: 0.04593985055193857\n",
      "Starting epoch 2851\n",
      "Training loss: 0.010173320419109259\n",
      "Test loss: 0.046107588680805986\n",
      "Starting epoch 2852\n",
      "Training loss: 0.009942023129370368\n",
      "Test loss: 0.046606401878374594\n",
      "Starting epoch 2853\n",
      "Training loss: 0.0099226320620443\n",
      "Test loss: 0.04624815892290186\n",
      "Starting epoch 2854\n",
      "Training loss: 0.009949561712316802\n",
      "Test loss: 0.04596929376324018\n",
      "Starting epoch 2855\n",
      "Training loss: 0.009918097452428496\n",
      "Test loss: 0.0457098038384208\n",
      "Starting epoch 2856\n",
      "Training loss: 0.009902763607927034\n",
      "Test loss: 0.045720372486997535\n",
      "Starting epoch 2857\n",
      "Training loss: 0.009901362471282482\n",
      "Test loss: 0.045855825284013045\n",
      "Starting epoch 2858\n",
      "Training loss: 0.009926877732648224\n",
      "Test loss: 0.04595608595344755\n",
      "Starting epoch 2859\n",
      "Training loss: 0.009934201485431586\n",
      "Test loss: 0.04601295830474959\n",
      "Starting epoch 2860\n",
      "Training loss: 0.010039647109806538\n",
      "Test loss: 0.045579918970664345\n",
      "Starting epoch 2861\n",
      "Training loss: 0.009981637103025054\n",
      "Test loss: 0.046097773092764395\n",
      "Starting epoch 2862\n",
      "Training loss: 0.010007264474254162\n",
      "Test loss: 0.04583779460302106\n",
      "Starting epoch 2863\n",
      "Training loss: 0.00995922299315695\n",
      "Test loss: 0.045620689168572426\n",
      "Starting epoch 2864\n",
      "Training loss: 0.009941111036316782\n",
      "Test loss: 0.046103598067054045\n",
      "Starting epoch 2865\n",
      "Training loss: 0.00991003795481119\n",
      "Test loss: 0.04597055111770277\n",
      "Starting epoch 2866\n",
      "Training loss: 0.009917681929884388\n",
      "Test loss: 0.04596085998195189\n",
      "Starting epoch 2867\n",
      "Training loss: 0.010094435656534844\n",
      "Test loss: 0.04568574232635675\n",
      "Starting epoch 2868\n",
      "Training loss: 0.009926553327040594\n",
      "Test loss: 0.046112768834939706\n",
      "Starting epoch 2869\n",
      "Training loss: 0.010041143623043279\n",
      "Test loss: 0.04613775211489863\n",
      "Starting epoch 2870\n",
      "Training loss: 0.01016459386551478\n",
      "Test loss: 0.04568998391429583\n",
      "Starting epoch 2871\n",
      "Training loss: 0.009911466344091737\n",
      "Test loss: 0.04524762859499013\n",
      "Starting epoch 2872\n",
      "Training loss: 0.01000196462283369\n",
      "Test loss: 0.04570688327981366\n",
      "Starting epoch 2873\n",
      "Training loss: 0.010091923589467025\n",
      "Test loss: 0.04625787337621053\n",
      "Starting epoch 2874\n",
      "Training loss: 0.009914932696179289\n",
      "Test loss: 0.046004752180090654\n",
      "Starting epoch 2875\n",
      "Training loss: 0.009916245097630337\n",
      "Test loss: 0.04598004219156725\n",
      "Starting epoch 2876\n",
      "Training loss: 0.009898837243557954\n",
      "Test loss: 0.04584606585127336\n",
      "Starting epoch 2877\n",
      "Training loss: 0.009904744363099825\n",
      "Test loss: 0.04602756765153673\n",
      "Starting epoch 2878\n",
      "Training loss: 0.009920675460188115\n",
      "Test loss: 0.04597289777464337\n",
      "Starting epoch 2879\n",
      "Training loss: 0.009896693613807687\n",
      "Test loss: 0.04588743547598521\n",
      "Starting epoch 2880\n",
      "Training loss: 0.00992183332316211\n",
      "Test loss: 0.045946557764653805\n",
      "Starting epoch 2881\n",
      "Training loss: 0.009915597003991486\n",
      "Test loss: 0.046000537634999665\n",
      "Starting epoch 2882\n",
      "Training loss: 0.009890725011708306\n",
      "Test loss: 0.045734030918942556\n",
      "Starting epoch 2883\n",
      "Training loss: 0.010240993386165041\n",
      "Test loss: 0.045899763151451396\n",
      "Starting epoch 2884\n",
      "Training loss: 0.01001869340534093\n",
      "Test loss: 0.046587761491537094\n",
      "Starting epoch 2885\n",
      "Training loss: 0.009981934943037932\n",
      "Test loss: 0.046136861460076437\n",
      "Starting epoch 2886\n",
      "Training loss: 0.009966688169563403\n",
      "Test loss: 0.04636902279324002\n",
      "Starting epoch 2887\n",
      "Training loss: 0.010447381369647433\n",
      "Test loss: 0.04610807386537393\n",
      "Starting epoch 2888\n",
      "Training loss: 0.00989469436959165\n",
      "Test loss: 0.04674355865076736\n",
      "Starting epoch 2889\n",
      "Training loss: 0.010348148170675411\n",
      "Test loss: 0.04647485442735531\n",
      "Starting epoch 2890\n",
      "Training loss: 0.009906542678287283\n",
      "Test loss: 0.04664649372851407\n",
      "Starting epoch 2891\n",
      "Training loss: 0.009902795501908318\n",
      "Test loss: 0.04611835049258338\n",
      "Starting epoch 2892\n",
      "Training loss: 0.009925288835265597\n",
      "Test loss: 0.045641198064442036\n",
      "Starting epoch 2893\n",
      "Training loss: 0.009909665059359347\n",
      "Test loss: 0.04587181795526434\n",
      "Starting epoch 2894\n",
      "Training loss: 0.009865098632872105\n",
      "Test loss: 0.04577073096125214\n",
      "Starting epoch 2895\n",
      "Training loss: 0.009894622802795445\n",
      "Test loss: 0.04587377828580362\n",
      "Starting epoch 2896\n",
      "Training loss: 0.009913098234988626\n",
      "Test loss: 0.045724583996666804\n",
      "Starting epoch 2897\n",
      "Training loss: 0.010045782800336352\n",
      "Test loss: 0.045851990649545635\n",
      "Starting epoch 2898\n",
      "Training loss: 0.010083035779658888\n",
      "Test loss: 0.0464446761265949\n",
      "Starting epoch 2899\n",
      "Training loss: 0.009937966624122174\n",
      "Test loss: 0.045814606878492564\n",
      "Starting epoch 2900\n",
      "Training loss: 0.00994511846391881\n",
      "Test loss: 0.04573649471556699\n",
      "Starting epoch 2901\n",
      "Training loss: 0.009929465755942415\n",
      "Test loss: 0.04572910063520626\n",
      "Starting epoch 2902\n",
      "Training loss: 0.010180540749284088\n",
      "Test loss: 0.04573382723524615\n",
      "Starting epoch 2903\n",
      "Training loss: 0.009883898722588038\n",
      "Test loss: 0.046286848822125685\n",
      "Starting epoch 2904\n",
      "Training loss: 0.009956120849266404\n",
      "Test loss: 0.04591683132780923\n",
      "Starting epoch 2905\n",
      "Training loss: 0.009929649104348948\n",
      "Test loss: 0.04588471891151534\n",
      "Starting epoch 2906\n",
      "Training loss: 0.00991011175067454\n",
      "Test loss: 0.045840935988558665\n",
      "Starting epoch 2907\n",
      "Training loss: 0.009886151439220201\n",
      "Test loss: 0.045635735133179915\n",
      "Starting epoch 2908\n",
      "Training loss: 0.009961382699672316\n",
      "Test loss: 0.045789147554724303\n",
      "Starting epoch 2909\n",
      "Training loss: 0.00994435542064612\n",
      "Test loss: 0.046218238977922335\n",
      "Starting epoch 2910\n",
      "Training loss: 0.009925371463425824\n",
      "Test loss: 0.045965220089311955\n",
      "Starting epoch 2911\n",
      "Training loss: 0.009881164832804048\n",
      "Test loss: 0.04592032399442461\n",
      "Starting epoch 2912\n",
      "Training loss: 0.00998964821767123\n",
      "Test loss: 0.04613467016153865\n",
      "Starting epoch 2913\n",
      "Training loss: 0.009960867143923142\n",
      "Test loss: 0.04561383477239697\n",
      "Starting epoch 2914\n",
      "Training loss: 0.009915966464237112\n",
      "Test loss: 0.04519745045238071\n",
      "Starting epoch 2915\n",
      "Training loss: 0.010017927858184596\n",
      "Test loss: 0.04551121316574238\n",
      "Starting epoch 2916\n",
      "Training loss: 0.009940112246292048\n",
      "Test loss: 0.04564628577618687\n",
      "Starting epoch 2917\n",
      "Training loss: 0.009910548487525494\n",
      "Test loss: 0.04567985153860516\n",
      "Starting epoch 2918\n",
      "Training loss: 0.009897797750156433\n",
      "Test loss: 0.045828676265147\n",
      "Starting epoch 2919\n",
      "Training loss: 0.01000354506197523\n",
      "Test loss: 0.04582539111099861\n",
      "Starting epoch 2920\n",
      "Training loss: 0.009865064593795382\n",
      "Test loss: 0.04625329237293314\n",
      "Starting epoch 2921\n",
      "Training loss: 0.010003757479860157\n",
      "Test loss: 0.04586334733499421\n",
      "Starting epoch 2922\n",
      "Training loss: 0.009986341228617019\n",
      "Test loss: 0.04557953623157961\n",
      "Starting epoch 2923\n",
      "Training loss: 0.010008105061581878\n",
      "Test loss: 0.046001225158020305\n",
      "Starting epoch 2924\n",
      "Training loss: 0.009940146254833604\n",
      "Test loss: 0.045662670775696086\n",
      "Starting epoch 2925\n",
      "Training loss: 0.009936430918999383\n",
      "Test loss: 0.04604836263590389\n",
      "Starting epoch 2926\n",
      "Training loss: 0.010009992867708206\n",
      "Test loss: 0.04573225036815361\n",
      "Starting epoch 2927\n",
      "Training loss: 0.009929508825794596\n",
      "Test loss: 0.04561647827978487\n",
      "Starting epoch 2928\n",
      "Training loss: 0.009963695685089122\n",
      "Test loss: 0.04608128695852227\n",
      "Starting epoch 2929\n",
      "Training loss: 0.009950521638708526\n",
      "Test loss: 0.04584581363532278\n",
      "Starting epoch 2930\n",
      "Training loss: 0.009910684643832386\n",
      "Test loss: 0.04566285773007958\n",
      "Starting epoch 2931\n",
      "Training loss: 0.009896515257900855\n",
      "Test loss: 0.045864010505654196\n",
      "Starting epoch 2932\n",
      "Training loss: 0.009986191896385834\n",
      "Test loss: 0.04573315947696015\n",
      "Starting epoch 2933\n",
      "Training loss: 0.010083829755055123\n",
      "Test loss: 0.04563823080173245\n",
      "Starting epoch 2934\n",
      "Training loss: 0.009968835662012218\n",
      "Test loss: 0.04608638363855856\n",
      "Starting epoch 2935\n",
      "Training loss: 0.009960687642947573\n",
      "Test loss: 0.04614692940204232\n",
      "Starting epoch 2936\n",
      "Training loss: 0.009940676681208805\n",
      "Test loss: 0.04573796510144516\n",
      "Starting epoch 2937\n",
      "Training loss: 0.009926177423874864\n",
      "Test loss: 0.04568571956069381\n",
      "Starting epoch 2938\n",
      "Training loss: 0.009976163292761709\n",
      "Test loss: 0.04581328578017376\n",
      "Starting epoch 2939\n",
      "Training loss: 0.009942576060163194\n",
      "Test loss: 0.04630086295030735\n",
      "Starting epoch 2940\n",
      "Training loss: 0.009951618988616545\n",
      "Test loss: 0.046106033165145804\n",
      "Starting epoch 2941\n",
      "Training loss: 0.0100080350139102\n",
      "Test loss: 0.04592195208425875\n",
      "Starting epoch 2942\n",
      "Training loss: 0.009900662376255285\n",
      "Test loss: 0.04598690321048101\n",
      "Starting epoch 2943\n",
      "Training loss: 0.009970638030743013\n",
      "Test loss: 0.04612261801958084\n",
      "Starting epoch 2944\n",
      "Training loss: 0.009999144837626667\n",
      "Test loss: 0.04634238476002658\n",
      "Starting epoch 2945\n",
      "Training loss: 0.009861253476778015\n",
      "Test loss: 0.045702891424298286\n",
      "Starting epoch 2946\n",
      "Training loss: 0.009940263341929092\n",
      "Test loss: 0.045586685822517785\n",
      "Starting epoch 2947\n",
      "Training loss: 0.00990357975185406\n",
      "Test loss: 0.04593831797440847\n",
      "Starting epoch 2948\n",
      "Training loss: 0.009963851238860458\n",
      "Test loss: 0.04624941558749587\n",
      "Starting epoch 2949\n",
      "Training loss: 0.010168516901550724\n",
      "Test loss: 0.045802856554035785\n",
      "Starting epoch 2950\n",
      "Training loss: 0.00988569345752724\n",
      "Test loss: 0.046096873504144174\n",
      "Starting epoch 2951\n",
      "Training loss: 0.009909995251381006\n",
      "Test loss: 0.045860694997288565\n",
      "Starting epoch 2952\n",
      "Training loss: 0.01001269637500165\n",
      "Test loss: 0.04592790647789284\n",
      "Starting epoch 2953\n",
      "Training loss: 0.010215453986750275\n",
      "Test loss: 0.046068219054076404\n",
      "Starting epoch 2954\n",
      "Training loss: 0.009995540756671155\n",
      "Test loss: 0.0456023725370566\n",
      "Starting epoch 2955\n",
      "Training loss: 0.010281869439316577\n",
      "Test loss: 0.04622798688985683\n",
      "Starting epoch 2956\n",
      "Training loss: 0.009961030369655031\n",
      "Test loss: 0.04668065836584127\n",
      "Starting epoch 2957\n",
      "Training loss: 0.010130653066224739\n",
      "Test loss: 0.046293767721012784\n",
      "Starting epoch 2958\n",
      "Training loss: 0.010106540468261867\n",
      "Test loss: 0.04556748381367436\n",
      "Starting epoch 2959\n",
      "Training loss: 0.010016140863910073\n",
      "Test loss: 0.04632032414277395\n",
      "Starting epoch 2960\n",
      "Training loss: 0.009893055226592744\n",
      "Test loss: 0.04663304887987949\n",
      "Starting epoch 2961\n",
      "Training loss: 0.010129926100251128\n",
      "Test loss: 0.046236883021063276\n",
      "Starting epoch 2962\n",
      "Training loss: 0.010027656087377032\n",
      "Test loss: 0.04556281271356123\n",
      "Starting epoch 2963\n",
      "Training loss: 0.009930138262446786\n",
      "Test loss: 0.0460983340938886\n",
      "Starting epoch 2964\n",
      "Training loss: 0.009927674761561096\n",
      "Test loss: 0.04609343161185583\n",
      "Starting epoch 2965\n",
      "Training loss: 0.009912620313832016\n",
      "Test loss: 0.046065389144199866\n",
      "Starting epoch 2966\n",
      "Training loss: 0.009872298733499206\n",
      "Test loss: 0.04588148191019341\n",
      "Starting epoch 2967\n",
      "Training loss: 0.010005995814428955\n",
      "Test loss: 0.04589472573112558\n",
      "Starting epoch 2968\n",
      "Training loss: 0.009864951331229483\n",
      "Test loss: 0.04622175185768693\n",
      "Starting epoch 2969\n",
      "Training loss: 0.009981336484312034\n",
      "Test loss: 0.046247138607281225\n",
      "Starting epoch 2970\n",
      "Training loss: 0.00996100691864725\n",
      "Test loss: 0.04569076650120594\n",
      "Starting epoch 2971\n",
      "Training loss: 0.009952476950453931\n",
      "Test loss: 0.04547273367643356\n",
      "Starting epoch 2972\n",
      "Training loss: 0.009909525063255282\n",
      "Test loss: 0.04611257822425277\n",
      "Starting epoch 2973\n",
      "Training loss: 0.009879053158105397\n",
      "Test loss: 0.045904680672619075\n",
      "Starting epoch 2974\n",
      "Training loss: 0.009909376257755717\n",
      "Test loss: 0.04595278369055854\n",
      "Starting epoch 2975\n",
      "Training loss: 0.010163815478320981\n",
      "Test loss: 0.046039540182661126\n",
      "Starting epoch 2976\n",
      "Training loss: 0.009878353643246362\n",
      "Test loss: 0.04653723068811275\n",
      "Starting epoch 2977\n",
      "Training loss: 0.009924135155609397\n",
      "Test loss: 0.04622890082774339\n",
      "Starting epoch 2978\n",
      "Training loss: 0.00992008744448912\n",
      "Test loss: 0.04617183338160868\n",
      "Starting epoch 2979\n",
      "Training loss: 0.00985757803513867\n",
      "Test loss: 0.0462211130393876\n",
      "Starting epoch 2980\n",
      "Training loss: 0.009928475103539522\n",
      "Test loss: 0.04605317377933749\n",
      "Starting epoch 2981\n",
      "Training loss: 0.010065775792129705\n",
      "Test loss: 0.045904143127026384\n",
      "Starting epoch 2982\n",
      "Training loss: 0.009898887519709399\n",
      "Test loss: 0.0465849512429149\n",
      "Starting epoch 2983\n",
      "Training loss: 0.009882235655286273\n",
      "Test loss: 0.04646638725642805\n",
      "Starting epoch 2984\n",
      "Training loss: 0.009882564015197949\n",
      "Test loss: 0.046111079415789354\n",
      "Starting epoch 2985\n",
      "Training loss: 0.009957036659976498\n",
      "Test loss: 0.04611119765926291\n",
      "Starting epoch 2986\n",
      "Training loss: 0.010010617739353024\n",
      "Test loss: 0.04560584581836506\n",
      "Starting epoch 2987\n",
      "Training loss: 0.009923835636162366\n",
      "Test loss: 0.045889399117893644\n",
      "Starting epoch 2988\n",
      "Training loss: 0.009898281411924323\n",
      "Test loss: 0.045921817145965715\n",
      "Starting epoch 2989\n",
      "Training loss: 0.009939371066198487\n",
      "Test loss: 0.04598325256396223\n",
      "Starting epoch 2990\n",
      "Training loss: 0.009889633486383274\n",
      "Test loss: 0.04599580085939831\n",
      "Starting epoch 2991\n",
      "Training loss: 0.009867218368854678\n",
      "Test loss: 0.04609666157651831\n",
      "Starting epoch 2992\n",
      "Training loss: 0.009969124387277931\n",
      "Test loss: 0.045959827317683784\n",
      "Starting epoch 2993\n",
      "Training loss: 0.009919638821824651\n",
      "Test loss: 0.04625373913182153\n",
      "Starting epoch 2994\n",
      "Training loss: 0.009864672743639008\n",
      "Test loss: 0.04639716515386546\n",
      "Starting epoch 2995\n",
      "Training loss: 0.010050330570608865\n",
      "Test loss: 0.04628558153355563\n",
      "Starting epoch 2996\n",
      "Training loss: 0.009883384601991684\n",
      "Test loss: 0.04564842636938448\n",
      "Starting epoch 2997\n",
      "Training loss: 0.009948375741722153\n",
      "Test loss: 0.04562185456355413\n",
      "Starting epoch 2998\n",
      "Training loss: 0.009933405906939115\n",
      "Test loss: 0.04614797455293161\n",
      "Starting epoch 2999\n",
      "Training loss: 0.009957375798801908\n",
      "Test loss: 0.04581018551080315\n",
      "Starting epoch 3000\n",
      "Training loss: 0.009864452875173483\n",
      "Test loss: 0.04624898635126926\n",
      "Starting epoch 3001\n",
      "Training loss: 0.01020316104786318\n",
      "Test loss: 0.046175054791900844\n",
      "Starting epoch 3002\n",
      "Training loss: 0.009903388845993847\n",
      "Test loss: 0.046725822543656384\n",
      "Starting epoch 3003\n",
      "Training loss: 0.00993794291356548\n",
      "Test loss: 0.046241062934751866\n",
      "Starting epoch 3004\n",
      "Training loss: 0.010106009614394337\n",
      "Test loss: 0.04592282600976803\n",
      "Starting epoch 3005\n",
      "Training loss: 0.010024832531076963\n",
      "Test loss: 0.04549363848787767\n",
      "Starting epoch 3006\n",
      "Training loss: 0.009854283031137263\n",
      "Test loss: 0.04598609385667024\n",
      "Starting epoch 3007\n",
      "Training loss: 0.009945752694592124\n",
      "Test loss: 0.04603722139641091\n",
      "Starting epoch 3008\n",
      "Training loss: 0.009877358654849842\n",
      "Test loss: 0.04574049153813609\n",
      "Starting epoch 3009\n",
      "Training loss: 0.009897741733393708\n",
      "Test loss: 0.045811344352033406\n",
      "Starting epoch 3010\n",
      "Training loss: 0.009977279093544015\n",
      "Test loss: 0.0460527942136482\n",
      "Starting epoch 3011\n",
      "Training loss: 0.010004850196056679\n",
      "Test loss: 0.04611101415422228\n",
      "Starting epoch 3012\n",
      "Training loss: 0.010120637073624329\n",
      "Test loss: 0.045840026879752124\n",
      "Starting epoch 3013\n",
      "Training loss: 0.0100802801793716\n",
      "Test loss: 0.046227988269593984\n",
      "Starting epoch 3014\n",
      "Training loss: 0.009953377600453917\n",
      "Test loss: 0.04567843979155576\n",
      "Starting epoch 3015\n",
      "Training loss: 0.009885496139282086\n",
      "Test loss: 0.045733044406882035\n",
      "Starting epoch 3016\n",
      "Training loss: 0.009929040309469232\n",
      "Test loss: 0.045747931908678124\n",
      "Starting epoch 3017\n",
      "Training loss: 0.00990069507941848\n",
      "Test loss: 0.045789980640014015\n",
      "Starting epoch 3018\n",
      "Training loss: 0.009935332416388833\n",
      "Test loss: 0.04602487019642636\n",
      "Starting epoch 3019\n",
      "Training loss: 0.009859325913865059\n",
      "Test loss: 0.04627343805299865\n",
      "Starting epoch 3020\n",
      "Training loss: 0.01003976819700882\n",
      "Test loss: 0.04609340001587515\n",
      "Starting epoch 3021\n",
      "Training loss: 0.00990771193851213\n",
      "Test loss: 0.045666614892306154\n",
      "Starting epoch 3022\n",
      "Training loss: 0.009929289522229648\n",
      "Test loss: 0.04569744222142078\n",
      "Starting epoch 3023\n",
      "Training loss: 0.009961417250213076\n",
      "Test loss: 0.04603427303610025\n",
      "Starting epoch 3024\n",
      "Training loss: 0.009997144982707306\n",
      "Test loss: 0.04583278588122792\n",
      "Starting epoch 3025\n",
      "Training loss: 0.009927802902386814\n",
      "Test loss: 0.04608541520105468\n",
      "Starting epoch 3026\n",
      "Training loss: 0.009988607441792722\n",
      "Test loss: 0.04633876198419818\n",
      "Starting epoch 3027\n",
      "Training loss: 0.009984406108250384\n",
      "Test loss: 0.045617746810118355\n",
      "Starting epoch 3028\n",
      "Training loss: 0.009861689900643513\n",
      "Test loss: 0.0460374541580677\n",
      "Starting epoch 3029\n",
      "Training loss: 0.009889052982335209\n",
      "Test loss: 0.04600342294132268\n",
      "Starting epoch 3030\n",
      "Training loss: 0.00995598850985531\n",
      "Test loss: 0.045918027145995036\n",
      "Starting epoch 3031\n",
      "Training loss: 0.010028395297952363\n",
      "Test loss: 0.04611769290985884\n",
      "Starting epoch 3032\n",
      "Training loss: 0.009922479783169558\n",
      "Test loss: 0.04566713739876394\n",
      "Starting epoch 3033\n",
      "Training loss: 0.009961621499940997\n",
      "Test loss: 0.04567232645220227\n",
      "Starting epoch 3034\n",
      "Training loss: 0.010078392480118353\n",
      "Test loss: 0.04603126645088196\n",
      "Starting epoch 3035\n",
      "Training loss: 0.009936437422989821\n",
      "Test loss: 0.045792929414245814\n",
      "Starting epoch 3036\n",
      "Training loss: 0.009984172407354488\n",
      "Test loss: 0.045744900281230606\n",
      "Starting epoch 3037\n",
      "Training loss: 0.009935117265606513\n",
      "Test loss: 0.046044241085096645\n",
      "Starting epoch 3038\n",
      "Training loss: 0.010010602563375333\n",
      "Test loss: 0.04602494891043062\n",
      "Starting epoch 3039\n",
      "Training loss: 0.00984984882114852\n",
      "Test loss: 0.045555434845111986\n",
      "Starting epoch 3040\n",
      "Training loss: 0.010301453702640339\n",
      "Test loss: 0.045918076678558635\n",
      "Starting epoch 3041\n",
      "Training loss: 0.009869312684311241\n",
      "Test loss: 0.046617505726990874\n",
      "Starting epoch 3042\n",
      "Training loss: 0.009916466889811343\n",
      "Test loss: 0.046396487289004855\n",
      "Starting epoch 3043\n",
      "Training loss: 0.00999550493892099\n",
      "Test loss: 0.04590603005554941\n",
      "Starting epoch 3044\n",
      "Training loss: 0.010136579010696685\n",
      "Test loss: 0.04603081845023014\n",
      "Starting epoch 3045\n",
      "Training loss: 0.009888825067853341\n",
      "Test loss: 0.04548471324421741\n",
      "Starting epoch 3046\n",
      "Training loss: 0.010318570800858443\n",
      "Test loss: 0.04579779878258705\n",
      "Starting epoch 3047\n",
      "Training loss: 0.010148299949579552\n",
      "Test loss: 0.04678457299316371\n",
      "Starting epoch 3048\n",
      "Training loss: 0.00997719222099566\n",
      "Test loss: 0.04595692828297615\n",
      "Starting epoch 3049\n",
      "Training loss: 0.009915946524773464\n",
      "Test loss: 0.046087402229507766\n",
      "Starting epoch 3050\n",
      "Training loss: 0.009861393404177955\n",
      "Test loss: 0.04627047472253994\n",
      "Starting epoch 3051\n",
      "Training loss: 0.010055472997979063\n",
      "Test loss: 0.04597917447487513\n",
      "Starting epoch 3052\n",
      "Training loss: 0.00991092608539296\n",
      "Test loss: 0.04569251380032963\n",
      "Starting epoch 3053\n",
      "Training loss: 0.009878357147156704\n",
      "Test loss: 0.04566406851841344\n",
      "Starting epoch 3054\n",
      "Training loss: 0.009885731153190136\n",
      "Test loss: 0.04595186795901369\n",
      "Starting epoch 3055\n",
      "Training loss: 0.009923810750001767\n",
      "Test loss: 0.045880174471272364\n",
      "Starting epoch 3056\n",
      "Training loss: 0.009955708746660928\n",
      "Test loss: 0.04617891736604549\n",
      "Starting epoch 3057\n",
      "Training loss: 0.009972066213911186\n",
      "Test loss: 0.04652241369088491\n",
      "Starting epoch 3058\n",
      "Training loss: 0.009851686015236573\n",
      "Test loss: 0.04577312991023064\n",
      "Starting epoch 3059\n",
      "Training loss: 0.009874013252556324\n",
      "Test loss: 0.04568682873138675\n",
      "Starting epoch 3060\n",
      "Training loss: 0.010042712199272679\n",
      "Test loss: 0.04576809124814139\n",
      "Starting epoch 3061\n",
      "Training loss: 0.010061760967383619\n",
      "Test loss: 0.04649344417783949\n",
      "Starting epoch 3062\n",
      "Training loss: 0.009906422606379282\n",
      "Test loss: 0.04593366660453655\n",
      "Starting epoch 3063\n",
      "Training loss: 0.00994683571587332\n",
      "Test loss: 0.04583826274783523\n",
      "Starting epoch 3064\n",
      "Training loss: 0.00996383520789811\n",
      "Test loss: 0.04576792664549969\n",
      "Starting epoch 3065\n",
      "Training loss: 0.009866361750564615\n",
      "Test loss: 0.04612883207974611\n",
      "Starting epoch 3066\n",
      "Training loss: 0.009911221467324944\n",
      "Test loss: 0.04612193670537737\n",
      "Starting epoch 3067\n",
      "Training loss: 0.01003151084678095\n",
      "Test loss: 0.04583703581657675\n",
      "Starting epoch 3068\n",
      "Training loss: 0.009926068695781172\n",
      "Test loss: 0.0456563602719042\n",
      "Starting epoch 3069\n",
      "Training loss: 0.010013618369082936\n",
      "Test loss: 0.04559513498787527\n",
      "Starting epoch 3070\n",
      "Training loss: 0.010000315510102959\n",
      "Test loss: 0.04546400773580427\n",
      "Starting epoch 3071\n",
      "Training loss: 0.009969996059405023\n",
      "Test loss: 0.04586231611945011\n",
      "Starting epoch 3072\n",
      "Training loss: 0.01001157188696451\n",
      "Test loss: 0.0461870652657968\n",
      "Starting epoch 3073\n",
      "Training loss: 0.009944663718953485\n",
      "Test loss: 0.04560403981142574\n",
      "Starting epoch 3074\n",
      "Training loss: 0.010069129286242312\n",
      "Test loss: 0.04569585938696508\n",
      "Starting epoch 3075\n",
      "Training loss: 0.009932357787353093\n",
      "Test loss: 0.04564201148847739\n",
      "Starting epoch 3076\n",
      "Training loss: 0.010169665878791301\n",
      "Test loss: 0.04575448400444455\n",
      "Starting epoch 3077\n",
      "Training loss: 0.010536025156129579\n",
      "Test loss: 0.045521245475996425\n",
      "Starting epoch 3078\n",
      "Training loss: 0.009886007313234885\n",
      "Test loss: 0.046488355155344364\n",
      "Starting epoch 3079\n",
      "Training loss: 0.010023398385917554\n",
      "Test loss: 0.046550586267753886\n",
      "Starting epoch 3080\n",
      "Training loss: 0.01007966587289435\n",
      "Test loss: 0.04602297188507186\n",
      "Starting epoch 3081\n",
      "Training loss: 0.010129554975838935\n",
      "Test loss: 0.04656876747806867\n",
      "Starting epoch 3082\n",
      "Training loss: 0.01006385039721356\n",
      "Test loss: 0.04669772571435681\n",
      "Starting epoch 3083\n",
      "Training loss: 0.00994332740846716\n",
      "Test loss: 0.04603865108004323\n",
      "Starting epoch 3084\n",
      "Training loss: 0.009931453350992476\n",
      "Test loss: 0.04594123722226531\n",
      "Starting epoch 3085\n",
      "Training loss: 0.009896419774435583\n",
      "Test loss: 0.04595627828880593\n",
      "Starting epoch 3086\n",
      "Training loss: 0.009895777665689344\n",
      "Test loss: 0.046178501651242924\n",
      "Starting epoch 3087\n",
      "Training loss: 0.009977079103471802\n",
      "Test loss: 0.045899606275337713\n",
      "Starting epoch 3088\n",
      "Training loss: 0.009867217620743102\n",
      "Test loss: 0.04615559898040913\n",
      "Starting epoch 3089\n",
      "Training loss: 0.010035114301765551\n",
      "Test loss: 0.04584153175905899\n",
      "Starting epoch 3090\n",
      "Training loss: 0.009943058118834848\n",
      "Test loss: 0.0463080192329707\n",
      "Starting epoch 3091\n",
      "Training loss: 0.009901385708544099\n",
      "Test loss: 0.046374421980645925\n",
      "Starting epoch 3092\n",
      "Training loss: 0.01016993199276631\n",
      "Test loss: 0.04613929338477276\n",
      "Starting epoch 3093\n",
      "Training loss: 0.009978790759857073\n",
      "Test loss: 0.04663903887073199\n",
      "Starting epoch 3094\n",
      "Training loss: 0.010191914396452123\n",
      "Test loss: 0.045969713341306756\n",
      "Starting epoch 3095\n",
      "Training loss: 0.010000549485815352\n",
      "Test loss: 0.046450782705236365\n",
      "Starting epoch 3096\n",
      "Training loss: 0.010040402710132424\n",
      "Test loss: 0.04604860215827271\n",
      "Starting epoch 3097\n",
      "Training loss: 0.009929737839542452\n",
      "Test loss: 0.04646232310268614\n",
      "Starting epoch 3098\n",
      "Training loss: 0.009924469790497764\n",
      "Test loss: 0.046512439364084494\n",
      "Starting epoch 3099\n",
      "Training loss: 0.009888310061737161\n",
      "Test loss: 0.04595101155616619\n",
      "Starting epoch 3100\n",
      "Training loss: 0.009908456645417408\n",
      "Test loss: 0.04572788694942439\n",
      "Starting epoch 3101\n",
      "Training loss: 0.009954233806519235\n",
      "Test loss: 0.046023881890707545\n",
      "Starting epoch 3102\n",
      "Training loss: 0.00998596097419008\n",
      "Test loss: 0.046124651476188945\n",
      "Starting epoch 3103\n",
      "Training loss: 0.009995218648834795\n",
      "Test loss: 0.04655455439179032\n",
      "Starting epoch 3104\n",
      "Training loss: 0.009950152628856604\n",
      "Test loss: 0.0460684260836354\n",
      "Starting epoch 3105\n",
      "Training loss: 0.009860091293077976\n",
      "Test loss: 0.04605198223833685\n",
      "Starting epoch 3106\n",
      "Training loss: 0.009875318707257021\n",
      "Test loss: 0.04588258180215403\n",
      "Starting epoch 3107\n",
      "Training loss: 0.010269314096477187\n",
      "Test loss: 0.04596302561737873\n",
      "Starting epoch 3108\n",
      "Training loss: 0.009975231222075517\n",
      "Test loss: 0.045315675575424125\n",
      "Starting epoch 3109\n",
      "Training loss: 0.009949926088457225\n",
      "Test loss: 0.04602389603301331\n",
      "Starting epoch 3110\n",
      "Training loss: 0.010103794456016823\n",
      "Test loss: 0.04664572714655488\n",
      "Starting epoch 3111\n",
      "Training loss: 0.009890032779486453\n",
      "Test loss: 0.04584060243710324\n",
      "Starting epoch 3112\n",
      "Training loss: 0.009865004752503067\n",
      "Test loss: 0.0456150621175766\n",
      "Starting epoch 3113\n",
      "Training loss: 0.009935587339225362\n",
      "Test loss: 0.04576096759626159\n",
      "Starting epoch 3114\n",
      "Training loss: 0.009934176553468236\n",
      "Test loss: 0.045888501598879146\n",
      "Starting epoch 3115\n",
      "Training loss: 0.009853829996141254\n",
      "Test loss: 0.046052275984375564\n",
      "Starting epoch 3116\n",
      "Training loss: 0.010258177417467852\n",
      "Test loss: 0.04620091313565219\n",
      "Starting epoch 3117\n",
      "Training loss: 0.009843178612531209\n",
      "Test loss: 0.04552660965257221\n",
      "Starting epoch 3118\n",
      "Training loss: 0.009954090588955118\n",
      "Test loss: 0.045705112594145315\n",
      "Starting epoch 3119\n",
      "Training loss: 0.009997641194428577\n",
      "Test loss: 0.04639370698067877\n",
      "Starting epoch 3120\n",
      "Training loss: 0.009906081360627393\n",
      "Test loss: 0.04593606541554133\n",
      "Starting epoch 3121\n",
      "Training loss: 0.010022806843406841\n",
      "Test loss: 0.04585676433311568\n",
      "Starting epoch 3122\n",
      "Training loss: 0.009968499027070452\n",
      "Test loss: 0.0453817075877278\n",
      "Starting epoch 3123\n",
      "Training loss: 0.00995534702707998\n",
      "Test loss: 0.045723578306259935\n",
      "Starting epoch 3124\n",
      "Training loss: 0.009999789557129633\n",
      "Test loss: 0.046061554440745604\n",
      "Starting epoch 3125\n",
      "Training loss: 0.00991853438012424\n",
      "Test loss: 0.04572172504332331\n",
      "Starting epoch 3126\n",
      "Training loss: 0.009884228441314619\n",
      "Test loss: 0.045824450682158825\n",
      "Starting epoch 3127\n",
      "Training loss: 0.009887641448466504\n",
      "Test loss: 0.04596611181343043\n",
      "Starting epoch 3128\n",
      "Training loss: 0.00992947487068958\n",
      "Test loss: 0.04582191217276785\n",
      "Starting epoch 3129\n",
      "Training loss: 0.010250115660248233\n",
      "Test loss: 0.0462090582759292\n",
      "Starting epoch 3130\n",
      "Training loss: 0.009894648353095914\n",
      "Test loss: 0.0467367181899371\n",
      "Starting epoch 3131\n",
      "Training loss: 0.010048442031638544\n",
      "Test loss: 0.046309329845287184\n",
      "Starting epoch 3132\n",
      "Training loss: 0.010189309792562586\n",
      "Test loss: 0.045581534090969295\n",
      "Starting epoch 3133\n",
      "Training loss: 0.01000306762938128\n",
      "Test loss: 0.044991500261757106\n",
      "Starting epoch 3134\n",
      "Training loss: 0.009943394585833197\n",
      "Test loss: 0.04580552972577236\n",
      "Starting epoch 3135\n",
      "Training loss: 0.00992250280668501\n",
      "Test loss: 0.0461837659003558\n",
      "Starting epoch 3136\n",
      "Training loss: 0.00999721152646864\n",
      "Test loss: 0.04640509429629202\n",
      "Starting epoch 3137\n",
      "Training loss: 0.009998125039407463\n",
      "Test loss: 0.045793200532595314\n",
      "Starting epoch 3138\n",
      "Training loss: 0.01001480273658135\n",
      "Test loss: 0.045919000964473794\n",
      "Starting epoch 3139\n",
      "Training loss: 0.009867303546701298\n",
      "Test loss: 0.04637951362464163\n",
      "Starting epoch 3140\n",
      "Training loss: 0.010211701291139985\n",
      "Test loss: 0.04614925074080626\n",
      "Starting epoch 3141\n",
      "Training loss: 0.009926584686656467\n",
      "Test loss: 0.04646104298256062\n",
      "Starting epoch 3142\n",
      "Training loss: 0.010068807124969412\n",
      "Test loss: 0.04641161893528921\n",
      "Starting epoch 3143\n",
      "Training loss: 0.009915182092150704\n",
      "Test loss: 0.046686412421641524\n",
      "Starting epoch 3144\n",
      "Training loss: 0.00988452605231375\n",
      "Test loss: 0.046114346840315394\n",
      "Starting epoch 3145\n",
      "Training loss: 0.01000567444707038\n",
      "Test loss: 0.04609066772240179\n",
      "Starting epoch 3146\n",
      "Training loss: 0.009884125721014913\n",
      "Test loss: 0.04640031937095854\n",
      "Starting epoch 3147\n",
      "Training loss: 0.010263497834322883\n",
      "Test loss: 0.046365111514374065\n",
      "Starting epoch 3148\n",
      "Training loss: 0.009900759279605795\n",
      "Test loss: 0.04681933864399239\n",
      "Starting epoch 3149\n",
      "Training loss: 0.009908688071443409\n",
      "Test loss: 0.04666173278733536\n",
      "Starting epoch 3150\n",
      "Training loss: 0.0098944308358382\n",
      "Test loss: 0.04621888076265653\n",
      "Starting epoch 3151\n",
      "Training loss: 0.00986236468201778\n",
      "Test loss: 0.04616625041321472\n",
      "Starting epoch 3152\n",
      "Training loss: 0.010033542939202219\n",
      "Test loss: 0.04625973960867635\n",
      "Starting epoch 3153\n",
      "Training loss: 0.009976673413251267\n",
      "Test loss: 0.045685815176478135\n",
      "Starting epoch 3154\n",
      "Training loss: 0.009866430088266974\n",
      "Test loss: 0.04611337957558809\n",
      "Starting epoch 3155\n",
      "Training loss: 0.00985891365094996\n",
      "Test loss: 0.04642743037806617\n",
      "Starting epoch 3156\n",
      "Training loss: 0.00998521187022084\n",
      "Test loss: 0.04629220385794287\n",
      "Starting epoch 3157\n",
      "Training loss: 0.009991712685002655\n",
      "Test loss: 0.04644804585863043\n",
      "Starting epoch 3158\n",
      "Training loss: 0.009847211330884793\n",
      "Test loss: 0.04669484027006008\n",
      "Starting epoch 3159\n",
      "Training loss: 0.00995461662590015\n",
      "Test loss: 0.04656764009484538\n",
      "Starting epoch 3160\n",
      "Training loss: 0.009919312263487792\n",
      "Test loss: 0.046645083361201815\n",
      "Starting epoch 3161\n",
      "Training loss: 0.009917599759751657\n",
      "Test loss: 0.04649349357242937\n",
      "Starting epoch 3162\n",
      "Training loss: 0.009957714021572323\n",
      "Test loss: 0.04597050903571977\n",
      "Starting epoch 3163\n",
      "Training loss: 0.009895594500493808\n",
      "Test loss: 0.046176043097619655\n",
      "Starting epoch 3164\n",
      "Training loss: 0.009950114505701378\n",
      "Test loss: 0.046161706938787746\n",
      "Starting epoch 3165\n",
      "Training loss: 0.009906278617802213\n",
      "Test loss: 0.04599745488829083\n",
      "Starting epoch 3166\n",
      "Training loss: 0.009876616833517786\n",
      "Test loss: 0.04578100862326445\n",
      "Starting epoch 3167\n",
      "Training loss: 0.009840312932969117\n",
      "Test loss: 0.04596759116760007\n",
      "Starting epoch 3168\n",
      "Training loss: 0.009930784111750907\n",
      "Test loss: 0.04597334701705862\n",
      "Starting epoch 3169\n",
      "Training loss: 0.00991646203471989\n",
      "Test loss: 0.04642574047600782\n",
      "Starting epoch 3170\n",
      "Training loss: 0.00990029545042847\n",
      "Test loss: 0.046040670670292994\n",
      "Starting epoch 3171\n",
      "Training loss: 0.00991476890554682\n",
      "Test loss: 0.04570050358220383\n",
      "Starting epoch 3172\n",
      "Training loss: 0.009929261658890326\n",
      "Test loss: 0.04611769235796399\n",
      "Starting epoch 3173\n",
      "Training loss: 0.009893519803881645\n",
      "Test loss: 0.046193774651598046\n",
      "Starting epoch 3174\n",
      "Training loss: 0.010208654629646754\n",
      "Test loss: 0.0458809293254658\n",
      "Starting epoch 3175\n",
      "Training loss: 0.010129389307293736\n",
      "Test loss: 0.04528560503213494\n",
      "Starting epoch 3176\n",
      "Training loss: 0.009942006777788772\n",
      "Test loss: 0.046047393370557715\n",
      "Starting epoch 3177\n",
      "Training loss: 0.009879412007380704\n",
      "Test loss: 0.046007622171331336\n",
      "Starting epoch 3178\n",
      "Training loss: 0.009859281683676556\n",
      "Test loss: 0.04604790028598574\n",
      "Starting epoch 3179\n",
      "Training loss: 0.0100144392917635\n",
      "Test loss: 0.04606811412506633\n",
      "Starting epoch 3180\n",
      "Training loss: 0.00993529780477774\n",
      "Test loss: 0.04639131499937287\n",
      "Starting epoch 3181\n",
      "Training loss: 0.00989883790006403\n",
      "Test loss: 0.04643645413495876\n",
      "Starting epoch 3182\n",
      "Training loss: 0.009876194883321152\n",
      "Test loss: 0.04641906637698412\n",
      "Starting epoch 3183\n",
      "Training loss: 0.010045280069357058\n",
      "Test loss: 0.046516614931601065\n",
      "Starting epoch 3184\n",
      "Training loss: 0.009858036902351458\n",
      "Test loss: 0.04660800427060436\n",
      "Starting epoch 3185\n",
      "Training loss: 0.009988353954109012\n",
      "Test loss: 0.04628764341274897\n",
      "Starting epoch 3186\n",
      "Training loss: 0.00984490052109859\n",
      "Test loss: 0.045808351012291734\n",
      "Starting epoch 3187\n",
      "Training loss: 0.009912698941885447\n",
      "Test loss: 0.045916737919604336\n",
      "Starting epoch 3188\n",
      "Training loss: 0.009924174484903694\n",
      "Test loss: 0.04586221608850691\n",
      "Starting epoch 3189\n",
      "Training loss: 0.00991588821787326\n",
      "Test loss: 0.04570103892021709\n",
      "Starting epoch 3190\n",
      "Training loss: 0.009877961541174865\n",
      "Test loss: 0.04573892177668987\n",
      "Starting epoch 3191\n",
      "Training loss: 0.00985952361379979\n",
      "Test loss: 0.04596634264345522\n",
      "Starting epoch 3192\n",
      "Training loss: 0.009859715618925994\n",
      "Test loss: 0.04589919469974659\n",
      "Starting epoch 3193\n",
      "Training loss: 0.009892107384493117\n",
      "Test loss: 0.045823537917048844\n",
      "Starting epoch 3194\n",
      "Training loss: 0.00996165898185773\n",
      "Test loss: 0.04602824165313332\n",
      "Starting epoch 3195\n",
      "Training loss: 0.0100056562633788\n",
      "Test loss: 0.046337942558306235\n",
      "Starting epoch 3196\n",
      "Training loss: 0.009874252083360171\n",
      "Test loss: 0.04579725295857147\n",
      "Starting epoch 3197\n",
      "Training loss: 0.009882749378925464\n",
      "Test loss: 0.04585453260827948\n",
      "Starting epoch 3198\n",
      "Training loss: 0.009975363668359693\n",
      "Test loss: 0.04618317495893549\n",
      "Starting epoch 3199\n",
      "Training loss: 0.00984760473070086\n",
      "Test loss: 0.045895629872878395\n",
      "Starting epoch 3200\n",
      "Training loss: 0.009885510490810285\n",
      "Test loss: 0.04600380574939428\n",
      "Starting epoch 3201\n",
      "Training loss: 0.00987480393015459\n",
      "Test loss: 0.04627297321955363\n",
      "Starting epoch 3202\n",
      "Training loss: 0.009862127637521166\n",
      "Test loss: 0.046248742275767855\n",
      "Starting epoch 3203\n",
      "Training loss: 0.00990545894706347\n",
      "Test loss: 0.04616386843500314\n",
      "Starting epoch 3204\n",
      "Training loss: 0.009897775322076727\n",
      "Test loss: 0.046062616148480666\n",
      "Starting epoch 3205\n",
      "Training loss: 0.00992335811196292\n",
      "Test loss: 0.04601746783764274\n",
      "Starting epoch 3206\n",
      "Training loss: 0.009866069987049846\n",
      "Test loss: 0.046329011105828814\n",
      "Starting epoch 3207\n",
      "Training loss: 0.009929796131175072\n",
      "Test loss: 0.046263322786048604\n",
      "Starting epoch 3208\n",
      "Training loss: 0.009890184004897953\n",
      "Test loss: 0.04626692058863463\n",
      "Starting epoch 3209\n",
      "Training loss: 0.009860051612629265\n",
      "Test loss: 0.04632266017573851\n",
      "Starting epoch 3210\n",
      "Training loss: 0.010131790287426262\n",
      "Test loss: 0.04613900060455004\n",
      "Starting epoch 3211\n",
      "Training loss: 0.009855945228186787\n",
      "Test loss: 0.046591712092911755\n",
      "Starting epoch 3212\n",
      "Training loss: 0.00992770071645252\n",
      "Test loss: 0.046672905760782736\n",
      "Starting epoch 3213\n",
      "Training loss: 0.009870144286666249\n",
      "Test loss: 0.04653873694715677\n",
      "Starting epoch 3214\n",
      "Training loss: 0.010173460468649864\n",
      "Test loss: 0.04621957470145491\n",
      "Starting epoch 3215\n",
      "Training loss: 0.009878267182922755\n",
      "Test loss: 0.046737014695450114\n",
      "Starting epoch 3216\n",
      "Training loss: 0.009937816467441496\n",
      "Test loss: 0.046408249962109106\n",
      "Starting epoch 3217\n",
      "Training loss: 0.00993750835234513\n",
      "Test loss: 0.046343786710942234\n",
      "Starting epoch 3218\n",
      "Training loss: 0.009856691263371804\n",
      "Test loss: 0.0458692334316395\n",
      "Starting epoch 3219\n",
      "Training loss: 0.010017541053964466\n",
      "Test loss: 0.04595152923354396\n",
      "Starting epoch 3220\n",
      "Training loss: 0.009854202448833184\n",
      "Test loss: 0.04578882786962721\n",
      "Starting epoch 3221\n",
      "Training loss: 0.009943356080988392\n",
      "Test loss: 0.04578331623364378\n",
      "Starting epoch 3222\n",
      "Training loss: 0.009849994977722402\n",
      "Test loss: 0.046332516328052235\n",
      "Starting epoch 3223\n",
      "Training loss: 0.009944653688151329\n",
      "Test loss: 0.04631720717858385\n",
      "Starting epoch 3224\n",
      "Training loss: 0.009886719698666549\n",
      "Test loss: 0.045889022311678636\n",
      "Starting epoch 3225\n",
      "Training loss: 0.010006966863255033\n",
      "Test loss: 0.04617081320396176\n",
      "Starting epoch 3226\n",
      "Training loss: 0.010002478606021796\n",
      "Test loss: 0.04673470943062394\n",
      "Starting epoch 3227\n",
      "Training loss: 0.009876047421368907\n",
      "Test loss: 0.04678599094903028\n",
      "Starting epoch 3228\n",
      "Training loss: 0.009920865984358748\n",
      "Test loss: 0.04641987548934089\n",
      "Starting epoch 3229\n",
      "Training loss: 0.010009450105125786\n",
      "Test loss: 0.04658192037432282\n",
      "Starting epoch 3230\n",
      "Training loss: 0.009995088530857055\n",
      "Test loss: 0.046561712192164526\n",
      "Starting epoch 3231\n",
      "Training loss: 0.009881322470600487\n",
      "Test loss: 0.045968133128351636\n",
      "Starting epoch 3232\n",
      "Training loss: 0.009914066421142856\n",
      "Test loss: 0.04617953196995788\n",
      "Starting epoch 3233\n",
      "Training loss: 0.009909886912610686\n",
      "Test loss: 0.046029079912437335\n",
      "Starting epoch 3234\n",
      "Training loss: 0.009979196671457564\n",
      "Test loss: 0.04571534099954146\n",
      "Starting epoch 3235\n",
      "Training loss: 0.009990104855816872\n",
      "Test loss: 0.0457230479353004\n",
      "Starting epoch 3236\n",
      "Training loss: 0.009890913047262879\n",
      "Test loss: 0.046521845239180105\n",
      "Starting epoch 3237\n",
      "Training loss: 0.009933995388326098\n",
      "Test loss: 0.04652194595999188\n",
      "Starting epoch 3238\n",
      "Training loss: 0.00988910687690387\n",
      "Test loss: 0.046461856061661685\n",
      "Starting epoch 3239\n",
      "Training loss: 0.009903155877941945\n",
      "Test loss: 0.04624644942857601\n",
      "Starting epoch 3240\n",
      "Training loss: 0.009850599054918915\n",
      "Test loss: 0.04624853007219456\n",
      "Starting epoch 3241\n",
      "Training loss: 0.00993584531557853\n",
      "Test loss: 0.04602200731083199\n",
      "Starting epoch 3242\n",
      "Training loss: 0.009841561485387262\n",
      "Test loss: 0.046069960213369794\n",
      "Starting epoch 3243\n",
      "Training loss: 0.009844249251802436\n",
      "Test loss: 0.04614951378769345\n",
      "Starting epoch 3244\n",
      "Training loss: 0.009851452344875843\n",
      "Test loss: 0.04611496144422778\n",
      "Starting epoch 3245\n",
      "Training loss: 0.01011939171213107\n",
      "Test loss: 0.045920326477951474\n",
      "Starting epoch 3246\n",
      "Training loss: 0.010070757406046156\n",
      "Test loss: 0.04652576162307351\n",
      "Starting epoch 3247\n",
      "Training loss: 0.009987578360882939\n",
      "Test loss: 0.04583277774077875\n",
      "Starting epoch 3248\n",
      "Training loss: 0.009853156252962644\n",
      "Test loss: 0.046040354641499345\n",
      "Starting epoch 3249\n",
      "Training loss: 0.01031400383923386\n",
      "Test loss: 0.04592058462677179\n",
      "Starting epoch 3250\n",
      "Training loss: 0.010498939998081474\n",
      "Test loss: 0.04543395823350659\n",
      "Starting epoch 3251\n",
      "Training loss: 0.009837811079914452\n",
      "Test loss: 0.046538556891459006\n",
      "Starting epoch 3252\n",
      "Training loss: 0.009954807043197702\n",
      "Test loss: 0.04632288390011699\n",
      "Starting epoch 3253\n",
      "Training loss: 0.009882070658514734\n",
      "Test loss: 0.045934824341977085\n",
      "Starting epoch 3254\n",
      "Training loss: 0.009869713626313404\n",
      "Test loss: 0.04574677872437018\n",
      "Starting epoch 3255\n",
      "Training loss: 0.009976845601054489\n",
      "Test loss: 0.046061699037198665\n",
      "Starting epoch 3256\n",
      "Training loss: 0.010024005959390616\n",
      "Test loss: 0.04588863281188188\n",
      "Starting epoch 3257\n",
      "Training loss: 0.009899513086029252\n",
      "Test loss: 0.04652633600764804\n",
      "Starting epoch 3258\n",
      "Training loss: 0.009860902856730048\n",
      "Test loss: 0.04638170299154741\n",
      "Starting epoch 3259\n",
      "Training loss: 0.009891108334919468\n",
      "Test loss: 0.04627990860629965\n",
      "Starting epoch 3260\n",
      "Training loss: 0.009913404914932172\n",
      "Test loss: 0.04631003454603531\n",
      "Starting epoch 3261\n",
      "Training loss: 0.009901192711024989\n",
      "Test loss: 0.04639770449311645\n",
      "Starting epoch 3262\n",
      "Training loss: 0.009909562751284389\n",
      "Test loss: 0.04634616068667836\n",
      "Starting epoch 3263\n",
      "Training loss: 0.010150648577169318\n",
      "Test loss: 0.04624897793487266\n",
      "Starting epoch 3264\n",
      "Training loss: 0.009838862146144031\n",
      "Test loss: 0.04678496166511818\n",
      "Starting epoch 3265\n",
      "Training loss: 0.009985867307567205\n",
      "Test loss: 0.046452156785461635\n",
      "Starting epoch 3266\n",
      "Training loss: 0.009954287517876899\n",
      "Test loss: 0.04596767374486835\n",
      "Starting epoch 3267\n",
      "Training loss: 0.009975007368770779\n",
      "Test loss: 0.04582644240171821\n",
      "Starting epoch 3268\n",
      "Training loss: 0.009876358551812953\n",
      "Test loss: 0.046120463560024895\n",
      "Starting epoch 3269\n",
      "Training loss: 0.009900415438364764\n",
      "Test loss: 0.0461500219448849\n",
      "Starting epoch 3270\n",
      "Training loss: 0.010127860930610876\n",
      "Test loss: 0.0462203903330697\n",
      "Starting epoch 3271\n",
      "Training loss: 0.009995489991957048\n",
      "Test loss: 0.046690189452083024\n",
      "Starting epoch 3272\n",
      "Training loss: 0.009824416936054582\n",
      "Test loss: 0.04605595512246644\n",
      "Starting epoch 3273\n",
      "Training loss: 0.009924828051971119\n",
      "Test loss: 0.045892613353552644\n",
      "Starting epoch 3274\n",
      "Training loss: 0.009876347940842637\n",
      "Test loss: 0.04603107121807558\n",
      "Starting epoch 3275\n",
      "Training loss: 0.009960514294808029\n",
      "Test loss: 0.046235769504198325\n",
      "Starting epoch 3276\n",
      "Training loss: 0.009977836589344213\n",
      "Test loss: 0.04591621927641056\n",
      "Starting epoch 3277\n",
      "Training loss: 0.009905002018833747\n",
      "Test loss: 0.0461256869689182\n",
      "Starting epoch 3278\n",
      "Training loss: 0.009934793867659374\n",
      "Test loss: 0.046166742151534115\n",
      "Starting epoch 3279\n",
      "Training loss: 0.010061229060052848\n",
      "Test loss: 0.04601087200420874\n",
      "Starting epoch 3280\n",
      "Training loss: 0.009841036860693674\n",
      "Test loss: 0.046335056976035786\n",
      "Starting epoch 3281\n",
      "Training loss: 0.009956819814492444\n",
      "Test loss: 0.04605797333297906\n",
      "Starting epoch 3282\n",
      "Training loss: 0.009886538747270576\n",
      "Test loss: 0.04628358091469164\n",
      "Starting epoch 3283\n",
      "Training loss: 0.010228923918893103\n",
      "Test loss: 0.04642208375864559\n",
      "Starting epoch 3284\n",
      "Training loss: 0.00992199219763279\n",
      "Test loss: 0.04676803187639625\n",
      "Starting epoch 3285\n",
      "Training loss: 0.010136859949494972\n",
      "Test loss: 0.04609221978871911\n",
      "Starting epoch 3286\n",
      "Training loss: 0.010095920154183615\n",
      "Test loss: 0.04551206280787786\n",
      "Starting epoch 3287\n",
      "Training loss: 0.01016715150631842\n",
      "Test loss: 0.04639729346942018\n",
      "Starting epoch 3288\n",
      "Training loss: 0.009885279660220028\n",
      "Test loss: 0.04588567486239804\n",
      "Starting epoch 3289\n",
      "Training loss: 0.009869093762435874\n",
      "Test loss: 0.046049147085459145\n",
      "Starting epoch 3290\n",
      "Training loss: 0.00991606164235072\n",
      "Test loss: 0.046101054935543624\n",
      "Starting epoch 3291\n",
      "Training loss: 0.010038918372793276\n",
      "Test loss: 0.04661199229734915\n",
      "Starting epoch 3292\n",
      "Training loss: 0.009874018336661527\n",
      "Test loss: 0.046022973540756434\n",
      "Starting epoch 3293\n",
      "Training loss: 0.01013257667483365\n",
      "Test loss: 0.046149603056686896\n",
      "Starting epoch 3294\n",
      "Training loss: 0.009863961564346414\n",
      "Test loss: 0.04566430010729366\n",
      "Starting epoch 3295\n",
      "Training loss: 0.010198133676999906\n",
      "Test loss: 0.046002014229695\n",
      "Starting epoch 3296\n",
      "Training loss: 0.009920076100674809\n",
      "Test loss: 0.046852849078951056\n",
      "Starting epoch 3297\n",
      "Training loss: 0.009859055723445337\n",
      "Test loss: 0.046281623412613514\n",
      "Starting epoch 3298\n",
      "Training loss: 0.009881522598080948\n",
      "Test loss: 0.046129555648399725\n",
      "Starting epoch 3299\n",
      "Training loss: 0.009916523150855401\n",
      "Test loss: 0.046169915684947264\n",
      "Starting epoch 3300\n",
      "Training loss: 0.009843868463010084\n",
      "Test loss: 0.04645618823943315\n",
      "Starting epoch 3301\n",
      "Training loss: 0.009875962861859407\n",
      "Test loss: 0.04649180408429216\n",
      "Starting epoch 3302\n",
      "Training loss: 0.009899134274388923\n",
      "Test loss: 0.0461928551947629\n",
      "Starting epoch 3303\n",
      "Training loss: 0.010253072746830886\n",
      "Test loss: 0.045926824350047996\n",
      "Starting epoch 3304\n",
      "Training loss: 0.009878492196563815\n",
      "Test loss: 0.04657016873911575\n",
      "Starting epoch 3305\n",
      "Training loss: 0.009934144354135286\n",
      "Test loss: 0.04651742663096498\n",
      "Starting epoch 3306\n",
      "Training loss: 0.009892766562397362\n",
      "Test loss: 0.046364087611436844\n",
      "Starting epoch 3307\n",
      "Training loss: 0.009861462993822137\n",
      "Test loss: 0.04618364186198623\n",
      "Starting epoch 3308\n",
      "Training loss: 0.00987354717904427\n",
      "Test loss: 0.04631020363282274\n",
      "Starting epoch 3309\n",
      "Training loss: 0.009877126160093019\n",
      "Test loss: 0.046216193586587906\n",
      "Starting epoch 3310\n",
      "Training loss: 0.009850585191953377\n",
      "Test loss: 0.04601486330782926\n",
      "Starting epoch 3311\n",
      "Training loss: 0.00989289920716012\n",
      "Test loss: 0.046033460922815184\n",
      "Starting epoch 3312\n",
      "Training loss: 0.009863391457522502\n",
      "Test loss: 0.046200671819625075\n",
      "Starting epoch 3313\n",
      "Training loss: 0.009858769273049519\n",
      "Test loss: 0.04603474256065157\n",
      "Starting epoch 3314\n",
      "Training loss: 0.010040760177691451\n",
      "Test loss: 0.04606513320295899\n",
      "Starting epoch 3315\n",
      "Training loss: 0.009856310352438787\n",
      "Test loss: 0.04556448109172009\n",
      "Starting epoch 3316\n",
      "Training loss: 0.009868836412053616\n",
      "Test loss: 0.0455414037461634\n",
      "Starting epoch 3317\n",
      "Training loss: 0.010039745952140113\n",
      "Test loss: 0.045817179398404226\n",
      "Starting epoch 3318\n",
      "Training loss: 0.009991918324080647\n",
      "Test loss: 0.045754089261646626\n",
      "Starting epoch 3319\n",
      "Training loss: 0.009930898389611089\n",
      "Test loss: 0.04554994459505434\n",
      "Starting epoch 3320\n",
      "Training loss: 0.009920812242465918\n",
      "Test loss: 0.04570146926023342\n",
      "Starting epoch 3321\n",
      "Training loss: 0.009913783062432633\n",
      "Test loss: 0.046320456321592686\n",
      "Starting epoch 3322\n",
      "Training loss: 0.010015255794478733\n",
      "Test loss: 0.04612336500927254\n",
      "Starting epoch 3323\n",
      "Training loss: 0.009905925829757432\n",
      "Test loss: 0.045504296819368996\n",
      "Starting epoch 3324\n",
      "Training loss: 0.009898859732707993\n",
      "Test loss: 0.04553349454093863\n",
      "Starting epoch 3325\n",
      "Training loss: 0.009926566396091805\n",
      "Test loss: 0.04598077221049203\n",
      "Starting epoch 3326\n",
      "Training loss: 0.010066583981645888\n",
      "Test loss: 0.04658333184542479\n",
      "Starting epoch 3327\n",
      "Training loss: 0.009912772119411679\n",
      "Test loss: 0.0460395212802622\n",
      "Starting epoch 3328\n",
      "Training loss: 0.009938284937964111\n",
      "Test loss: 0.045880339211887784\n",
      "Starting epoch 3329\n",
      "Training loss: 0.009883412419528257\n",
      "Test loss: 0.04617496345330168\n",
      "Starting epoch 3330\n",
      "Training loss: 0.01020528554733171\n",
      "Test loss: 0.046070015954750555\n",
      "Starting epoch 3331\n",
      "Training loss: 0.009859204528944903\n",
      "Test loss: 0.045657385899512855\n",
      "Starting epoch 3332\n",
      "Training loss: 0.009899441488697881\n",
      "Test loss: 0.04584608254609285\n",
      "Starting epoch 3333\n",
      "Training loss: 0.00988356599614757\n",
      "Test loss: 0.046308083528721775\n",
      "Starting epoch 3334\n",
      "Training loss: 0.00987555988926868\n",
      "Test loss: 0.04626924944696603\n",
      "Starting epoch 3335\n",
      "Training loss: 0.010016967053906839\n",
      "Test loss: 0.04633823796002953\n",
      "Starting epoch 3336\n",
      "Training loss: 0.009850875665357367\n",
      "Test loss: 0.04672311356774083\n",
      "Starting epoch 3337\n",
      "Training loss: 0.009944654405727739\n",
      "Test loss: 0.04648097425147339\n",
      "Starting epoch 3338\n",
      "Training loss: 0.010014224812754841\n",
      "Test loss: 0.04648823691187082\n",
      "Starting epoch 3339\n",
      "Training loss: 0.009838579192024762\n",
      "Test loss: 0.04683308365444342\n",
      "Starting epoch 3340\n",
      "Training loss: 0.009887998267153248\n",
      "Test loss: 0.046490416758590274\n",
      "Starting epoch 3341\n",
      "Training loss: 0.009881211154651447\n",
      "Test loss: 0.046498708288978646\n",
      "Starting epoch 3342\n",
      "Training loss: 0.010014395275321162\n",
      "Test loss: 0.04632484429964313\n",
      "Starting epoch 3343\n",
      "Training loss: 0.009857504613331108\n",
      "Test loss: 0.046631322552760444\n",
      "Starting epoch 3344\n",
      "Training loss: 0.009834840420450344\n",
      "Test loss: 0.046262690176566444\n",
      "Starting epoch 3345\n",
      "Training loss: 0.009850844221769786\n",
      "Test loss: 0.0460125264470224\n",
      "Starting epoch 3346\n",
      "Training loss: 0.009853864279499308\n",
      "Test loss: 0.04616569092980138\n",
      "Starting epoch 3347\n",
      "Training loss: 0.009951785893835982\n",
      "Test loss: 0.04594563927363466\n",
      "Starting epoch 3348\n",
      "Training loss: 0.009867302539040808\n",
      "Test loss: 0.04628448298683873\n",
      "Starting epoch 3349\n",
      "Training loss: 0.0099028187697051\n",
      "Test loss: 0.04610876304407915\n",
      "Starting epoch 3350\n",
      "Training loss: 0.009894485814405269\n",
      "Test loss: 0.04604244825464708\n",
      "Starting epoch 3351\n",
      "Training loss: 0.009994436147027328\n",
      "Test loss: 0.04629499989527243\n",
      "Starting epoch 3352\n",
      "Training loss: 0.009965262482645081\n",
      "Test loss: 0.046671989339369314\n",
      "Starting epoch 3353\n",
      "Training loss: 0.00999374585378854\n",
      "Test loss: 0.04628091360683794\n",
      "Starting epoch 3354\n",
      "Training loss: 0.009962709620594978\n",
      "Test loss: 0.04648243194376981\n",
      "Starting epoch 3355\n",
      "Training loss: 0.009963392394549046\n",
      "Test loss: 0.04652439271686254\n",
      "Starting epoch 3356\n",
      "Training loss: 0.00983653627488701\n",
      "Test loss: 0.04625153500172827\n",
      "Starting epoch 3357\n",
      "Training loss: 0.010086092914714188\n",
      "Test loss: 0.0460527867630676\n",
      "Starting epoch 3358\n",
      "Training loss: 0.009853016966801197\n",
      "Test loss: 0.04660752332872815\n",
      "Starting epoch 3359\n",
      "Training loss: 0.009910159820660215\n",
      "Test loss: 0.046607719734311104\n",
      "Starting epoch 3360\n",
      "Training loss: 0.010009186953061917\n",
      "Test loss: 0.04620966453243185\n",
      "Starting epoch 3361\n",
      "Training loss: 0.009895203360280052\n",
      "Test loss: 0.045706308274357406\n",
      "Starting epoch 3362\n",
      "Training loss: 0.009863965686593876\n",
      "Test loss: 0.04625512121452226\n",
      "Starting epoch 3363\n",
      "Training loss: 0.009990896220456381\n",
      "Test loss: 0.04614759995429604\n",
      "Starting epoch 3364\n",
      "Training loss: 0.009898182600125914\n",
      "Test loss: 0.046506308019161224\n",
      "Starting epoch 3365\n",
      "Training loss: 0.00985277205949924\n",
      "Test loss: 0.04619528587769579\n",
      "Starting epoch 3366\n",
      "Training loss: 0.009905755596204859\n",
      "Test loss: 0.04605594967250471\n",
      "Starting epoch 3367\n",
      "Training loss: 0.009864651185811544\n",
      "Test loss: 0.04571723351599993\n",
      "Starting epoch 3368\n",
      "Training loss: 0.009995171998734356\n",
      "Test loss: 0.0461498181577082\n",
      "Starting epoch 3369\n",
      "Training loss: 0.009920711842838858\n",
      "Test loss: 0.04621951337213869\n",
      "Starting epoch 3370\n",
      "Training loss: 0.010016575761017252\n",
      "Test loss: 0.04612026370509907\n",
      "Starting epoch 3371\n",
      "Training loss: 0.009856422233288406\n",
      "Test loss: 0.04568878630245173\n",
      "Starting epoch 3372\n",
      "Training loss: 0.0099035648201577\n",
      "Test loss: 0.04602038756840759\n",
      "Starting epoch 3373\n",
      "Training loss: 0.010089073849261785\n",
      "Test loss: 0.046078889044346635\n",
      "Starting epoch 3374\n",
      "Training loss: 0.009887079830418845\n",
      "Test loss: 0.04572179216753553\n",
      "Starting epoch 3375\n",
      "Training loss: 0.010046690732973521\n",
      "Test loss: 0.04576714447251073\n",
      "Starting epoch 3376\n",
      "Training loss: 0.009858476227057761\n",
      "Test loss: 0.04658821100989977\n",
      "Starting epoch 3377\n",
      "Training loss: 0.009863230842547338\n",
      "Test loss: 0.046489370779858694\n",
      "Starting epoch 3378\n",
      "Training loss: 0.009918287839190881\n",
      "Test loss: 0.04612956058096002\n",
      "Starting epoch 3379\n",
      "Training loss: 0.009926942436665784\n",
      "Test loss: 0.04626692983287352\n",
      "Starting epoch 3380\n",
      "Training loss: 0.009934723911593194\n",
      "Test loss: 0.04653626004302943\n",
      "Starting epoch 3381\n",
      "Training loss: 0.009943568865295316\n",
      "Test loss: 0.04640187909481702\n",
      "Starting epoch 3382\n",
      "Training loss: 0.010189861868370752\n",
      "Test loss: 0.04675722032509468\n",
      "Starting epoch 3383\n",
      "Training loss: 0.010231954121931654\n",
      "Test loss: 0.04720397817867773\n",
      "Starting epoch 3384\n",
      "Training loss: 0.010020986336787215\n",
      "Test loss: 0.045977689463783195\n",
      "Starting epoch 3385\n",
      "Training loss: 0.00984938926689449\n",
      "Test loss: 0.04621047292042662\n",
      "Starting epoch 3386\n",
      "Training loss: 0.010016398534911578\n",
      "Test loss: 0.046047297271865385\n",
      "Starting epoch 3387\n",
      "Training loss: 0.00983086211576325\n",
      "Test loss: 0.04648614040127507\n",
      "Starting epoch 3388\n",
      "Training loss: 0.009881192680875787\n",
      "Test loss: 0.04638438257906172\n",
      "Starting epoch 3389\n",
      "Training loss: 0.009929070142326786\n",
      "Test loss: 0.046428251183695264\n",
      "Starting epoch 3390\n",
      "Training loss: 0.009920353726407543\n",
      "Test loss: 0.046059638192808186\n",
      "Starting epoch 3391\n",
      "Training loss: 0.009912689017956375\n",
      "Test loss: 0.04624795403193544\n",
      "Starting epoch 3392\n",
      "Training loss: 0.00986692909395597\n",
      "Test loss: 0.04587274527660123\n",
      "Starting epoch 3393\n",
      "Training loss: 0.009868434294447547\n",
      "Test loss: 0.045868726516211475\n",
      "Starting epoch 3394\n",
      "Training loss: 0.009891442679723755\n",
      "Test loss: 0.04602211368856607\n",
      "Starting epoch 3395\n",
      "Training loss: 0.009929650661642433\n",
      "Test loss: 0.04580505026711358\n",
      "Starting epoch 3396\n",
      "Training loss: 0.00990216847753427\n",
      "Test loss: 0.04632628212372462\n",
      "Starting epoch 3397\n",
      "Training loss: 0.009907219345208074\n",
      "Test loss: 0.04635850409114802\n",
      "Starting epoch 3398\n",
      "Training loss: 0.009944400658495113\n",
      "Test loss: 0.046123247041746425\n",
      "Starting epoch 3399\n",
      "Training loss: 0.009892583229258413\n",
      "Test loss: 0.046325902420061606\n",
      "Starting epoch 3400\n",
      "Training loss: 0.010108035775359537\n",
      "Test loss: 0.046123658341390116\n",
      "Starting epoch 3401\n",
      "Training loss: 0.010097326802425697\n",
      "Test loss: 0.04583103471884021\n",
      "Starting epoch 3402\n",
      "Training loss: 0.009902497799303687\n",
      "Test loss: 0.04658011526421264\n",
      "Starting epoch 3403\n",
      "Training loss: 0.009904729576445505\n",
      "Test loss: 0.04654960706830025\n",
      "Starting epoch 3404\n",
      "Training loss: 0.009908962506251257\n",
      "Test loss: 0.046024454136689506\n",
      "Starting epoch 3405\n",
      "Training loss: 0.009970353885752256\n",
      "Test loss: 0.046350680843547536\n",
      "Starting epoch 3406\n",
      "Training loss: 0.00996707816470842\n",
      "Test loss: 0.04646033048629761\n",
      "Starting epoch 3407\n",
      "Training loss: 0.009831554363252686\n",
      "Test loss: 0.046583639940729844\n",
      "Starting epoch 3408\n",
      "Training loss: 0.009926261532990659\n",
      "Test loss: 0.046496161846099074\n",
      "Starting epoch 3409\n",
      "Training loss: 0.009853483727354496\n",
      "Test loss: 0.04634338755298544\n",
      "Starting epoch 3410\n",
      "Training loss: 0.009829594814752946\n",
      "Test loss: 0.04597096897109791\n",
      "Starting epoch 3411\n",
      "Training loss: 0.009985437983127891\n",
      "Test loss: 0.045854462793579805\n",
      "Starting epoch 3412\n",
      "Training loss: 0.009837829278873616\n",
      "Test loss: 0.045684563754885284\n",
      "Starting epoch 3413\n",
      "Training loss: 0.009866900497772654\n",
      "Test loss: 0.04578702013801645\n",
      "Starting epoch 3414\n",
      "Training loss: 0.009851740566311313\n",
      "Test loss: 0.046179836684906925\n",
      "Starting epoch 3415\n",
      "Training loss: 0.0099045280367136\n",
      "Test loss: 0.0462389195130931\n",
      "Starting epoch 3416\n",
      "Training loss: 0.009824759678029623\n",
      "Test loss: 0.045941191966886875\n",
      "Starting epoch 3417\n",
      "Training loss: 0.009907069142724647\n",
      "Test loss: 0.04609161139362388\n",
      "Starting epoch 3418\n",
      "Training loss: 0.0098861373625085\n",
      "Test loss: 0.04604827971370132\n",
      "Starting epoch 3419\n",
      "Training loss: 0.009932870167444964\n",
      "Test loss: 0.04611346360158037\n",
      "Starting epoch 3420\n",
      "Training loss: 0.009876868412753597\n",
      "Test loss: 0.04635745963012731\n",
      "Starting epoch 3421\n",
      "Training loss: 0.009835799155970578\n",
      "Test loss: 0.04612175264844188\n",
      "Starting epoch 3422\n",
      "Training loss: 0.009853619196620143\n",
      "Test loss: 0.04622569762998157\n",
      "Starting epoch 3423\n",
      "Training loss: 0.00987324172814117\n",
      "Test loss: 0.04595845813552538\n",
      "Starting epoch 3424\n",
      "Training loss: 0.009955550131739163\n",
      "Test loss: 0.046221459215437924\n",
      "Starting epoch 3425\n",
      "Training loss: 0.009887702366123434\n",
      "Test loss: 0.04586157230315385\n",
      "Starting epoch 3426\n",
      "Training loss: 0.009936857831160554\n",
      "Test loss: 0.045947441900217975\n",
      "Starting epoch 3427\n",
      "Training loss: 0.009889364822721873\n",
      "Test loss: 0.04636715490508963\n",
      "Starting epoch 3428\n",
      "Training loss: 0.009948025480462391\n",
      "Test loss: 0.046609485866846864\n",
      "Starting epoch 3429\n",
      "Training loss: 0.009871081052134271\n",
      "Test loss: 0.045968330913671744\n",
      "Starting epoch 3430\n",
      "Training loss: 0.00981874119673596\n",
      "Test loss: 0.04608336387685052\n",
      "Starting epoch 3431\n",
      "Training loss: 0.009913947769120092\n",
      "Test loss: 0.04609752480906469\n",
      "Starting epoch 3432\n",
      "Training loss: 0.00984740422153082\n",
      "Test loss: 0.04644528721217756\n",
      "Starting epoch 3433\n",
      "Training loss: 0.009910469584655567\n",
      "Test loss: 0.04658039231543188\n",
      "Starting epoch 3434\n",
      "Training loss: 0.010032698115120168\n",
      "Test loss: 0.046136204843167904\n",
      "Starting epoch 3435\n",
      "Training loss: 0.010075529273904737\n",
      "Test loss: 0.04568709170928708\n",
      "Starting epoch 3436\n",
      "Training loss: 0.009866568595781678\n",
      "Test loss: 0.04549453890434018\n",
      "Starting epoch 3437\n",
      "Training loss: 0.01011102234364533\n",
      "Test loss: 0.04603212678598033\n",
      "Starting epoch 3438\n",
      "Training loss: 0.009946303678768092\n",
      "Test loss: 0.047002119598565276\n",
      "Starting epoch 3439\n",
      "Training loss: 0.010012740673894276\n",
      "Test loss: 0.04664113006933972\n",
      "Starting epoch 3440\n",
      "Training loss: 0.009958033129328588\n",
      "Test loss: 0.0457553764184316\n",
      "Starting epoch 3441\n",
      "Training loss: 0.0098985864277013\n",
      "Test loss: 0.04605740474330054\n",
      "Starting epoch 3442\n",
      "Training loss: 0.00992265546724933\n",
      "Test loss: 0.046197864330477185\n",
      "Starting epoch 3443\n",
      "Training loss: 0.009848292428450506\n",
      "Test loss: 0.04612122669264122\n",
      "Starting epoch 3444\n",
      "Training loss: 0.009944813234395668\n",
      "Test loss: 0.046238962284944674\n",
      "Starting epoch 3445\n",
      "Training loss: 0.00988732366898998\n",
      "Test loss: 0.04605074985711663\n",
      "Starting epoch 3446\n",
      "Training loss: 0.009909484718666702\n",
      "Test loss: 0.04639938363322505\n",
      "Starting epoch 3447\n",
      "Training loss: 0.00988284783956946\n",
      "Test loss: 0.04620614434006037\n",
      "Starting epoch 3448\n",
      "Training loss: 0.010184503145149498\n",
      "Test loss: 0.04651103982770884\n",
      "Starting epoch 3449\n",
      "Training loss: 0.009908703323759016\n",
      "Test loss: 0.047083805004755654\n",
      "Starting epoch 3450\n",
      "Training loss: 0.009888792792182476\n",
      "Test loss: 0.04654360176236541\n",
      "Starting epoch 3451\n",
      "Training loss: 0.009921398349717015\n",
      "Test loss: 0.04621613025665283\n",
      "Starting epoch 3452\n",
      "Training loss: 0.009895788215589328\n",
      "Test loss: 0.04646910306204249\n",
      "Starting epoch 3453\n",
      "Training loss: 0.009842872818107487\n",
      "Test loss: 0.04623248497093165\n",
      "Starting epoch 3454\n",
      "Training loss: 0.009991258825557153\n",
      "Test loss: 0.046141167205792886\n",
      "Starting epoch 3455\n",
      "Training loss: 0.009862967186653223\n",
      "Test loss: 0.046595350873691065\n",
      "Starting epoch 3456\n",
      "Training loss: 0.009843587608183504\n",
      "Test loss: 0.04640620850302555\n",
      "Starting epoch 3457\n",
      "Training loss: 0.009979245191836943\n",
      "Test loss: 0.04608183195469556\n",
      "Starting epoch 3458\n",
      "Training loss: 0.00994049714969807\n",
      "Test loss: 0.0465267099854019\n",
      "Starting epoch 3459\n",
      "Training loss: 0.009921632111683244\n",
      "Test loss: 0.0464557932206878\n",
      "Starting epoch 3460\n",
      "Training loss: 0.009925860262735457\n",
      "Test loss: 0.04613293941926073\n",
      "Starting epoch 3461\n",
      "Training loss: 0.009890741897655314\n",
      "Test loss: 0.04606322861380047\n",
      "Starting epoch 3462\n",
      "Training loss: 0.00986849728849579\n",
      "Test loss: 0.04625469680737566\n",
      "Starting epoch 3463\n",
      "Training loss: 0.009964713780972802\n",
      "Test loss: 0.046246608788216556\n",
      "Starting epoch 3464\n",
      "Training loss: 0.009967472266833314\n",
      "Test loss: 0.0458893501372249\n",
      "Starting epoch 3465\n",
      "Training loss: 0.009910717331727997\n",
      "Test loss: 0.045707428758894955\n",
      "Starting epoch 3466\n",
      "Training loss: 0.010022759345955536\n",
      "Test loss: 0.045905126327717746\n",
      "Starting epoch 3467\n",
      "Training loss: 0.01008287102716868\n",
      "Test loss: 0.046435442511682155\n",
      "Starting epoch 3468\n",
      "Training loss: 0.009855931563699832\n",
      "Test loss: 0.04595632078471007\n",
      "Starting epoch 3469\n",
      "Training loss: 0.0098940035877902\n",
      "Test loss: 0.04582614851770578\n",
      "Starting epoch 3470\n",
      "Training loss: 0.009877412396742672\n",
      "Test loss: 0.04592696315160504\n",
      "Starting epoch 3471\n",
      "Training loss: 0.0100919049019452\n",
      "Test loss: 0.04620316721223019\n",
      "Starting epoch 3472\n",
      "Training loss: 0.009928810288060884\n",
      "Test loss: 0.04600272148295685\n",
      "Starting epoch 3473\n",
      "Training loss: 0.009936910183703313\n",
      "Test loss: 0.046435956877690775\n",
      "Starting epoch 3474\n",
      "Training loss: 0.009874561847355521\n",
      "Test loss: 0.046729522170843904\n",
      "Starting epoch 3475\n",
      "Training loss: 0.00994869094097712\n",
      "Test loss: 0.046311778050881845\n",
      "Starting epoch 3476\n",
      "Training loss: 0.009869288408853968\n",
      "Test loss: 0.04578376395834817\n",
      "Starting epoch 3477\n",
      "Training loss: 0.009991655630044272\n",
      "Test loss: 0.046209682882935914\n",
      "Starting epoch 3478\n",
      "Training loss: 0.00993917282426455\n",
      "Test loss: 0.046698687253174956\n",
      "Starting epoch 3479\n",
      "Training loss: 0.00984126256900977\n",
      "Test loss: 0.04610007276965512\n",
      "Starting epoch 3480\n",
      "Training loss: 0.009849273874500736\n",
      "Test loss: 0.045913022425439626\n",
      "Starting epoch 3481\n",
      "Training loss: 0.01001872929942901\n",
      "Test loss: 0.04606810764030174\n",
      "Starting epoch 3482\n",
      "Training loss: 0.009878278664145314\n",
      "Test loss: 0.04559329434953354\n",
      "Starting epoch 3483\n",
      "Training loss: 0.009914681193281392\n",
      "Test loss: 0.04609585236068125\n",
      "Starting epoch 3484\n",
      "Training loss: 0.009881530140267044\n",
      "Test loss: 0.04598680042006351\n",
      "Starting epoch 3485\n",
      "Training loss: 0.009877686907888436\n",
      "Test loss: 0.046296505326474155\n",
      "Starting epoch 3486\n",
      "Training loss: 0.009865391938412776\n",
      "Test loss: 0.04615285758067061\n",
      "Starting epoch 3487\n",
      "Training loss: 0.00994311149430568\n",
      "Test loss: 0.046031727421062964\n",
      "Starting epoch 3488\n",
      "Training loss: 0.009875013050241549\n",
      "Test loss: 0.04626607825910604\n",
      "Starting epoch 3489\n",
      "Training loss: 0.0099310146369895\n",
      "Test loss: 0.046373097915892246\n",
      "Starting epoch 3490\n",
      "Training loss: 0.010095393056141549\n",
      "Test loss: 0.046069899642909015\n",
      "Starting epoch 3491\n",
      "Training loss: 0.00995721238985902\n",
      "Test loss: 0.04647305828553659\n",
      "Starting epoch 3492\n",
      "Training loss: 0.009882331451737001\n",
      "Test loss: 0.04624551562247453\n",
      "Starting epoch 3493\n",
      "Training loss: 0.010012073320199232\n",
      "Test loss: 0.045984017490236846\n",
      "Starting epoch 3494\n",
      "Training loss: 0.00986586007304856\n",
      "Test loss: 0.046370568650740164\n",
      "Starting epoch 3495\n",
      "Training loss: 0.009988060037865013\n",
      "Test loss: 0.046171248649005535\n",
      "Starting epoch 3496\n",
      "Training loss: 0.009828458036311338\n",
      "Test loss: 0.0456369466803692\n",
      "Starting epoch 3497\n",
      "Training loss: 0.009900464798461218\n",
      "Test loss: 0.045837930507130094\n",
      "Starting epoch 3498\n",
      "Training loss: 0.009929692739101707\n",
      "Test loss: 0.04580833169597166\n",
      "Starting epoch 3499\n",
      "Training loss: 0.01004792959047634\n",
      "Test loss: 0.04599070162684829\n",
      "Starting epoch 3500\n",
      "Training loss: 0.009866926238917913\n",
      "Test loss: 0.04663896864211118\n",
      "Starting epoch 3501\n",
      "Training loss: 0.009892744974034732\n",
      "Test loss: 0.04659458249807358\n",
      "Starting epoch 3502\n",
      "Training loss: 0.009868071338192362\n",
      "Test loss: 0.046296693246673654\n",
      "Starting epoch 3503\n",
      "Training loss: 0.010026220247515889\n",
      "Test loss: 0.046222679041050097\n",
      "Starting epoch 3504\n",
      "Training loss: 0.009953206954676597\n",
      "Test loss: 0.046639531712841104\n",
      "Starting epoch 3505\n",
      "Training loss: 0.009851027249557073\n",
      "Test loss: 0.04614312070663328\n",
      "Starting epoch 3506\n",
      "Training loss: 0.009852740577742701\n",
      "Test loss: 0.04589843639621028\n",
      "Starting epoch 3507\n",
      "Training loss: 0.010140845674227496\n",
      "Test loss: 0.04594917919624735\n",
      "Starting epoch 3508\n",
      "Training loss: 0.00999502735365121\n",
      "Test loss: 0.04545639896834338\n",
      "Starting epoch 3509\n",
      "Training loss: 0.010010516576346804\n",
      "Test loss: 0.04597035271150095\n",
      "Starting epoch 3510\n",
      "Training loss: 0.009907602378335155\n",
      "Test loss: 0.04573356870699812\n",
      "Starting epoch 3511\n",
      "Training loss: 0.009945712601918667\n",
      "Test loss: 0.04586585873255023\n",
      "Starting epoch 3512\n",
      "Training loss: 0.0098900545357925\n",
      "Test loss: 0.04589684128209397\n",
      "Starting epoch 3513\n",
      "Training loss: 0.009894789547705259\n",
      "Test loss: 0.04594015136913017\n",
      "Starting epoch 3514\n",
      "Training loss: 0.009861003057878525\n",
      "Test loss: 0.04590873365049009\n",
      "Starting epoch 3515\n",
      "Training loss: 0.00989656968683493\n",
      "Test loss: 0.04582978191751021\n",
      "Starting epoch 3516\n",
      "Training loss: 0.009866365033095001\n",
      "Test loss: 0.04583883809822577\n",
      "Starting epoch 3517\n",
      "Training loss: 0.009934863594711804\n",
      "Test loss: 0.045916384706894554\n",
      "Starting epoch 3518\n",
      "Training loss: 0.009851392831836567\n",
      "Test loss: 0.045935515521301165\n",
      "Starting epoch 3519\n",
      "Training loss: 0.009948867892266297\n",
      "Test loss: 0.04625606040159861\n",
      "Starting epoch 3520\n",
      "Training loss: 0.009916743141461591\n",
      "Test loss: 0.046627414792224216\n",
      "Starting epoch 3521\n",
      "Training loss: 0.009934371169351164\n",
      "Test loss: 0.046400495839339716\n",
      "Starting epoch 3522\n",
      "Training loss: 0.009946829166080131\n",
      "Test loss: 0.04596724588837889\n",
      "Starting epoch 3523\n",
      "Training loss: 0.009990503400808474\n",
      "Test loss: 0.04585644816634832\n",
      "Starting epoch 3524\n",
      "Training loss: 0.00984097478270042\n",
      "Test loss: 0.04654628479922259\n",
      "Starting epoch 3525\n",
      "Training loss: 0.009942343901293199\n",
      "Test loss: 0.0465524029676561\n",
      "Starting epoch 3526\n",
      "Training loss: 0.009994683619283263\n",
      "Test loss: 0.046780231374281424\n",
      "Starting epoch 3527\n",
      "Training loss: 0.010124158824137489\n",
      "Test loss: 0.046802466528283224\n",
      "Starting epoch 3528\n",
      "Training loss: 0.009981716402852144\n",
      "Test loss: 0.046067270208840015\n",
      "Starting epoch 3529\n",
      "Training loss: 0.009896805219840809\n",
      "Test loss: 0.046345418250119244\n",
      "Starting epoch 3530\n",
      "Training loss: 0.009952073565638457\n",
      "Test loss: 0.046218456562470506\n",
      "Starting epoch 3531\n",
      "Training loss: 0.00986685692409023\n",
      "Test loss: 0.046197458963703225\n",
      "Starting epoch 3532\n",
      "Training loss: 0.009928917955057542\n",
      "Test loss: 0.04612705815169546\n",
      "Starting epoch 3533\n",
      "Training loss: 0.009911406342489798\n",
      "Test loss: 0.0459405490093761\n",
      "Starting epoch 3534\n",
      "Training loss: 0.009872537930725052\n",
      "Test loss: 0.0462525590426392\n",
      "Starting epoch 3535\n",
      "Training loss: 0.009872625093357484\n",
      "Test loss: 0.04635405657743966\n",
      "Starting epoch 3536\n",
      "Training loss: 0.010028655961400172\n",
      "Test loss: 0.04644496256002673\n",
      "Starting epoch 3537\n",
      "Training loss: 0.009958691604924007\n",
      "Test loss: 0.04675055764339588\n",
      "Starting epoch 3538\n",
      "Training loss: 0.009841025684822778\n",
      "Test loss: 0.04613703999806334\n",
      "Starting epoch 3539\n",
      "Training loss: 0.00986948304000448\n",
      "Test loss: 0.046009957514427324\n",
      "Starting epoch 3540\n",
      "Training loss: 0.009949096020494328\n",
      "Test loss: 0.04614270768231816\n",
      "Starting epoch 3541\n",
      "Training loss: 0.009943231528044725\n",
      "Test loss: 0.04590397576491038\n",
      "Starting epoch 3542\n",
      "Training loss: 0.010057361430076302\n",
      "Test loss: 0.046165038314130574\n",
      "Starting epoch 3543\n",
      "Training loss: 0.010010892983342781\n",
      "Test loss: 0.045957493837232945\n",
      "Starting epoch 3544\n",
      "Training loss: 0.00993790540111358\n",
      "Test loss: 0.04558186619370072\n",
      "Starting epoch 3545\n",
      "Training loss: 0.009894116659511308\n",
      "Test loss: 0.04599285525856195\n",
      "Starting epoch 3546\n",
      "Training loss: 0.010002501522664164\n",
      "Test loss: 0.046397088992374914\n",
      "Starting epoch 3547\n",
      "Training loss: 0.009915020652726049\n",
      "Test loss: 0.04615576358305083\n",
      "Starting epoch 3548\n",
      "Training loss: 0.01008399924049612\n",
      "Test loss: 0.04628593916142428\n",
      "Starting epoch 3549\n",
      "Training loss: 0.009903460466226593\n",
      "Test loss: 0.046142497548350585\n",
      "Starting epoch 3550\n",
      "Training loss: 0.010221766475893434\n",
      "Test loss: 0.0459723399469146\n",
      "Starting epoch 3551\n",
      "Training loss: 0.009859140351658961\n",
      "Test loss: 0.04669345197854219\n",
      "Starting epoch 3552\n",
      "Training loss: 0.009922449843438923\n",
      "Test loss: 0.046723345294594765\n",
      "Starting epoch 3553\n",
      "Training loss: 0.00990714993877489\n",
      "Test loss: 0.04608509675772102\n",
      "Starting epoch 3554\n",
      "Training loss: 0.009871897257131631\n",
      "Test loss: 0.04603762690115858\n",
      "Starting epoch 3555\n",
      "Training loss: 0.009854476929443782\n",
      "Test loss: 0.04631678635875384\n",
      "Starting epoch 3556\n",
      "Training loss: 0.009894313993024045\n",
      "Test loss: 0.04637061680356661\n",
      "Starting epoch 3557\n",
      "Training loss: 0.010041619483076159\n",
      "Test loss: 0.04624755335626779\n",
      "Starting epoch 3558\n",
      "Training loss: 0.009884606329266165\n",
      "Test loss: 0.045928854081365794\n",
      "Starting epoch 3559\n",
      "Training loss: 0.00985877976187917\n",
      "Test loss: 0.04598747055839609\n",
      "Starting epoch 3560\n",
      "Training loss: 0.009941371951679715\n",
      "Test loss: 0.04603780240372375\n",
      "Starting epoch 3561\n",
      "Training loss: 0.010067374002738078\n",
      "Test loss: 0.046049251600548076\n",
      "Starting epoch 3562\n",
      "Training loss: 0.010027524007514853\n",
      "Test loss: 0.045787462343772255\n",
      "Starting epoch 3563\n",
      "Training loss: 0.009859172856343574\n",
      "Test loss: 0.04623526196788858\n",
      "Starting epoch 3564\n",
      "Training loss: 0.009906887672230845\n",
      "Test loss: 0.046616328259309135\n",
      "Starting epoch 3565\n",
      "Training loss: 0.009883543369589283\n",
      "Test loss: 0.046218447387218475\n",
      "Starting epoch 3566\n",
      "Training loss: 0.00988203843628041\n",
      "Test loss: 0.04619012138357869\n",
      "Starting epoch 3567\n",
      "Training loss: 0.00992659331284097\n",
      "Test loss: 0.04598678289740174\n",
      "Starting epoch 3568\n",
      "Training loss: 0.00990786823276125\n",
      "Test loss: 0.046284621098527205\n",
      "Starting epoch 3569\n",
      "Training loss: 0.009857583317722454\n",
      "Test loss: 0.04633331409207097\n",
      "Starting epoch 3570\n",
      "Training loss: 0.010292202417479187\n",
      "Test loss: 0.04612993124734472\n",
      "Starting epoch 3571\n",
      "Training loss: 0.01001521615219898\n",
      "Test loss: 0.04541227828573297\n",
      "Starting epoch 3572\n",
      "Training loss: 0.010118180916446154\n",
      "Test loss: 0.04635418634171839\n",
      "Starting epoch 3573\n",
      "Training loss: 0.00999456048622483\n",
      "Test loss: 0.04608685888901905\n",
      "Starting epoch 3574\n",
      "Training loss: 0.0099209602158822\n",
      "Test loss: 0.046378034822366854\n",
      "Starting epoch 3575\n",
      "Training loss: 0.009975591002673399\n",
      "Test loss: 0.0462138286481301\n",
      "Starting epoch 3576\n",
      "Training loss: 0.009911466786851649\n",
      "Test loss: 0.046479941656192146\n",
      "Starting epoch 3577\n",
      "Training loss: 0.009913802055306122\n",
      "Test loss: 0.046025098887858565\n",
      "Starting epoch 3578\n",
      "Training loss: 0.0098763998353579\n",
      "Test loss: 0.046284142467710704\n",
      "Starting epoch 3579\n",
      "Training loss: 0.010052768116603132\n",
      "Test loss: 0.04624784227322649\n",
      "Starting epoch 3580\n",
      "Training loss: 0.009888464187989469\n",
      "Test loss: 0.045740382814848865\n",
      "Starting epoch 3581\n",
      "Training loss: 0.009949163625352695\n",
      "Test loss: 0.046082853305119055\n",
      "Starting epoch 3582\n",
      "Training loss: 0.009978878235474962\n",
      "Test loss: 0.046428377015723124\n",
      "Starting epoch 3583\n",
      "Training loss: 0.010016901601777702\n",
      "Test loss: 0.046237978946279595\n",
      "Starting epoch 3584\n",
      "Training loss: 0.009871131786313212\n",
      "Test loss: 0.04669964465278166\n",
      "Starting epoch 3585\n",
      "Training loss: 0.009946308236141673\n",
      "Test loss: 0.046416944789665716\n",
      "Starting epoch 3586\n",
      "Training loss: 0.009900904917081848\n",
      "Test loss: 0.04660090790302665\n",
      "Starting epoch 3587\n",
      "Training loss: 0.010198589734977385\n",
      "Test loss: 0.046586181278581974\n",
      "Starting epoch 3588\n",
      "Training loss: 0.009873796849832183\n",
      "Test loss: 0.04590408655780333\n",
      "Starting epoch 3589\n",
      "Training loss: 0.010002260401722838\n",
      "Test loss: 0.04611501787547712\n",
      "Starting epoch 3590\n",
      "Training loss: 0.010238884093209367\n",
      "Test loss: 0.04603012361460262\n",
      "Starting epoch 3591\n",
      "Training loss: 0.010354016205204314\n",
      "Test loss: 0.046955334229601756\n",
      "Starting epoch 3592\n",
      "Training loss: 0.009920982934045986\n",
      "Test loss: 0.046029037278559476\n",
      "Starting epoch 3593\n",
      "Training loss: 0.00984851736575365\n",
      "Test loss: 0.04632216485010253\n",
      "Starting epoch 3594\n",
      "Training loss: 0.009850768738838493\n",
      "Test loss: 0.04629943464641218\n",
      "Starting epoch 3595\n",
      "Training loss: 0.009875731924396069\n",
      "Test loss: 0.04636895718673865\n",
      "Starting epoch 3596\n",
      "Training loss: 0.009958685513158313\n",
      "Test loss: 0.04637580930634781\n",
      "Starting epoch 3597\n",
      "Training loss: 0.009914071757163181\n",
      "Test loss: 0.046902602814413885\n",
      "Starting epoch 3598\n",
      "Training loss: 0.009915381700533335\n",
      "Test loss: 0.046863452013995915\n",
      "Starting epoch 3599\n",
      "Training loss: 0.009919451610719571\n",
      "Test loss: 0.046275038824037266\n",
      "Starting epoch 3600\n",
      "Training loss: 0.009874457218607918\n",
      "Test loss: 0.045896868462915776\n",
      "Starting epoch 3601\n",
      "Training loss: 0.010065781471670652\n",
      "Test loss: 0.04612300779532503\n",
      "Starting epoch 3602\n",
      "Training loss: 0.009834914590369483\n",
      "Test loss: 0.04586851500250675\n",
      "Starting epoch 3603\n",
      "Training loss: 0.01003880861413772\n",
      "Test loss: 0.04620762997203403\n",
      "Starting epoch 3604\n",
      "Training loss: 0.009845425809932048\n",
      "Test loss: 0.0461088126456296\n",
      "Starting epoch 3605\n",
      "Training loss: 0.009864241190132548\n",
      "Test loss: 0.04616115387115213\n",
      "Starting epoch 3606\n",
      "Training loss: 0.009884864122408335\n",
      "Test loss: 0.046099152967885686\n",
      "Starting epoch 3607\n",
      "Training loss: 0.009876448920637857\n",
      "Test loss: 0.04634548806481891\n",
      "Starting epoch 3608\n",
      "Training loss: 0.010028183902995508\n",
      "Test loss: 0.04639176424178812\n",
      "Starting epoch 3609\n",
      "Training loss: 0.009880456004719266\n",
      "Test loss: 0.045915570144576055\n",
      "Starting epoch 3610\n",
      "Training loss: 0.009930946604638804\n",
      "Test loss: 0.0460346737117679\n",
      "Starting epoch 3611\n",
      "Training loss: 0.01002611032091692\n",
      "Test loss: 0.04617441928497067\n",
      "Starting epoch 3612\n",
      "Training loss: 0.009862397614194721\n",
      "Test loss: 0.04599552532588994\n",
      "Starting epoch 3613\n",
      "Training loss: 0.009886702049340381\n",
      "Test loss: 0.046534668930150846\n",
      "Starting epoch 3614\n",
      "Training loss: 0.009866476929212203\n",
      "Test loss: 0.046329838810143645\n",
      "Starting epoch 3615\n",
      "Training loss: 0.0099130780054409\n",
      "Test loss: 0.04619056800449336\n",
      "Starting epoch 3616\n",
      "Training loss: 0.010379691913601805\n",
      "Test loss: 0.0462140670667092\n",
      "Starting epoch 3617\n",
      "Training loss: 0.009819929793354918\n",
      "Test loss: 0.04552901481036787\n",
      "Starting epoch 3618\n",
      "Training loss: 0.00998619963705051\n",
      "Test loss: 0.04582610436611705\n",
      "Starting epoch 3619\n",
      "Training loss: 0.010093164172206745\n",
      "Test loss: 0.04626995421670101\n",
      "Starting epoch 3620\n",
      "Training loss: 0.009853819003481357\n",
      "Test loss: 0.045933261789657454\n",
      "Starting epoch 3621\n",
      "Training loss: 0.009892029718297427\n",
      "Test loss: 0.04611180681321356\n",
      "Starting epoch 3622\n",
      "Training loss: 0.009999333285405988\n",
      "Test loss: 0.0465897117499952\n",
      "Starting epoch 3623\n",
      "Training loss: 0.009932780760477801\n",
      "Test loss: 0.04614197614568251\n",
      "Starting epoch 3624\n",
      "Training loss: 0.00985350207898949\n",
      "Test loss: 0.04607277591195372\n",
      "Starting epoch 3625\n",
      "Training loss: 0.009915622516122997\n",
      "Test loss: 0.046271363342249835\n",
      "Starting epoch 3626\n",
      "Training loss: 0.009833968152887508\n",
      "Test loss: 0.04650415742286929\n",
      "Starting epoch 3627\n",
      "Training loss: 0.00993381556673128\n",
      "Test loss: 0.04634325985831243\n",
      "Starting epoch 3628\n",
      "Training loss: 0.0099065435714409\n",
      "Test loss: 0.046191049532757864\n",
      "Starting epoch 3629\n",
      "Training loss: 0.009922239715691472\n",
      "Test loss: 0.04655884447749014\n",
      "Starting epoch 3630\n",
      "Training loss: 0.009990954435751086\n",
      "Test loss: 0.04641948854205785\n",
      "Starting epoch 3631\n",
      "Training loss: 0.009833332487061376\n",
      "Test loss: 0.04641367398478367\n",
      "Starting epoch 3632\n",
      "Training loss: 0.009891865149018218\n",
      "Test loss: 0.04640040698426741\n",
      "Starting epoch 3633\n",
      "Training loss: 0.009862161287274516\n",
      "Test loss: 0.04619056710766421\n",
      "Starting epoch 3634\n",
      "Training loss: 0.009881655517660204\n",
      "Test loss: 0.04601162444386217\n",
      "Starting epoch 3635\n",
      "Training loss: 0.009961883644344376\n",
      "Test loss: 0.0461246426458712\n",
      "Starting epoch 3636\n",
      "Training loss: 0.009904445996355326\n",
      "Test loss: 0.046164050974227766\n",
      "Starting epoch 3637\n",
      "Training loss: 0.009914676093908607\n",
      "Test loss: 0.0461197663443508\n",
      "Starting epoch 3638\n",
      "Training loss: 0.009868805724211404\n",
      "Test loss: 0.04607091692310793\n",
      "Starting epoch 3639\n",
      "Training loss: 0.00991915413712869\n",
      "Test loss: 0.04612389689794293\n",
      "Starting epoch 3640\n",
      "Training loss: 0.009897469916976378\n",
      "Test loss: 0.04673135349595988\n",
      "Starting epoch 3641\n",
      "Training loss: 0.009897640555119906\n",
      "Test loss: 0.046348031196329326\n",
      "Starting epoch 3642\n",
      "Training loss: 0.009982967428618768\n",
      "Test loss: 0.046252731440795794\n",
      "Starting epoch 3643\n",
      "Training loss: 0.00987384167545643\n",
      "Test loss: 0.04651079892560288\n",
      "Starting epoch 3644\n",
      "Training loss: 0.010034746047658999\n",
      "Test loss: 0.046674032868058594\n",
      "Starting epoch 3645\n",
      "Training loss: 0.009854664110013695\n",
      "Test loss: 0.047009975407962444\n",
      "Starting epoch 3646\n",
      "Training loss: 0.009841205582755511\n",
      "Test loss: 0.04688262746289924\n",
      "Starting epoch 3647\n",
      "Training loss: 0.00990540381582057\n",
      "Test loss: 0.046522059788306556\n",
      "Starting epoch 3648\n",
      "Training loss: 0.010331920500783647\n",
      "Test loss: 0.04638274759054184\n",
      "Starting epoch 3649\n",
      "Training loss: 0.009872630269068186\n",
      "Test loss: 0.04700889134848559\n",
      "Starting epoch 3650\n",
      "Training loss: 0.009917521146965808\n",
      "Test loss: 0.046704493876960546\n",
      "Starting epoch 3651\n",
      "Training loss: 0.009907285797363911\n",
      "Test loss: 0.04617621942802712\n",
      "Starting epoch 3652\n",
      "Training loss: 0.009986736338402405\n",
      "Test loss: 0.04633567496030419\n",
      "Starting epoch 3653\n",
      "Training loss: 0.010077987476939061\n",
      "Test loss: 0.046207612173424825\n",
      "Starting epoch 3654\n",
      "Training loss: 0.009892596786872286\n",
      "Test loss: 0.04682068940665987\n",
      "Starting epoch 3655\n",
      "Training loss: 0.010025916407342817\n",
      "Test loss: 0.046440995263832587\n",
      "Starting epoch 3656\n",
      "Training loss: 0.009921937127460222\n",
      "Test loss: 0.046799742996140765\n",
      "Starting epoch 3657\n",
      "Training loss: 0.009875879500855188\n",
      "Test loss: 0.046494976996823596\n",
      "Starting epoch 3658\n",
      "Training loss: 0.009957493908825468\n",
      "Test loss: 0.046221594567652104\n",
      "Starting epoch 3659\n",
      "Training loss: 0.009840394034371024\n",
      "Test loss: 0.046645293633143105\n",
      "Starting epoch 3660\n",
      "Training loss: 0.009852158226317069\n",
      "Test loss: 0.04666101849741406\n",
      "Starting epoch 3661\n",
      "Training loss: 0.009840933506789266\n",
      "Test loss: 0.04640740763258051\n",
      "Starting epoch 3662\n",
      "Training loss: 0.009865658754696612\n",
      "Test loss: 0.04648619614265583\n",
      "Starting epoch 3663\n",
      "Training loss: 0.010055833557223687\n",
      "Test loss: 0.04652247239870054\n",
      "Starting epoch 3664\n",
      "Training loss: 0.00996939395172674\n",
      "Test loss: 0.04681793627915559\n",
      "Starting epoch 3665\n",
      "Training loss: 0.009858904482766252\n",
      "Test loss: 0.04695803313343613\n",
      "Starting epoch 3666\n",
      "Training loss: 0.009941056431805501\n",
      "Test loss: 0.04660489458452772\n",
      "Starting epoch 3667\n",
      "Training loss: 0.00987992771580571\n",
      "Test loss: 0.04668463173287886\n",
      "Starting epoch 3668\n",
      "Training loss: 0.009876191738199015\n",
      "Test loss: 0.046547749528178466\n",
      "Starting epoch 3669\n",
      "Training loss: 0.0099075905306906\n",
      "Test loss: 0.04665740110256054\n",
      "Starting epoch 3670\n",
      "Training loss: 0.009916247311429899\n",
      "Test loss: 0.0467860660067311\n",
      "Starting epoch 3671\n",
      "Training loss: 0.00986325740814209\n",
      "Test loss: 0.04675413337018755\n",
      "Starting epoch 3672\n",
      "Training loss: 0.009889123320090966\n",
      "Test loss: 0.04638109949451906\n",
      "Starting epoch 3673\n",
      "Training loss: 0.009852017077510475\n",
      "Test loss: 0.0462560144563516\n",
      "Starting epoch 3674\n",
      "Training loss: 0.010052261965685204\n",
      "Test loss: 0.0462397861259955\n",
      "Starting epoch 3675\n",
      "Training loss: 0.009868335024621642\n",
      "Test loss: 0.045895132339662977\n",
      "Starting epoch 3676\n",
      "Training loss: 0.009843746475020393\n",
      "Test loss: 0.04606197470868075\n",
      "Starting epoch 3677\n",
      "Training loss: 0.009829048708569809\n",
      "Test loss: 0.046273434465682065\n",
      "Starting epoch 3678\n",
      "Training loss: 0.009893965052410227\n",
      "Test loss: 0.046132348063919276\n",
      "Starting epoch 3679\n",
      "Training loss: 0.009985617331427629\n",
      "Test loss: 0.04608919236946989\n",
      "Starting epoch 3680\n",
      "Training loss: 0.009878357551747659\n",
      "Test loss: 0.046579397249001044\n",
      "Starting epoch 3681\n",
      "Training loss: 0.01006194813268595\n",
      "Test loss: 0.04648237316696732\n",
      "Starting epoch 3682\n",
      "Training loss: 0.010347851063506525\n",
      "Test loss: 0.04635147170888053\n",
      "Starting epoch 3683\n",
      "Training loss: 0.009850496365154376\n",
      "Test loss: 0.04703918692690355\n",
      "Starting epoch 3684\n",
      "Training loss: 0.009947740045360854\n",
      "Test loss: 0.04672195721003744\n",
      "Starting epoch 3685\n",
      "Training loss: 0.009888011809499537\n",
      "Test loss: 0.04597618561927919\n",
      "Starting epoch 3686\n",
      "Training loss: 0.009847714947383911\n",
      "Test loss: 0.04643931695156627\n",
      "Starting epoch 3687\n",
      "Training loss: 0.00986104418874764\n",
      "Test loss: 0.04652044949708162\n",
      "Starting epoch 3688\n",
      "Training loss: 0.009879002714010536\n",
      "Test loss: 0.04643564229762113\n",
      "Starting epoch 3689\n",
      "Training loss: 0.00988683646514279\n",
      "Test loss: 0.046486815644635096\n",
      "Starting epoch 3690\n",
      "Training loss: 0.00998579344299973\n",
      "Test loss: 0.04621540796425608\n",
      "Starting epoch 3691\n",
      "Training loss: 0.009928226852636845\n",
      "Test loss: 0.04664563829148257\n",
      "Starting epoch 3692\n",
      "Training loss: 0.009886554442346096\n",
      "Test loss: 0.04673967882990837\n",
      "Starting epoch 3693\n",
      "Training loss: 0.009871911761335662\n",
      "Test loss: 0.04665209546133324\n",
      "Starting epoch 3694\n",
      "Training loss: 0.009906133026128909\n",
      "Test loss: 0.046592612785321695\n",
      "Starting epoch 3695\n",
      "Training loss: 0.009953211687627386\n",
      "Test loss: 0.046215447010817354\n",
      "Starting epoch 3696\n",
      "Training loss: 0.009905773573784067\n",
      "Test loss: 0.045783385565435444\n",
      "Starting epoch 3697\n",
      "Training loss: 0.009984430918073068\n",
      "Test loss: 0.04612307788597213\n",
      "Starting epoch 3698\n",
      "Training loss: 0.00989120682609863\n",
      "Test loss: 0.04665377004830926\n",
      "Starting epoch 3699\n",
      "Training loss: 0.010105962513900195\n",
      "Test loss: 0.04684633968604936\n",
      "Starting epoch 3700\n",
      "Training loss: 0.010046322089543596\n",
      "Test loss: 0.04612674964246927\n",
      "Starting epoch 3701\n",
      "Training loss: 0.00997982019955506\n",
      "Test loss: 0.04660007937086953\n",
      "Starting epoch 3702\n",
      "Training loss: 0.010143944337109074\n",
      "Test loss: 0.04607591750445189\n",
      "Starting epoch 3703\n",
      "Training loss: 0.009838746540004113\n",
      "Test loss: 0.04569454366962115\n",
      "Starting epoch 3704\n",
      "Training loss: 0.01002789305553573\n",
      "Test loss: 0.04612482849646498\n",
      "Starting epoch 3705\n",
      "Training loss: 0.009907598607242107\n",
      "Test loss: 0.04596284328511468\n",
      "Starting epoch 3706\n",
      "Training loss: 0.00986533520407364\n",
      "Test loss: 0.04635238219742422\n",
      "Starting epoch 3707\n",
      "Training loss: 0.009989118333295232\n",
      "Test loss: 0.046294845916606764\n",
      "Starting epoch 3708\n",
      "Training loss: 0.010017549832824801\n",
      "Test loss: 0.04581334441900253\n",
      "Starting epoch 3709\n",
      "Training loss: 0.010008104924173629\n",
      "Test loss: 0.04570484451121754\n",
      "Starting epoch 3710\n",
      "Training loss: 0.009856886856380056\n",
      "Test loss: 0.04616731239689721\n",
      "Starting epoch 3711\n",
      "Training loss: 0.009976437009993146\n",
      "Test loss: 0.046140091562712635\n",
      "Starting epoch 3712\n",
      "Training loss: 0.009964800332901909\n",
      "Test loss: 0.04659972919358148\n",
      "Starting epoch 3713\n",
      "Training loss: 0.010027438951808899\n",
      "Test loss: 0.046838271880039466\n",
      "Starting epoch 3714\n",
      "Training loss: 0.009876475669443607\n",
      "Test loss: 0.047040031739959014\n",
      "Starting epoch 3715\n",
      "Training loss: 0.009938427521923527\n",
      "Test loss: 0.04663941850540815\n",
      "Starting epoch 3716\n",
      "Training loss: 0.009980601548659996\n",
      "Test loss: 0.04681250397805815\n",
      "Starting epoch 3717\n",
      "Training loss: 0.009884252548828477\n",
      "Test loss: 0.04625167449315389\n",
      "Starting epoch 3718\n",
      "Training loss: 0.009829277997134163\n",
      "Test loss: 0.04637643819053968\n",
      "Starting epoch 3719\n",
      "Training loss: 0.010072241262456432\n",
      "Test loss: 0.04650325528173535\n",
      "Starting epoch 3720\n",
      "Training loss: 0.009931142167111889\n",
      "Test loss: 0.046210193661627944\n",
      "Starting epoch 3721\n",
      "Training loss: 0.009877684724624039\n",
      "Test loss: 0.04600162707545139\n",
      "Starting epoch 3722\n",
      "Training loss: 0.009876393056550964\n",
      "Test loss: 0.046465495256362135\n",
      "Starting epoch 3723\n",
      "Training loss: 0.009904151156422545\n",
      "Test loss: 0.04684490365562616\n",
      "Starting epoch 3724\n",
      "Training loss: 0.00988427888540948\n",
      "Test loss: 0.04675885276110084\n",
      "Starting epoch 3725\n",
      "Training loss: 0.009835255149080128\n",
      "Test loss: 0.0467683568044945\n",
      "Starting epoch 3726\n",
      "Training loss: 0.00987714922941122\n",
      "Test loss: 0.04633536286376141\n",
      "Starting epoch 3727\n",
      "Training loss: 0.009928157415668496\n",
      "Test loss: 0.046323402888245054\n",
      "Starting epoch 3728\n",
      "Training loss: 0.00989628476319743\n",
      "Test loss: 0.04663452161131082\n",
      "Starting epoch 3729\n",
      "Training loss: 0.00991977839807018\n",
      "Test loss: 0.04681265657698667\n",
      "Starting epoch 3730\n",
      "Training loss: 0.009861544423477083\n",
      "Test loss: 0.04662504599050239\n",
      "Starting epoch 3731\n",
      "Training loss: 0.009903563232329047\n",
      "Test loss: 0.046312406521152566\n",
      "Starting epoch 3732\n",
      "Training loss: 0.009873320837123473\n",
      "Test loss: 0.0466294897099336\n",
      "Starting epoch 3733\n",
      "Training loss: 0.009851304814219475\n",
      "Test loss: 0.04673864761436427\n",
      "Starting epoch 3734\n",
      "Training loss: 0.009976921786294609\n",
      "Test loss: 0.046694614062154735\n",
      "Starting epoch 3735\n",
      "Training loss: 0.009901367616458018\n",
      "Test loss: 0.04639894128949554\n",
      "Starting epoch 3736\n",
      "Training loss: 0.009908245555812219\n",
      "Test loss: 0.046555151955948934\n",
      "Starting epoch 3737\n",
      "Training loss: 0.01031322615434889\n",
      "Test loss: 0.046362144941533054\n",
      "Starting epoch 3738\n",
      "Training loss: 0.01021922547675547\n",
      "Test loss: 0.045788233547850894\n",
      "Starting epoch 3739\n",
      "Training loss: 0.009890967521999703\n",
      "Test loss: 0.0466475261858216\n",
      "Starting epoch 3740\n",
      "Training loss: 0.009905364623934518\n",
      "Test loss: 0.04709909259583111\n",
      "Starting epoch 3741\n",
      "Training loss: 0.009887669769833322\n",
      "Test loss: 0.046981467968887754\n",
      "Starting epoch 3742\n",
      "Training loss: 0.009953142159053536\n",
      "Test loss: 0.046491978896988764\n",
      "Starting epoch 3743\n",
      "Training loss: 0.010089724614727692\n",
      "Test loss: 0.046658144504935654\n",
      "Starting epoch 3744\n",
      "Training loss: 0.009816306902736913\n",
      "Test loss: 0.046013038881399015\n",
      "Starting epoch 3745\n",
      "Training loss: 0.009934562258422375\n",
      "Test loss: 0.04600031011634403\n",
      "Starting epoch 3746\n",
      "Training loss: 0.009979463457206234\n",
      "Test loss: 0.04665695020446071\n",
      "Starting epoch 3747\n",
      "Training loss: 0.009839197758157722\n",
      "Test loss: 0.04615842937319367\n",
      "Starting epoch 3748\n",
      "Training loss: 0.009878879199262525\n",
      "Test loss: 0.046224406747906295\n",
      "Starting epoch 3749\n",
      "Training loss: 0.009865382380905698\n",
      "Test loss: 0.04663433410503246\n",
      "Starting epoch 3750\n",
      "Training loss: 0.00998216693396451\n",
      "Test loss: 0.04628786065236286\n",
      "Starting epoch 3751\n",
      "Training loss: 0.009912598389582555\n",
      "Test loss: 0.046016759204643744\n",
      "Starting epoch 3752\n",
      "Training loss: 0.009864290855580667\n",
      "Test loss: 0.046419608786150264\n",
      "Starting epoch 3753\n",
      "Training loss: 0.009918329153270995\n",
      "Test loss: 0.0467608119878504\n",
      "Starting epoch 3754\n",
      "Training loss: 0.009931444839314848\n",
      "Test loss: 0.04686151396621157\n",
      "Starting epoch 3755\n",
      "Training loss: 0.009916442507480989\n",
      "Test loss: 0.04616806173214206\n",
      "Starting epoch 3756\n",
      "Training loss: 0.009851537705933461\n",
      "Test loss: 0.04637973872875726\n",
      "Starting epoch 3757\n",
      "Training loss: 0.009810446074507276\n",
      "Test loss: 0.04650646075606346\n",
      "Starting epoch 3758\n",
      "Training loss: 0.009936798684543273\n",
      "Test loss: 0.046421290961680586\n",
      "Starting epoch 3759\n",
      "Training loss: 0.009880980140850192\n",
      "Test loss: 0.0460174983298337\n",
      "Starting epoch 3760\n",
      "Training loss: 0.009812892765783872\n",
      "Test loss: 0.04641717423995336\n",
      "Starting epoch 3761\n",
      "Training loss: 0.009948064603644316\n",
      "Test loss: 0.04655656673842006\n",
      "Starting epoch 3762\n",
      "Training loss: 0.009943165617888092\n",
      "Test loss: 0.04613249804134722\n",
      "Starting epoch 3763\n",
      "Training loss: 0.009892338260886122\n",
      "Test loss: 0.04597378480765554\n",
      "Starting epoch 3764\n",
      "Training loss: 0.00990285829747798\n",
      "Test loss: 0.04650142085221079\n",
      "Starting epoch 3765\n",
      "Training loss: 0.009825956274862171\n",
      "Test loss: 0.04635760422658037\n",
      "Starting epoch 3766\n",
      "Training loss: 0.010106418556610092\n",
      "Test loss: 0.04636142375292601\n",
      "Starting epoch 3767\n",
      "Training loss: 0.010119958521157015\n",
      "Test loss: 0.04600652472840415\n",
      "Starting epoch 3768\n",
      "Training loss: 0.009825724154161136\n",
      "Test loss: 0.04682943355981951\n",
      "Starting epoch 3769\n",
      "Training loss: 0.009863744764665111\n",
      "Test loss: 0.04666279449507042\n",
      "Starting epoch 3770\n",
      "Training loss: 0.010006707253270462\n",
      "Test loss: 0.04655333194467756\n",
      "Starting epoch 3771\n",
      "Training loss: 0.009875425297889064\n",
      "Test loss: 0.046169213950634\n",
      "Starting epoch 3772\n",
      "Training loss: 0.009919809437066805\n",
      "Test loss: 0.0463820927672916\n",
      "Starting epoch 3773\n",
      "Training loss: 0.009902884313439737\n",
      "Test loss: 0.046712441438878025\n",
      "Starting epoch 3774\n",
      "Training loss: 0.009906775424959229\n",
      "Test loss: 0.04700987454917696\n",
      "Starting epoch 3775\n",
      "Training loss: 0.009843612700456479\n",
      "Test loss: 0.04689861709872881\n",
      "Starting epoch 3776\n",
      "Training loss: 0.009908326336594879\n",
      "Test loss: 0.0463180902103583\n",
      "Starting epoch 3777\n",
      "Training loss: 0.00987099213372977\n",
      "Test loss: 0.04635556456115511\n",
      "Starting epoch 3778\n",
      "Training loss: 0.010080329279919139\n",
      "Test loss: 0.04641253583961063\n",
      "Starting epoch 3779\n",
      "Training loss: 0.009847544843605796\n",
      "Test loss: 0.04699878229035272\n",
      "Starting epoch 3780\n",
      "Training loss: 0.009865385800844333\n",
      "Test loss: 0.04677937117715677\n",
      "Starting epoch 3781\n",
      "Training loss: 0.009888964949450532\n",
      "Test loss: 0.04629754461348057\n",
      "Starting epoch 3782\n",
      "Training loss: 0.009860482982924728\n",
      "Test loss: 0.046542247412381346\n",
      "Starting epoch 3783\n",
      "Training loss: 0.010005131623417627\n",
      "Test loss: 0.04639321372464851\n",
      "Starting epoch 3784\n",
      "Training loss: 0.009837571852153441\n",
      "Test loss: 0.04606035607004607\n",
      "Starting epoch 3785\n",
      "Training loss: 0.009894995034107418\n",
      "Test loss: 0.046224259943873795\n",
      "Starting epoch 3786\n",
      "Training loss: 0.009820175158684372\n",
      "Test loss: 0.04603269323706627\n",
      "Starting epoch 3787\n",
      "Training loss: 0.010004455437425707\n",
      "Test loss: 0.04619288817048073\n",
      "Starting epoch 3788\n",
      "Training loss: 0.009887390510469187\n",
      "Test loss: 0.04683932772389165\n",
      "Starting epoch 3789\n",
      "Training loss: 0.009837588615959784\n",
      "Test loss: 0.04670907074102649\n",
      "Starting epoch 3790\n",
      "Training loss: 0.009904223845386114\n",
      "Test loss: 0.0464425426390436\n",
      "Starting epoch 3791\n",
      "Training loss: 0.009853798896074295\n",
      "Test loss: 0.04638298973441124\n",
      "Starting epoch 3792\n",
      "Training loss: 0.009852984278905587\n",
      "Test loss: 0.046611236615313426\n",
      "Starting epoch 3793\n",
      "Training loss: 0.009927051920504843\n",
      "Test loss: 0.046574488247710245\n",
      "Starting epoch 3794\n",
      "Training loss: 0.009827841065640821\n",
      "Test loss: 0.04654074349889049\n",
      "Starting epoch 3795\n",
      "Training loss: 0.009833332777145456\n",
      "Test loss: 0.046515774809651904\n",
      "Starting epoch 3796\n",
      "Training loss: 0.009904196149990207\n",
      "Test loss: 0.0461278578473462\n",
      "Starting epoch 3797\n",
      "Training loss: 0.009864739279766551\n",
      "Test loss: 0.04628442283029909\n",
      "Starting epoch 3798\n",
      "Training loss: 0.00983855740518355\n",
      "Test loss: 0.04616634754670991\n",
      "Starting epoch 3799\n",
      "Training loss: 0.009899026882208761\n",
      "Test loss: 0.04616602579200709\n",
      "Starting epoch 3800\n",
      "Training loss: 0.010193808523357892\n",
      "Test loss: 0.0461001665227943\n",
      "Starting epoch 3801\n",
      "Training loss: 0.009818757013952146\n",
      "Test loss: 0.04678207580690031\n",
      "Starting epoch 3802\n",
      "Training loss: 0.009900866335899126\n",
      "Test loss: 0.04654604982998636\n",
      "Starting epoch 3803\n",
      "Training loss: 0.009939962875891904\n",
      "Test loss: 0.04626918984232126\n",
      "Starting epoch 3804\n",
      "Training loss: 0.009900588404814729\n",
      "Test loss: 0.04666608213274567\n",
      "Starting epoch 3805\n",
      "Training loss: 0.009928008739943386\n",
      "Test loss: 0.04637983151608043\n",
      "Starting epoch 3806\n",
      "Training loss: 0.009838198540640658\n",
      "Test loss: 0.04665269412928157\n",
      "Starting epoch 3807\n",
      "Training loss: 0.009825696183948731\n",
      "Test loss: 0.046522055787068826\n",
      "Starting epoch 3808\n",
      "Training loss: 0.010016774575485558\n",
      "Test loss: 0.04647448382995747\n",
      "Starting epoch 3809\n",
      "Training loss: 0.010006664000207284\n",
      "Test loss: 0.046133768848247\n",
      "Starting epoch 3810\n",
      "Training loss: 0.010183301983309573\n",
      "Test loss: 0.04660487754477395\n",
      "Starting epoch 3811\n",
      "Training loss: 0.009839799529949173\n",
      "Test loss: 0.046268727354429384\n",
      "Starting epoch 3812\n",
      "Training loss: 0.0098529543849777\n",
      "Test loss: 0.04650207871088275\n",
      "Starting epoch 3813\n",
      "Training loss: 0.009992409314288467\n",
      "Test loss: 0.046515252303194116\n",
      "Starting epoch 3814\n",
      "Training loss: 0.009881020401467065\n",
      "Test loss: 0.04609445744642505\n",
      "Starting epoch 3815\n",
      "Training loss: 0.010050753681141823\n",
      "Test loss: 0.04603694765656083\n",
      "Starting epoch 3816\n",
      "Training loss: 0.010012003577879219\n",
      "Test loss: 0.04675812481178178\n",
      "Starting epoch 3817\n",
      "Training loss: 0.009889666601771215\n",
      "Test loss: 0.04636661666962835\n",
      "Starting epoch 3818\n",
      "Training loss: 0.00988210752972814\n",
      "Test loss: 0.046365050943913286\n",
      "Starting epoch 3819\n",
      "Training loss: 0.00988877156260805\n",
      "Test loss: 0.04639490348873315\n",
      "Starting epoch 3820\n",
      "Training loss: 0.01001367203463785\n",
      "Test loss: 0.04664403903815481\n",
      "Starting epoch 3821\n",
      "Training loss: 0.009826455662240748\n",
      "Test loss: 0.04700964578875789\n",
      "Starting epoch 3822\n",
      "Training loss: 0.009919727282201658\n",
      "Test loss: 0.046777682585848704\n",
      "Starting epoch 3823\n",
      "Training loss: 0.010133684215853448\n",
      "Test loss: 0.046907379671379375\n",
      "Starting epoch 3824\n",
      "Training loss: 0.009850670995770907\n",
      "Test loss: 0.04626115349431833\n",
      "Starting epoch 3825\n",
      "Training loss: 0.009858877008750301\n",
      "Test loss: 0.046355407409093996\n",
      "Starting epoch 3826\n",
      "Training loss: 0.009920619794579803\n",
      "Test loss: 0.046525296099759916\n",
      "Starting epoch 3827\n",
      "Training loss: 0.00994462866458248\n",
      "Test loss: 0.046164011789692774\n",
      "Starting epoch 3828\n",
      "Training loss: 0.009851576821481595\n",
      "Test loss: 0.046498776172046306\n",
      "Starting epoch 3829\n",
      "Training loss: 0.009814061804628764\n",
      "Test loss: 0.04664171487092972\n",
      "Starting epoch 3830\n",
      "Training loss: 0.009983707616319422\n",
      "Test loss: 0.04656485730299243\n",
      "Starting epoch 3831\n",
      "Training loss: 0.009909071272513906\n",
      "Test loss: 0.04605812413824929\n",
      "Starting epoch 3832\n",
      "Training loss: 0.009911944845416507\n",
      "Test loss: 0.046286465117224944\n",
      "Starting epoch 3833\n",
      "Training loss: 0.009848433455116436\n",
      "Test loss: 0.04678538303684305\n",
      "Starting epoch 3834\n",
      "Training loss: 0.009827467338105694\n",
      "Test loss: 0.04664171252537657\n",
      "Starting epoch 3835\n",
      "Training loss: 0.009917275002989613\n",
      "Test loss: 0.046329893861655834\n",
      "Starting epoch 3836\n",
      "Training loss: 0.009873455848361625\n",
      "Test loss: 0.04660448507854232\n",
      "Starting epoch 3837\n",
      "Training loss: 0.009964270456159701\n",
      "Test loss: 0.046306319534778595\n",
      "Starting epoch 3838\n",
      "Training loss: 0.009845402778782805\n",
      "Test loss: 0.046619680882604035\n",
      "Starting epoch 3839\n",
      "Training loss: 0.009999305360996332\n",
      "Test loss: 0.04669087352576079\n",
      "Starting epoch 3840\n",
      "Training loss: 0.0098986595441572\n",
      "Test loss: 0.046341845834696735\n",
      "Starting epoch 3841\n",
      "Training loss: 0.009888655383933763\n",
      "Test loss: 0.045978570356965065\n",
      "Starting epoch 3842\n",
      "Training loss: 0.009843717665090913\n",
      "Test loss: 0.04642032542162471\n",
      "Starting epoch 3843\n",
      "Training loss: 0.009923428220704932\n",
      "Test loss: 0.04644586863341155\n",
      "Starting epoch 3844\n",
      "Training loss: 0.009914237089821549\n",
      "Test loss: 0.0466484802050723\n",
      "Starting epoch 3845\n",
      "Training loss: 0.009855730016334135\n",
      "Test loss: 0.04674766295486026\n",
      "Starting epoch 3846\n",
      "Training loss: 0.009841022264884144\n",
      "Test loss: 0.04666942723647312\n",
      "Starting epoch 3847\n",
      "Training loss: 0.009860032489981319\n",
      "Test loss: 0.046583178004732835\n",
      "Starting epoch 3848\n",
      "Training loss: 0.009941421418649251\n",
      "Test loss: 0.046458182787453686\n",
      "Starting epoch 3849\n",
      "Training loss: 0.009865246377274638\n",
      "Test loss: 0.046195904482845905\n",
      "Starting epoch 3850\n",
      "Training loss: 0.00984429343618819\n",
      "Test loss: 0.04643039846861804\n",
      "Starting epoch 3851\n",
      "Training loss: 0.009898274617849803\n",
      "Test loss: 0.0465215479058248\n",
      "Starting epoch 3852\n",
      "Training loss: 0.009876744836935254\n",
      "Test loss: 0.046264488250017166\n",
      "Starting epoch 3853\n",
      "Training loss: 0.009875143130050331\n",
      "Test loss: 0.04613206328617202\n",
      "Starting epoch 3854\n",
      "Training loss: 0.009915536987121965\n",
      "Test loss: 0.04599530691349948\n",
      "Starting epoch 3855\n",
      "Training loss: 0.009962364649552791\n",
      "Test loss: 0.04601204622950819\n",
      "Starting epoch 3856\n",
      "Training loss: 0.009902250296512588\n",
      "Test loss: 0.046745191500694665\n",
      "Starting epoch 3857\n",
      "Training loss: 0.009842307871726692\n",
      "Test loss: 0.04661286774056929\n",
      "Starting epoch 3858\n",
      "Training loss: 0.009863612089367186\n",
      "Test loss: 0.04622672415441937\n",
      "Starting epoch 3859\n",
      "Training loss: 0.010318282777901555\n",
      "Test loss: 0.04656765885927059\n",
      "Starting epoch 3860\n",
      "Training loss: 0.010054727191807793\n",
      "Test loss: 0.04596352342654158\n",
      "Starting epoch 3861\n",
      "Training loss: 0.009894146416030947\n",
      "Test loss: 0.0466678821378284\n",
      "Starting epoch 3862\n",
      "Training loss: 0.009814631392354847\n",
      "Test loss: 0.04646486540635427\n",
      "Starting epoch 3863\n",
      "Training loss: 0.009898376895389596\n",
      "Test loss: 0.04645839181763155\n",
      "Starting epoch 3864\n",
      "Training loss: 0.009967751785746364\n",
      "Test loss: 0.04621624629254694\n",
      "Starting epoch 3865\n",
      "Training loss: 0.010024778361691803\n",
      "Test loss: 0.04660725427998437\n",
      "Starting epoch 3866\n",
      "Training loss: 0.009960770668064962\n",
      "Test loss: 0.046175874217792796\n",
      "Starting epoch 3867\n",
      "Training loss: 0.009819136856154341\n",
      "Test loss: 0.04626574767408548\n",
      "Starting epoch 3868\n",
      "Training loss: 0.009913874820607608\n",
      "Test loss: 0.04633602720719797\n",
      "Starting epoch 3869\n",
      "Training loss: 0.010033323719608979\n",
      "Test loss: 0.046444251857422014\n",
      "Starting epoch 3870\n",
      "Training loss: 0.009910997934639454\n",
      "Test loss: 0.046118308479587235\n",
      "Starting epoch 3871\n",
      "Training loss: 0.009860776670154978\n",
      "Test loss: 0.04624167043301794\n",
      "Starting epoch 3872\n",
      "Training loss: 0.009875785941105397\n",
      "Test loss: 0.04617022743655576\n",
      "Starting epoch 3873\n",
      "Training loss: 0.00981515751327159\n",
      "Test loss: 0.0461978308028645\n",
      "Starting epoch 3874\n",
      "Training loss: 0.009974356725445537\n",
      "Test loss: 0.046409558366846154\n",
      "Starting epoch 3875\n",
      "Training loss: 0.009857360380472706\n",
      "Test loss: 0.0460881986827762\n",
      "Starting epoch 3876\n",
      "Training loss: 0.009946412040439785\n",
      "Test loss: 0.04614912007969839\n",
      "Starting epoch 3877\n",
      "Training loss: 0.009922371184850325\n",
      "Test loss: 0.04674737983279758\n",
      "Starting epoch 3878\n",
      "Training loss: 0.009799937335926979\n",
      "Test loss: 0.047122492006531465\n",
      "Starting epoch 3879\n",
      "Training loss: 0.009886343780233234\n",
      "Test loss: 0.04691837107141813\n",
      "Starting epoch 3880\n",
      "Training loss: 0.00990597166304217\n",
      "Test loss: 0.04630449248684777\n",
      "Starting epoch 3881\n",
      "Training loss: 0.010085684980158924\n",
      "Test loss: 0.04675123702596735\n",
      "Starting epoch 3882\n",
      "Training loss: 0.009812211617827415\n",
      "Test loss: 0.04615892925196224\n",
      "Starting epoch 3883\n",
      "Training loss: 0.009874471432727868\n",
      "Test loss: 0.04601884074509144\n",
      "Starting epoch 3884\n",
      "Training loss: 0.009966706093706068\n",
      "Test loss: 0.046314286137068714\n",
      "Starting epoch 3885\n",
      "Training loss: 0.009855990130148951\n",
      "Test loss: 0.046163585450914174\n",
      "Starting epoch 3886\n",
      "Training loss: 0.009884902276098728\n",
      "Test loss: 0.0461743812042254\n",
      "Starting epoch 3887\n",
      "Training loss: 0.009978463102254222\n",
      "Test loss: 0.046461730574568115\n",
      "Starting epoch 3888\n",
      "Training loss: 0.010019114485285321\n",
      "Test loss: 0.04607531773271384\n",
      "Starting epoch 3889\n",
      "Training loss: 0.009919538987098171\n",
      "Test loss: 0.04588790058537766\n",
      "Starting epoch 3890\n",
      "Training loss: 0.009858311779919218\n",
      "Test loss: 0.04663113966860153\n",
      "Starting epoch 3891\n",
      "Training loss: 0.009885179245325386\n",
      "Test loss: 0.046656576847588574\n",
      "Starting epoch 3892\n",
      "Training loss: 0.009839542118496582\n",
      "Test loss: 0.04621621607630341\n",
      "Starting epoch 3893\n",
      "Training loss: 0.009914845533546855\n",
      "Test loss: 0.04638177570369509\n",
      "Starting epoch 3894\n",
      "Training loss: 0.009813956122417918\n",
      "Test loss: 0.046484102529508096\n",
      "Starting epoch 3895\n",
      "Training loss: 0.00983637456827965\n",
      "Test loss: 0.04646243072218365\n",
      "Starting epoch 3896\n",
      "Training loss: 0.009872318825638686\n",
      "Test loss: 0.04637967864120448\n",
      "Starting epoch 3897\n",
      "Training loss: 0.009825545164649604\n",
      "Test loss: 0.046278912712026526\n",
      "Starting epoch 3898\n",
      "Training loss: 0.009875878966489776\n",
      "Test loss: 0.04624435388379627\n",
      "Starting epoch 3899\n",
      "Training loss: 0.009960330045614087\n",
      "Test loss: 0.04645713549797182\n",
      "Starting epoch 3900\n",
      "Training loss: 0.009822572657807928\n",
      "Test loss: 0.04608291925655471\n",
      "Starting epoch 3901\n",
      "Training loss: 0.009851423336467782\n",
      "Test loss: 0.04627902253910347\n",
      "Starting epoch 3902\n",
      "Training loss: 0.009979454571472817\n",
      "Test loss: 0.04685753804666025\n",
      "Starting epoch 3903\n",
      "Training loss: 0.009818506961474653\n",
      "Test loss: 0.04635239668466427\n",
      "Starting epoch 3904\n",
      "Training loss: 0.009957478641242277\n",
      "Test loss: 0.04645069647166464\n",
      "Starting epoch 3905\n",
      "Training loss: 0.010133553754355087\n",
      "Test loss: 0.04619011006973408\n",
      "Starting epoch 3906\n",
      "Training loss: 0.009809922762825841\n",
      "Test loss: 0.04671810346621054\n",
      "Starting epoch 3907\n",
      "Training loss: 0.009873736527610998\n",
      "Test loss: 0.04662020476879897\n",
      "Starting epoch 3908\n",
      "Training loss: 0.010212967508151883\n",
      "Test loss: 0.04634755725661913\n",
      "Starting epoch 3909\n",
      "Training loss: 0.010032813217429841\n",
      "Test loss: 0.047020694999783126\n",
      "Starting epoch 3910\n",
      "Training loss: 0.010037086995654419\n",
      "Test loss: 0.047268077041263935\n",
      "Starting epoch 3911\n",
      "Training loss: 0.00991209187224263\n",
      "Test loss: 0.04621600732207298\n",
      "Starting epoch 3912\n",
      "Training loss: 0.009956129673929488\n",
      "Test loss: 0.04593182769086626\n",
      "Starting epoch 3913\n",
      "Training loss: 0.009883763543406471\n",
      "Test loss: 0.046722608583944815\n",
      "Starting epoch 3914\n",
      "Training loss: 0.009828427684355955\n",
      "Test loss: 0.04688986025198742\n",
      "Starting epoch 3915\n",
      "Training loss: 0.009921464595760479\n",
      "Test loss: 0.04687004232848132\n",
      "Starting epoch 3916\n",
      "Training loss: 0.00985386233288245\n",
      "Test loss: 0.04699578301774131\n",
      "Starting epoch 3917\n",
      "Training loss: 0.009870427141546225\n",
      "Test loss: 0.046507850703265935\n",
      "Starting epoch 3918\n",
      "Training loss: 0.009907532605479975\n",
      "Test loss: 0.04666249964524199\n",
      "Starting epoch 3919\n",
      "Training loss: 0.00984908506029942\n",
      "Test loss: 0.046505813107446385\n",
      "Starting epoch 3920\n",
      "Training loss: 0.009840833801837241\n",
      "Test loss: 0.04642304709112203\n",
      "Starting epoch 3921\n",
      "Training loss: 0.00985034478858846\n",
      "Test loss: 0.04630881720395\n",
      "Starting epoch 3922\n",
      "Training loss: 0.010040313478742466\n",
      "Test loss: 0.04663541312846872\n",
      "Starting epoch 3923\n",
      "Training loss: 0.009856973316703663\n",
      "Test loss: 0.046222096102105245\n",
      "Starting epoch 3924\n",
      "Training loss: 0.009952137979571937\n",
      "Test loss: 0.046121810735375794\n",
      "Starting epoch 3925\n",
      "Training loss: 0.009973065683343371\n",
      "Test loss: 0.04679553031369492\n",
      "Starting epoch 3926\n",
      "Training loss: 0.009819797606619655\n",
      "Test loss: 0.04699528175923559\n",
      "Starting epoch 3927\n",
      "Training loss: 0.009963555429436144\n",
      "Test loss: 0.04689481509504495\n",
      "Starting epoch 3928\n",
      "Training loss: 0.009895832140426167\n",
      "Test loss: 0.04696439344573904\n",
      "Starting epoch 3929\n",
      "Training loss: 0.009821066388585528\n",
      "Test loss: 0.04692095545706926\n",
      "Starting epoch 3930\n",
      "Training loss: 0.00989951709376984\n",
      "Test loss: 0.04679569802074521\n",
      "Starting epoch 3931\n",
      "Training loss: 0.009901899401648123\n",
      "Test loss: 0.04663949652954384\n",
      "Starting epoch 3932\n",
      "Training loss: 0.00980447567083308\n",
      "Test loss: 0.04619122172395388\n",
      "Starting epoch 3933\n",
      "Training loss: 0.009805829447434574\n",
      "Test loss: 0.04601944638071237\n",
      "Starting epoch 3934\n",
      "Training loss: 0.009854030734325041\n",
      "Test loss: 0.046194254938099116\n",
      "Starting epoch 3935\n",
      "Training loss: 0.009849566599873245\n",
      "Test loss: 0.04653152602690237\n",
      "Starting epoch 3936\n",
      "Training loss: 0.009815320158835317\n",
      "Test loss: 0.04640983955727683\n",
      "Starting epoch 3937\n",
      "Training loss: 0.00987554859125712\n",
      "Test loss: 0.04656235694333359\n",
      "Starting epoch 3938\n",
      "Training loss: 0.009813383007880117\n",
      "Test loss: 0.046821721036125113\n",
      "Starting epoch 3939\n",
      "Training loss: 0.009951446770278156\n",
      "Test loss: 0.04678455767808137\n",
      "Starting epoch 3940\n",
      "Training loss: 0.009864281046158467\n",
      "Test loss: 0.04709949061550476\n",
      "Starting epoch 3941\n",
      "Training loss: 0.009866430775308218\n",
      "Test loss: 0.04648573172313196\n",
      "Starting epoch 3942\n",
      "Training loss: 0.009863289699080537\n",
      "Test loss: 0.046348894083941425\n",
      "Starting epoch 3943\n",
      "Training loss: 0.009937746725121483\n",
      "Test loss: 0.0466076350874371\n",
      "Starting epoch 3944\n",
      "Training loss: 0.009941662394548537\n",
      "Test loss: 0.04650195301682861\n",
      "Starting epoch 3945\n",
      "Training loss: 0.009798306406887829\n",
      "Test loss: 0.04624809711067765\n",
      "Starting epoch 3946\n",
      "Training loss: 0.009877127595245838\n",
      "Test loss: 0.04634317672914929\n",
      "Starting epoch 3947\n",
      "Training loss: 0.00987264416256889\n",
      "Test loss: 0.046772819564298344\n",
      "Starting epoch 3948\n",
      "Training loss: 0.009837204620974963\n",
      "Test loss: 0.046903971720624854\n",
      "Starting epoch 3949\n",
      "Training loss: 0.00987025469419409\n",
      "Test loss: 0.046860166997821244\n",
      "Starting epoch 3950\n",
      "Training loss: 0.009874333535916493\n",
      "Test loss: 0.04658474400639534\n",
      "Starting epoch 3951\n",
      "Training loss: 0.009817520049629642\n",
      "Test loss: 0.04678166726673091\n",
      "Starting epoch 3952\n",
      "Training loss: 0.009875334722951788\n",
      "Test loss: 0.046829985937586534\n",
      "Starting epoch 3953\n",
      "Training loss: 0.00987820797523514\n",
      "Test loss: 0.04681012834663744\n",
      "Starting epoch 3954\n",
      "Training loss: 0.009850465379594291\n",
      "Test loss: 0.046681482482839515\n",
      "Starting epoch 3955\n",
      "Training loss: 0.009848156837044193\n",
      "Test loss: 0.04652457953327232\n",
      "Starting epoch 3956\n",
      "Training loss: 0.009822751853431835\n",
      "Test loss: 0.04661686959917898\n",
      "Starting epoch 3957\n",
      "Training loss: 0.009874209975365733\n",
      "Test loss: 0.04662708220658479\n",
      "Starting epoch 3958\n",
      "Training loss: 0.00997568092873839\n",
      "Test loss: 0.04649093084865146\n",
      "Starting epoch 3959\n",
      "Training loss: 0.010016155551325102\n",
      "Test loss: 0.04692036893080782\n",
      "Starting epoch 3960\n",
      "Training loss: 0.009850629070987467\n",
      "Test loss: 0.04707684202326669\n",
      "Starting epoch 3961\n",
      "Training loss: 0.009816895666547486\n",
      "Test loss: 0.04690707971652349\n",
      "Starting epoch 3962\n",
      "Training loss: 0.00989643659931226\n",
      "Test loss: 0.04664159455785045\n",
      "Starting epoch 3963\n",
      "Training loss: 0.009853384709443714\n",
      "Test loss: 0.04692583931265054\n",
      "Starting epoch 3964\n",
      "Training loss: 0.00982975342967471\n",
      "Test loss: 0.046490123840393846\n",
      "Starting epoch 3965\n",
      "Training loss: 0.009954178934825248\n",
      "Test loss: 0.04654160969787174\n",
      "Starting epoch 3966\n",
      "Training loss: 0.009831418695508456\n",
      "Test loss: 0.04626915893620915\n",
      "Starting epoch 3967\n",
      "Training loss: 0.009869710420120935\n",
      "Test loss: 0.046683659570084676\n",
      "Starting epoch 3968\n",
      "Training loss: 0.00983181833976605\n",
      "Test loss: 0.046698186132642955\n",
      "Starting epoch 3969\n",
      "Training loss: 0.009823310540103521\n",
      "Test loss: 0.04652648695089199\n",
      "Starting epoch 3970\n",
      "Training loss: 0.009844472784487928\n",
      "Test loss: 0.04621387369654797\n",
      "Starting epoch 3971\n",
      "Training loss: 0.009798841276129738\n",
      "Test loss: 0.046363910729134525\n",
      "Starting epoch 3972\n",
      "Training loss: 0.009959759175411014\n",
      "Test loss: 0.046688095976909004\n",
      "Starting epoch 3973\n",
      "Training loss: 0.010090252842570915\n",
      "Test loss: 0.046396208030206186\n",
      "Starting epoch 3974\n",
      "Training loss: 0.009895107281379035\n",
      "Test loss: 0.04693518372045623\n",
      "Starting epoch 3975\n",
      "Training loss: 0.009857014951403023\n",
      "Test loss: 0.04699538082436279\n",
      "Starting epoch 3976\n",
      "Training loss: 0.009892047848552465\n",
      "Test loss: 0.04697092532835625\n",
      "Starting epoch 3977\n",
      "Training loss: 0.009892754943766555\n",
      "Test loss: 0.046652840795340364\n",
      "Starting epoch 3978\n",
      "Training loss: 0.009790567612489228\n",
      "Test loss: 0.04663979607047858\n",
      "Starting epoch 3979\n",
      "Training loss: 0.009816394469960302\n",
      "Test loss: 0.04662708565592766\n",
      "Starting epoch 3980\n",
      "Training loss: 0.00986734537987924\n",
      "Test loss: 0.046592809121917794\n",
      "Starting epoch 3981\n",
      "Training loss: 0.010004566020530755\n",
      "Test loss: 0.04632995236251089\n",
      "Starting epoch 3982\n",
      "Training loss: 0.009943867667165936\n",
      "Test loss: 0.04682030142457397\n",
      "Starting epoch 3983\n",
      "Training loss: 0.01000069829894871\n",
      "Test loss: 0.046905145325042585\n",
      "Starting epoch 3984\n",
      "Training loss: 0.009897024981433251\n",
      "Test loss: 0.04701689492773126\n",
      "Starting epoch 3985\n",
      "Training loss: 0.00997757160517036\n",
      "Test loss: 0.046329010691907674\n",
      "Starting epoch 3986\n",
      "Training loss: 0.01002479758357904\n",
      "Test loss: 0.046795874972034385\n",
      "Starting epoch 3987\n",
      "Training loss: 0.009849344899297738\n",
      "Test loss: 0.04632399051829621\n",
      "Starting epoch 3988\n",
      "Training loss: 0.009885024401496668\n",
      "Test loss: 0.04623931501474646\n",
      "Starting epoch 3989\n",
      "Training loss: 0.010011254443374813\n",
      "Test loss: 0.04648849450879627\n",
      "Starting epoch 3990\n",
      "Training loss: 0.009867148275380253\n",
      "Test loss: 0.046863998389906354\n",
      "Starting epoch 3991\n",
      "Training loss: 0.009945671150430303\n",
      "Test loss: 0.046979869681376\n",
      "Starting epoch 3992\n",
      "Training loss: 0.009943197786685874\n",
      "Test loss: 0.046525692774189845\n",
      "Starting epoch 3993\n",
      "Training loss: 0.009807285425237944\n",
      "Test loss: 0.04612499006368496\n",
      "Starting epoch 3994\n",
      "Training loss: 0.009851347197030411\n",
      "Test loss: 0.04625349105508239\n",
      "Starting epoch 3995\n",
      "Training loss: 0.009865170879075761\n",
      "Test loss: 0.04642464910392408\n",
      "Starting epoch 3996\n",
      "Training loss: 0.009960078786997522\n",
      "Test loss: 0.04660864188163369\n",
      "Starting epoch 3997\n",
      "Training loss: 0.009977853276812639\n",
      "Test loss: 0.04631243025263151\n",
      "Starting epoch 3998\n",
      "Training loss: 0.009807853501473294\n",
      "Test loss: 0.04683933600231453\n",
      "Starting epoch 3999\n",
      "Training loss: 0.009851775376400987\n",
      "Test loss: 0.04675655135953868\n",
      "Starting epoch 4000\n",
      "Training loss: 0.010038790247235143\n",
      "Test loss: 0.0468063836020452\n",
      "Starting epoch 4001\n",
      "Training loss: 0.009793500942712436\n",
      "Test loss: 0.04634807245047004\n",
      "Starting epoch 4002\n",
      "Training loss: 0.009865039730536157\n",
      "Test loss: 0.04646073281764984\n",
      "Starting epoch 4003\n",
      "Training loss: 0.009888301542425742\n",
      "Test loss: 0.04674004611593706\n",
      "Starting epoch 4004\n",
      "Training loss: 0.009813602219839564\n",
      "Test loss: 0.04694823079087116\n",
      "Starting epoch 4005\n",
      "Training loss: 0.009874809853976867\n",
      "Test loss: 0.04663313262992435\n",
      "Starting epoch 4006\n",
      "Training loss: 0.010235712359674642\n",
      "Test loss: 0.046740353038465535\n",
      "Starting epoch 4007\n",
      "Training loss: 0.010014562455357099\n",
      "Test loss: 0.046264265560441546\n",
      "Starting epoch 4008\n",
      "Training loss: 0.00980988615116135\n",
      "Test loss: 0.04667047618163957\n",
      "Starting epoch 4009\n",
      "Training loss: 0.009908534296345516\n",
      "Test loss: 0.04669255777089684\n",
      "Starting epoch 4010\n",
      "Training loss: 0.009841329173841437\n",
      "Test loss: 0.046633309995134674\n",
      "Starting epoch 4011\n",
      "Training loss: 0.00989118935998346\n",
      "Test loss: 0.046981081366539\n",
      "Starting epoch 4012\n",
      "Training loss: 0.009905080295732765\n",
      "Test loss: 0.04703008390411183\n",
      "Starting epoch 4013\n",
      "Training loss: 0.009936402872449061\n",
      "Test loss: 0.04633469024190196\n",
      "Starting epoch 4014\n",
      "Training loss: 0.009978964619460653\n",
      "Test loss: 0.04607944811383883\n",
      "Starting epoch 4015\n",
      "Training loss: 0.00999425780638808\n",
      "Test loss: 0.04683458860273714\n",
      "Starting epoch 4016\n",
      "Training loss: 0.009863455199682321\n",
      "Test loss: 0.04729766978157891\n",
      "Starting epoch 4017\n",
      "Training loss: 0.00985937956415239\n",
      "Test loss: 0.04718245496904409\n",
      "Starting epoch 4018\n",
      "Training loss: 0.009961227687900184\n",
      "Test loss: 0.0468923922766138\n",
      "Starting epoch 4019\n",
      "Training loss: 0.010007834367331912\n",
      "Test loss: 0.0464860079465089\n",
      "Starting epoch 4020\n",
      "Training loss: 0.009856942323509787\n",
      "Test loss: 0.046763805465565786\n",
      "Starting epoch 4021\n",
      "Training loss: 0.009877081365003938\n",
      "Test loss: 0.04685068033911564\n",
      "Starting epoch 4022\n",
      "Training loss: 0.009896156668174462\n",
      "Test loss: 0.046780874883687054\n",
      "Starting epoch 4023\n",
      "Training loss: 0.009935466603177493\n",
      "Test loss: 0.046957351819232655\n",
      "Starting epoch 4024\n",
      "Training loss: 0.009844171035973752\n",
      "Test loss: 0.04634076329293074\n",
      "Starting epoch 4025\n",
      "Training loss: 0.00992790333254904\n",
      "Test loss: 0.046349990009157745\n",
      "Starting epoch 4026\n",
      "Training loss: 0.009823137405710142\n",
      "Test loss: 0.046084109555791924\n",
      "Starting epoch 4027\n",
      "Training loss: 0.009834435798960631\n",
      "Test loss: 0.046161832494868174\n",
      "Starting epoch 4028\n",
      "Training loss: 0.009928252059416692\n",
      "Test loss: 0.04648115444514486\n",
      "Starting epoch 4029\n",
      "Training loss: 0.009828071964935202\n",
      "Test loss: 0.04699148375678946\n",
      "Starting epoch 4030\n",
      "Training loss: 0.00986256371023225\n",
      "Test loss: 0.04696463158837071\n",
      "Starting epoch 4031\n",
      "Training loss: 0.009970808600182415\n",
      "Test loss: 0.046798919362050516\n",
      "Starting epoch 4032\n",
      "Training loss: 0.00986342939746673\n",
      "Test loss: 0.046626747068431645\n",
      "Starting epoch 4033\n",
      "Training loss: 0.009898020412589683\n",
      "Test loss: 0.04634670775245737\n",
      "Starting epoch 4034\n",
      "Training loss: 0.00985202237536184\n",
      "Test loss: 0.0466376852106165\n",
      "Starting epoch 4035\n",
      "Training loss: 0.010074414099093343\n",
      "Test loss: 0.04669386796929218\n",
      "Starting epoch 4036\n",
      "Training loss: 0.009915758886176055\n",
      "Test loss: 0.04722160487263291\n",
      "Starting epoch 4037\n",
      "Training loss: 0.00981211195104435\n",
      "Test loss: 0.046574349894567775\n",
      "Starting epoch 4038\n",
      "Training loss: 0.009847170123677761\n",
      "Test loss: 0.04641268181580084\n",
      "Starting epoch 4039\n",
      "Training loss: 0.010002028823021005\n",
      "Test loss: 0.046778727874711705\n",
      "Starting epoch 4040\n",
      "Training loss: 0.009865156573350312\n",
      "Test loss: 0.04697547604640325\n",
      "Starting epoch 4041\n",
      "Training loss: 0.010230781632612963\n",
      "Test loss: 0.046553999875430706\n",
      "Starting epoch 4042\n",
      "Training loss: 0.009826332651322982\n",
      "Test loss: 0.0472265355013035\n",
      "Starting epoch 4043\n",
      "Training loss: 0.00986442395837092\n",
      "Test loss: 0.047025367341659685\n",
      "Starting epoch 4044\n",
      "Training loss: 0.009887231330646843\n",
      "Test loss: 0.04664552701568162\n",
      "Starting epoch 4045\n",
      "Training loss: 0.009828208457128923\n",
      "Test loss: 0.04691184408686779\n",
      "Starting epoch 4046\n",
      "Training loss: 0.009884583855383709\n",
      "Test loss: 0.04673857931737547\n",
      "Starting epoch 4047\n",
      "Training loss: 0.009882827655824482\n",
      "Test loss: 0.0465775512986713\n",
      "Starting epoch 4048\n",
      "Training loss: 0.009791779438736008\n",
      "Test loss: 0.046331895860256975\n",
      "Starting epoch 4049\n",
      "Training loss: 0.009799863959922165\n",
      "Test loss: 0.04638622563194345\n",
      "Starting epoch 4050\n",
      "Training loss: 0.009830299047295188\n",
      "Test loss: 0.046721664567788444\n",
      "Starting epoch 4051\n",
      "Training loss: 0.009803411123327544\n",
      "Test loss: 0.046653317770472276\n",
      "Starting epoch 4052\n",
      "Training loss: 0.009819115443368916\n",
      "Test loss: 0.0465973565975825\n",
      "Starting epoch 4053\n",
      "Training loss: 0.009793495163932199\n",
      "Test loss: 0.0467380523957588\n",
      "Starting epoch 4054\n",
      "Training loss: 0.009803951733180733\n",
      "Test loss: 0.04653305036050302\n",
      "Starting epoch 4055\n",
      "Training loss: 0.009828862734138966\n",
      "Test loss: 0.046574261591390324\n",
      "Starting epoch 4056\n",
      "Training loss: 0.009843431245230261\n",
      "Test loss: 0.04642119720854141\n",
      "Starting epoch 4057\n",
      "Training loss: 0.009785828073738052\n",
      "Test loss: 0.04666828019199548\n",
      "Starting epoch 4058\n",
      "Training loss: 0.009805934335731093\n",
      "Test loss: 0.04675589557047243\n",
      "Starting epoch 4059\n",
      "Training loss: 0.009838730936534092\n",
      "Test loss: 0.04661631570370109\n",
      "Starting epoch 4060\n",
      "Training loss: 0.009815377847398402\n",
      "Test loss: 0.04681383218202326\n",
      "Starting epoch 4061\n",
      "Training loss: 0.009801596448924697\n",
      "Test loss: 0.04677569673017219\n",
      "Starting epoch 4062\n",
      "Training loss: 0.01009364824619938\n",
      "Test loss: 0.04663116388298847\n",
      "Starting epoch 4063\n",
      "Training loss: 0.00982551236988091\n",
      "Test loss: 0.04603279037056146\n",
      "Starting epoch 4064\n",
      "Training loss: 0.009933108143264154\n",
      "Test loss: 0.046217446525891624\n",
      "Starting epoch 4065\n",
      "Training loss: 0.009840206823265944\n",
      "Test loss: 0.04619364771578047\n",
      "Starting epoch 4066\n",
      "Training loss: 0.010188695333409504\n",
      "Test loss: 0.04661863869814961\n",
      "Starting epoch 4067\n",
      "Training loss: 0.009954835196621105\n",
      "Test loss: 0.046174209426950524\n",
      "Starting epoch 4068\n",
      "Training loss: 0.009933763870694598\n",
      "Test loss: 0.04584837884262756\n",
      "Starting epoch 4069\n",
      "Training loss: 0.009875163817625553\n",
      "Test loss: 0.04664614920814832\n",
      "Starting epoch 4070\n",
      "Training loss: 0.009853859477844394\n",
      "Test loss: 0.04681750097208553\n",
      "Starting epoch 4071\n",
      "Training loss: 0.009841429115441001\n",
      "Test loss: 0.046627484261989594\n",
      "Starting epoch 4072\n",
      "Training loss: 0.009901292179329474\n",
      "Test loss: 0.046703636853231326\n",
      "Starting epoch 4073\n",
      "Training loss: 0.009891787490456319\n",
      "Test loss: 0.046554535489391397\n",
      "Starting epoch 4074\n",
      "Training loss: 0.009962382466822375\n",
      "Test loss: 0.04655670029697595\n",
      "Starting epoch 4075\n",
      "Training loss: 0.009932121781052137\n",
      "Test loss: 0.04714506857649044\n",
      "Starting epoch 4076\n",
      "Training loss: 0.009902099910818163\n",
      "Test loss: 0.047199667465907556\n",
      "Starting epoch 4077\n",
      "Training loss: 0.009867081968266456\n",
      "Test loss: 0.04663970473187941\n",
      "Starting epoch 4078\n",
      "Training loss: 0.009917126250926589\n",
      "Test loss: 0.04628959905218195\n",
      "Starting epoch 4079\n",
      "Training loss: 0.00984434818574151\n",
      "Test loss: 0.046124324616458684\n",
      "Starting epoch 4080\n",
      "Training loss: 0.00980976301810292\n",
      "Test loss: 0.046632774036239694\n",
      "Starting epoch 4081\n",
      "Training loss: 0.010064161810107896\n",
      "Test loss: 0.04693197307211382\n",
      "Starting epoch 4082\n",
      "Training loss: 0.009903913516490186\n",
      "Test loss: 0.04721272046919222\n",
      "Starting epoch 4083\n",
      "Training loss: 0.009800747357553145\n",
      "Test loss: 0.047173538831649\n",
      "Starting epoch 4084\n",
      "Training loss: 0.009824248565147157\n",
      "Test loss: 0.046832073065969676\n",
      "Starting epoch 4085\n",
      "Training loss: 0.009805351465207631\n",
      "Test loss: 0.046697364223224146\n",
      "Starting epoch 4086\n",
      "Training loss: 0.009883786933343918\n",
      "Test loss: 0.046616807166073054\n",
      "Starting epoch 4087\n",
      "Training loss: 0.00989424898365482\n",
      "Test loss: 0.047090544537813576\n",
      "Starting epoch 4088\n",
      "Training loss: 0.009916894801999213\n",
      "Test loss: 0.0473093682969058\n",
      "Starting epoch 4089\n",
      "Training loss: 0.009827554057978216\n",
      "Test loss: 0.0466414183654167\n",
      "Starting epoch 4090\n",
      "Training loss: 0.0098304597080731\n",
      "Test loss: 0.04670794059832891\n",
      "Starting epoch 4091\n",
      "Training loss: 0.009960524753102513\n",
      "Test loss: 0.04640801133656943\n",
      "Starting epoch 4092\n",
      "Training loss: 0.009898088704489294\n",
      "Test loss: 0.04674918218343346\n",
      "Starting epoch 4093\n",
      "Training loss: 0.00987283529744285\n",
      "Test loss: 0.047058848181256545\n",
      "Starting epoch 4094\n",
      "Training loss: 0.009887083051878898\n",
      "Test loss: 0.04687686347299152\n",
      "Starting epoch 4095\n",
      "Training loss: 0.009955931775516173\n",
      "Test loss: 0.046732932674112146\n",
      "Starting epoch 4096\n",
      "Training loss: 0.009881212788282847\n",
      "Test loss: 0.046315609098032666\n",
      "Starting epoch 4097\n",
      "Training loss: 0.00987321295638065\n",
      "Test loss: 0.04670035080225379\n",
      "Starting epoch 4098\n",
      "Training loss: 0.009825125947350362\n",
      "Test loss: 0.046982353967097074\n",
      "Starting epoch 4099\n",
      "Training loss: 0.00998149749624436\n",
      "Test loss: 0.046828569637404546\n",
      "Starting epoch 4100\n",
      "Training loss: 0.009980744529576575\n",
      "Test loss: 0.047283923598351305\n",
      "Starting epoch 4101\n",
      "Training loss: 0.009841919785029575\n",
      "Test loss: 0.04658040017993362\n",
      "Starting epoch 4102\n",
      "Training loss: 0.009868287328691756\n",
      "Test loss: 0.04630820949872335\n",
      "Starting epoch 4103\n",
      "Training loss: 0.009838355476128274\n",
      "Test loss: 0.04660086085398992\n",
      "Starting epoch 4104\n",
      "Training loss: 0.00996452708896555\n",
      "Test loss: 0.04697840026131383\n",
      "Starting epoch 4105\n",
      "Training loss: 0.009892080437208786\n",
      "Test loss: 0.04717852575359521\n",
      "Starting epoch 4106\n",
      "Training loss: 0.009964681612175019\n",
      "Test loss: 0.04705387850602468\n",
      "Starting epoch 4107\n",
      "Training loss: 0.009823534958308837\n",
      "Test loss: 0.047197529287249955\n",
      "Starting epoch 4108\n",
      "Training loss: 0.009825750589981431\n",
      "Test loss: 0.046972304306648394\n",
      "Starting epoch 4109\n",
      "Training loss: 0.009836228869733264\n",
      "Test loss: 0.04675330207855613\n",
      "Starting epoch 4110\n",
      "Training loss: 0.009970690810778102\n",
      "Test loss: 0.046823268135388695\n",
      "Starting epoch 4111\n",
      "Training loss: 0.009997567772621015\n",
      "Test loss: 0.047073507888449564\n",
      "Starting epoch 4112\n",
      "Training loss: 0.009888085750404929\n",
      "Test loss: 0.046470793033087696\n",
      "Starting epoch 4113\n",
      "Training loss: 0.009844986920351864\n",
      "Test loss: 0.04668215855404183\n",
      "Starting epoch 4114\n",
      "Training loss: 0.009857327341422683\n",
      "Test loss: 0.046967695708628056\n",
      "Starting epoch 4115\n",
      "Training loss: 0.009816841710908492\n",
      "Test loss: 0.046585199457627756\n",
      "Starting epoch 4116\n",
      "Training loss: 0.009890421736435812\n",
      "Test loss: 0.0466934570490762\n",
      "Starting epoch 4117\n",
      "Training loss: 0.00986260227614739\n",
      "Test loss: 0.04629976702509103\n",
      "Starting epoch 4118\n",
      "Training loss: 0.009966480125841058\n",
      "Test loss: 0.04661554415468817\n",
      "Starting epoch 4119\n",
      "Training loss: 0.009955365424517726\n",
      "Test loss: 0.046962765493878615\n",
      "Starting epoch 4120\n",
      "Training loss: 0.009811835607788602\n",
      "Test loss: 0.046563232524527445\n",
      "Starting epoch 4121\n",
      "Training loss: 0.00978525018044671\n",
      "Test loss: 0.04660572442743513\n",
      "Starting epoch 4122\n",
      "Training loss: 0.009810546092444757\n",
      "Test loss: 0.04680869493771483\n",
      "Starting epoch 4123\n",
      "Training loss: 0.009923244093651654\n",
      "Test loss: 0.04678039859842371\n",
      "Starting epoch 4124\n",
      "Training loss: 0.009799708589361827\n",
      "Test loss: 0.04688042181509512\n",
      "Starting epoch 4125\n",
      "Training loss: 0.009826994302575706\n",
      "Test loss: 0.04662553262379435\n",
      "Starting epoch 4126\n",
      "Training loss: 0.009925973113076608\n",
      "Test loss: 0.046690547079951676\n",
      "Starting epoch 4127\n",
      "Training loss: 0.009828691798277566\n",
      "Test loss: 0.046948924419228676\n",
      "Starting epoch 4128\n",
      "Training loss: 0.00987224175487874\n",
      "Test loss: 0.04682492147441263\n",
      "Starting epoch 4129\n",
      "Training loss: 0.009866005221961951\n",
      "Test loss: 0.04673566062141348\n",
      "Starting epoch 4130\n",
      "Training loss: 0.009873389327501664\n",
      "Test loss: 0.04640701854670489\n",
      "Starting epoch 4131\n",
      "Training loss: 0.009800573062823444\n",
      "Test loss: 0.046288574735323586\n",
      "Starting epoch 4132\n",
      "Training loss: 0.010075415591480301\n",
      "Test loss: 0.04655955593895029\n",
      "Starting epoch 4133\n",
      "Training loss: 0.009917076448070222\n",
      "Test loss: 0.04624579570911549\n",
      "Starting epoch 4134\n",
      "Training loss: 0.009965922469731237\n",
      "Test loss: 0.04687035525286639\n",
      "Starting epoch 4135\n",
      "Training loss: 0.009835481552071259\n",
      "Test loss: 0.04661981692468679\n",
      "Starting epoch 4136\n",
      "Training loss: 0.009808520420042217\n",
      "Test loss: 0.04691602262081923\n",
      "Starting epoch 4137\n",
      "Training loss: 0.009954435330983556\n",
      "Test loss: 0.04702016435287617\n",
      "Starting epoch 4138\n",
      "Training loss: 0.01005486293588994\n",
      "Test loss: 0.047018720802885515\n",
      "Starting epoch 4139\n",
      "Training loss: 0.009865589775755758\n",
      "Test loss: 0.04649747066475727\n",
      "Starting epoch 4140\n",
      "Training loss: 0.009828126905333311\n",
      "Test loss: 0.046489097040008615\n",
      "Starting epoch 4141\n",
      "Training loss: 0.00980969239026308\n",
      "Test loss: 0.046936339526264755\n",
      "Starting epoch 4142\n",
      "Training loss: 0.009819183376480321\n",
      "Test loss: 0.046674099371389104\n",
      "Starting epoch 4143\n",
      "Training loss: 0.009777859173959395\n",
      "Test loss: 0.046808554618446914\n",
      "Starting epoch 4144\n",
      "Training loss: 0.010033706218370648\n",
      "Test loss: 0.0469119358393881\n",
      "Starting epoch 4145\n",
      "Training loss: 0.009825416810077722\n",
      "Test loss: 0.04736099523250704\n",
      "Starting epoch 4146\n",
      "Training loss: 0.009905757245103845\n",
      "Test loss: 0.046866088139790076\n",
      "Starting epoch 4147\n",
      "Training loss: 0.009826842741277373\n",
      "Test loss: 0.046854790300130844\n",
      "Starting epoch 4148\n",
      "Training loss: 0.01012552091393803\n",
      "Test loss: 0.046578024962434066\n",
      "Starting epoch 4149\n",
      "Training loss: 0.00985823043423598\n",
      "Test loss: 0.04594796551046548\n",
      "Starting epoch 4150\n",
      "Training loss: 0.009879668273764556\n",
      "Test loss: 0.04635498382978969\n",
      "Starting epoch 4151\n",
      "Training loss: 0.009847404832234148\n",
      "Test loss: 0.046495463285181254\n",
      "Starting epoch 4152\n",
      "Training loss: 0.009799873212077578\n",
      "Test loss: 0.04675894106427828\n",
      "Starting epoch 4153\n",
      "Training loss: 0.009905870209951869\n",
      "Test loss: 0.04670871187139441\n",
      "Starting epoch 4154\n",
      "Training loss: 0.009811416207278361\n",
      "Test loss: 0.04697694215509626\n",
      "Starting epoch 4155\n",
      "Training loss: 0.009849306470790848\n",
      "Test loss: 0.046681646464599505\n",
      "Starting epoch 4156\n",
      "Training loss: 0.009821320441169817\n",
      "Test loss: 0.04664675132543952\n",
      "Starting epoch 4157\n",
      "Training loss: 0.009899599080691572\n",
      "Test loss: 0.04650418025751909\n",
      "Starting epoch 4158\n",
      "Training loss: 0.00979518601823537\n",
      "Test loss: 0.04678852456035437\n",
      "Starting epoch 4159\n",
      "Training loss: 0.009894346673285863\n",
      "Test loss: 0.046962162686718836\n",
      "Starting epoch 4160\n",
      "Training loss: 0.009882627940568768\n",
      "Test loss: 0.04654839193379438\n",
      "Starting epoch 4161\n",
      "Training loss: 0.009911818567235938\n",
      "Test loss: 0.04688667368005823\n",
      "Starting epoch 4162\n",
      "Training loss: 0.009816684210520298\n",
      "Test loss: 0.04639870166364643\n",
      "Starting epoch 4163\n",
      "Training loss: 0.009880730622738112\n",
      "Test loss: 0.04638381329951463\n",
      "Starting epoch 4164\n",
      "Training loss: 0.009831127008331603\n",
      "Test loss: 0.0468684779825034\n",
      "Starting epoch 4165\n",
      "Training loss: 0.010007416478312407\n",
      "Test loss: 0.04689470540594171\n",
      "Starting epoch 4166\n",
      "Training loss: 0.009844087644434367\n",
      "Test loss: 0.04718535317590943\n",
      "Starting epoch 4167\n",
      "Training loss: 0.009838307841268719\n",
      "Test loss: 0.046636097685054494\n",
      "Starting epoch 4168\n",
      "Training loss: 0.00982756355441496\n",
      "Test loss: 0.04643186223175791\n",
      "Starting epoch 4169\n",
      "Training loss: 0.009870702751957979\n",
      "Test loss: 0.04643924368752374\n",
      "Starting epoch 4170\n",
      "Training loss: 0.009893584233082708\n",
      "Test loss: 0.04628624256562303\n",
      "Starting epoch 4171\n",
      "Training loss: 0.009949991586389111\n",
      "Test loss: 0.04628179415508553\n",
      "Starting epoch 4172\n",
      "Training loss: 0.009832851252839213\n",
      "Test loss: 0.04692792630305997\n",
      "Starting epoch 4173\n",
      "Training loss: 0.009841210979846169\n",
      "Test loss: 0.04683638957363588\n",
      "Starting epoch 4174\n",
      "Training loss: 0.01003265486205699\n",
      "Test loss: 0.04657739456053133\n",
      "Starting epoch 4175\n",
      "Training loss: 0.00990836070392464\n",
      "Test loss: 0.045971321424952256\n",
      "Starting epoch 4176\n",
      "Training loss: 0.009879417977005731\n",
      "Test loss: 0.046092008550961815\n",
      "Starting epoch 4177\n",
      "Training loss: 0.009824421134639959\n",
      "Test loss: 0.04624672247855752\n",
      "Starting epoch 4178\n",
      "Training loss: 0.009837021577920093\n",
      "Test loss: 0.046639774132657935\n",
      "Starting epoch 4179\n",
      "Training loss: 0.009794814008303353\n",
      "Test loss: 0.046669859163187166\n",
      "Starting epoch 4180\n",
      "Training loss: 0.009935066806244069\n",
      "Test loss: 0.04665332632484259\n",
      "Starting epoch 4181\n",
      "Training loss: 0.00982276360947089\n",
      "Test loss: 0.04633139715426498\n",
      "Starting epoch 4182\n",
      "Training loss: 0.009882608672878782\n",
      "Test loss: 0.04657646792906302\n",
      "Starting epoch 4183\n",
      "Training loss: 0.00979221899245606\n",
      "Test loss: 0.046435131380955376\n",
      "Starting epoch 4184\n",
      "Training loss: 0.009843196597744207\n",
      "Test loss: 0.04645499945790679\n",
      "Starting epoch 4185\n",
      "Training loss: 0.009802387340269128\n",
      "Test loss: 0.04647377885326191\n",
      "Starting epoch 4186\n",
      "Training loss: 0.00983472957779638\n",
      "Test loss: 0.04666185730861293\n",
      "Starting epoch 4187\n",
      "Training loss: 0.009817734040075639\n",
      "Test loss: 0.04646716570412671\n",
      "Starting epoch 4188\n",
      "Training loss: 0.00990258949640833\n",
      "Test loss: 0.046520242674483195\n",
      "Starting epoch 4189\n",
      "Training loss: 0.00993981517729212\n",
      "Test loss: 0.04688208067306766\n",
      "Starting epoch 4190\n",
      "Training loss: 0.009847368144231742\n",
      "Test loss: 0.046705274394264924\n",
      "Starting epoch 4191\n",
      "Training loss: 0.01007850181128158\n",
      "Test loss: 0.04672492750816875\n",
      "Starting epoch 4192\n",
      "Training loss: 0.00994110925764334\n",
      "Test loss: 0.04629371970616005\n",
      "Starting epoch 4193\n",
      "Training loss: 0.009880004282735411\n",
      "Test loss: 0.04675915575137845\n",
      "Starting epoch 4194\n",
      "Training loss: 0.009885906333439663\n",
      "Test loss: 0.04649508758275597\n",
      "Starting epoch 4195\n",
      "Training loss: 0.010031159661832403\n",
      "Test loss: 0.04688180327691414\n",
      "Starting epoch 4196\n",
      "Training loss: 0.009810563703601966\n",
      "Test loss: 0.04731222324901157\n",
      "Starting epoch 4197\n",
      "Training loss: 0.009928492340640943\n",
      "Test loss: 0.04683867303861512\n",
      "Starting epoch 4198\n",
      "Training loss: 0.009829301158057862\n",
      "Test loss: 0.046829248744028586\n",
      "Starting epoch 4199\n",
      "Training loss: 0.009846091942220438\n",
      "Test loss: 0.04665413871407509\n",
      "Starting epoch 4200\n",
      "Training loss: 0.009887273805063278\n",
      "Test loss: 0.04667330188331781\n",
      "Starting epoch 4201\n",
      "Training loss: 0.009819652137087017\n",
      "Test loss: 0.04699504554823593\n",
      "Starting epoch 4202\n",
      "Training loss: 0.009994519630172213\n",
      "Test loss: 0.04676749626243556\n",
      "Starting epoch 4203\n",
      "Training loss: 0.010008610265909648\n",
      "Test loss: 0.04660829336003021\n",
      "Starting epoch 4204\n",
      "Training loss: 0.00994630400702113\n",
      "Test loss: 0.047291343134862406\n",
      "Starting epoch 4205\n",
      "Training loss: 0.009812833176406681\n",
      "Test loss: 0.046665627509355545\n",
      "Starting epoch 4206\n",
      "Training loss: 0.009860607367924979\n",
      "Test loss: 0.04666833082834879\n",
      "Starting epoch 4207\n",
      "Training loss: 0.009789017364993447\n",
      "Test loss: 0.04696922362954528\n",
      "Starting epoch 4208\n",
      "Training loss: 0.009829093305180307\n",
      "Test loss: 0.04680337425735262\n",
      "Starting epoch 4209\n",
      "Training loss: 0.009996252882553906\n",
      "Test loss: 0.04689190246992641\n",
      "Starting epoch 4210\n",
      "Training loss: 0.009874958560237141\n",
      "Test loss: 0.047376547284700254\n",
      "Starting epoch 4211\n",
      "Training loss: 0.00982111855791729\n",
      "Test loss: 0.04680912458786258\n",
      "Starting epoch 4212\n",
      "Training loss: 0.009918713300931649\n",
      "Test loss: 0.04655438240755488\n",
      "Starting epoch 4213\n",
      "Training loss: 0.009934022503553843\n",
      "Test loss: 0.046771064676620344\n",
      "Starting epoch 4214\n",
      "Training loss: 0.009848906215829928\n",
      "Test loss: 0.04694524076249865\n",
      "Starting epoch 4215\n",
      "Training loss: 0.009823277745334829\n",
      "Test loss: 0.046572552924906765\n",
      "Starting epoch 4216\n",
      "Training loss: 0.010033973706428145\n",
      "Test loss: 0.046607893236257414\n",
      "Starting epoch 4217\n",
      "Training loss: 0.009797315021640942\n",
      "Test loss: 0.04705937724146578\n",
      "Starting epoch 4218\n",
      "Training loss: 0.00985199031343714\n",
      "Test loss: 0.04699233850395238\n",
      "Starting epoch 4219\n",
      "Training loss: 0.009851999451085681\n",
      "Test loss: 0.04657168465631979\n",
      "Starting epoch 4220\n",
      "Training loss: 0.009816494785615654\n",
      "Test loss: 0.046371345304780535\n",
      "Starting epoch 4221\n",
      "Training loss: 0.009932152178810268\n",
      "Test loss: 0.046407082566508544\n",
      "Starting epoch 4222\n",
      "Training loss: 0.00989512731244818\n",
      "Test loss: 0.04604169740169137\n",
      "Starting epoch 4223\n",
      "Training loss: 0.009809333678396022\n",
      "Test loss: 0.04639480131919737\n",
      "Starting epoch 4224\n",
      "Training loss: 0.009884620665526781\n",
      "Test loss: 0.046459774038305986\n",
      "Starting epoch 4225\n",
      "Training loss: 0.009851768200636888\n",
      "Test loss: 0.04666688914100329\n",
      "Starting epoch 4226\n",
      "Training loss: 0.009828058911151573\n",
      "Test loss: 0.0464400464185962\n",
      "Starting epoch 4227\n",
      "Training loss: 0.009788607292976535\n",
      "Test loss: 0.04655613777814088\n",
      "Starting epoch 4228\n",
      "Training loss: 0.009834031879779746\n",
      "Test loss: 0.0464423482340795\n",
      "Starting epoch 4229\n",
      "Training loss: 0.010050784460589534\n",
      "Test loss: 0.04677547404059657\n",
      "Starting epoch 4230\n",
      "Training loss: 0.00983632149816048\n",
      "Test loss: 0.04616720212140569\n",
      "Starting epoch 4231\n",
      "Training loss: 0.009878340253575903\n",
      "Test loss: 0.0465542606457516\n",
      "Starting epoch 4232\n",
      "Training loss: 0.009943366920972457\n",
      "Test loss: 0.04643195019000106\n",
      "Starting epoch 4233\n",
      "Training loss: 0.009972498477360264\n",
      "Test loss: 0.046927062484125294\n",
      "Starting epoch 4234\n",
      "Training loss: 0.009821009165683731\n",
      "Test loss: 0.04660659255804839\n",
      "Starting epoch 4235\n",
      "Training loss: 0.009874215792314928\n",
      "Test loss: 0.04647221719777143\n",
      "Starting epoch 4236\n",
      "Training loss: 0.009861438626759365\n",
      "Test loss: 0.04634011150510223\n",
      "Starting epoch 4237\n",
      "Training loss: 0.009827988939817811\n",
      "Test loss: 0.04679858656945052\n",
      "Starting epoch 4238\n",
      "Training loss: 0.00981369068021657\n",
      "Test loss: 0.046803228143188685\n",
      "Starting epoch 4239\n",
      "Training loss: 0.00988417379863438\n",
      "Test loss: 0.04665359413182294\n",
      "Starting epoch 4240\n",
      "Training loss: 0.00982908953408726\n",
      "Test loss: 0.04692285597600319\n",
      "Starting epoch 4241\n",
      "Training loss: 0.009836043338062333\n",
      "Test loss: 0.04710513077400349\n",
      "Starting epoch 4242\n",
      "Training loss: 0.009881049532015792\n",
      "Test loss: 0.04703069754220821\n",
      "Starting epoch 4243\n",
      "Training loss: 0.009890319336755354\n",
      "Test loss: 0.046581010851595134\n",
      "Starting epoch 4244\n",
      "Training loss: 0.00992933321805274\n",
      "Test loss: 0.04650030395498982\n",
      "Starting epoch 4245\n",
      "Training loss: 0.009832933178690613\n",
      "Test loss: 0.046840584802406805\n",
      "Starting epoch 4246\n",
      "Training loss: 0.009860302313979044\n",
      "Test loss: 0.0470611696579942\n",
      "Starting epoch 4247\n",
      "Training loss: 0.009792897697599208\n",
      "Test loss: 0.0466744975635299\n",
      "Starting epoch 4248\n",
      "Training loss: 0.009828157402330735\n",
      "Test loss: 0.046701912526731136\n",
      "Starting epoch 4249\n",
      "Training loss: 0.009802105607556516\n",
      "Test loss: 0.046693244879996335\n",
      "Starting epoch 4250\n",
      "Training loss: 0.009855213529262387\n",
      "Test loss: 0.046635063641049246\n",
      "Starting epoch 4251\n",
      "Training loss: 0.009897091761842126\n",
      "Test loss: 0.046445660982970836\n",
      "Starting epoch 4252\n",
      "Training loss: 0.009887046287538575\n",
      "Test loss: 0.04657544499194181\n",
      "Starting epoch 4253\n",
      "Training loss: 0.00982314294784284\n",
      "Test loss: 0.04689763872711747\n",
      "Starting epoch 4254\n",
      "Training loss: 0.009857455619656648\n",
      "Test loss: 0.046900485193839776\n",
      "Starting epoch 4255\n",
      "Training loss: 0.009845889631475582\n",
      "Test loss: 0.04685873193321405\n",
      "Starting epoch 4256\n",
      "Training loss: 0.009797234668350611\n",
      "Test loss: 0.04689426264829106\n",
      "Starting epoch 4257\n",
      "Training loss: 0.010022243805473945\n",
      "Test loss: 0.04669167266951667\n",
      "Starting epoch 4258\n",
      "Training loss: 0.009885352150705016\n",
      "Test loss: 0.04622976233561834\n",
      "Starting epoch 4259\n",
      "Training loss: 0.0098125052745225\n",
      "Test loss: 0.046283805949820414\n",
      "Starting epoch 4260\n",
      "Training loss: 0.009905587637522181\n",
      "Test loss: 0.04658465349563846\n",
      "Starting epoch 4261\n",
      "Training loss: 0.009901491680839022\n",
      "Test loss: 0.046464452175078566\n",
      "Starting epoch 4262\n",
      "Training loss: 0.009861018310194133\n",
      "Test loss: 0.04697282639918504\n",
      "Starting epoch 4263\n",
      "Training loss: 0.009837610418068582\n",
      "Test loss: 0.04712585249432811\n",
      "Starting epoch 4264\n",
      "Training loss: 0.009878798845972194\n",
      "Test loss: 0.04694513948979201\n",
      "Starting epoch 4265\n",
      "Training loss: 0.009960475072386812\n",
      "Test loss: 0.046584413283401065\n",
      "Starting epoch 4266\n",
      "Training loss: 0.009950464530313601\n",
      "Test loss: 0.04697696285115348\n",
      "Starting epoch 4267\n",
      "Training loss: 0.009849449161623345\n",
      "Test loss: 0.04656314256566542\n",
      "Starting epoch 4268\n",
      "Training loss: 0.009996153612728001\n",
      "Test loss: 0.04675385459429688\n",
      "Starting epoch 4269\n",
      "Training loss: 0.010130147571812888\n",
      "Test loss: 0.04716237882773081\n",
      "Starting epoch 4270\n",
      "Training loss: 0.009859659640332226\n",
      "Test loss: 0.04665470771767475\n",
      "Starting epoch 4271\n",
      "Training loss: 0.009872481150583166\n",
      "Test loss: 0.04631979239207727\n",
      "Starting epoch 4272\n",
      "Training loss: 0.00994488674780873\n",
      "Test loss: 0.04682056026326285\n",
      "Starting epoch 4273\n",
      "Training loss: 0.009800292001884491\n",
      "Test loss: 0.04720937591735964\n",
      "Starting epoch 4274\n",
      "Training loss: 0.009895019981338352\n",
      "Test loss: 0.047261565923690796\n",
      "Starting epoch 4275\n",
      "Training loss: 0.009891152855192051\n",
      "Test loss: 0.04674854529676614\n",
      "Starting epoch 4276\n",
      "Training loss: 0.00986823035007129\n",
      "Test loss: 0.04685035444520138\n",
      "Starting epoch 4277\n",
      "Training loss: 0.009817289570193798\n",
      "Test loss: 0.04691764988281109\n",
      "Starting epoch 4278\n",
      "Training loss: 0.009872365803992162\n",
      "Test loss: 0.04663360215447567\n",
      "Starting epoch 4279\n",
      "Training loss: 0.009989857001871359\n",
      "Test loss: 0.04677719139942416\n",
      "Starting epoch 4280\n",
      "Training loss: 0.009958967703898421\n",
      "Test loss: 0.046279954689520376\n",
      "Starting epoch 4281\n",
      "Training loss: 0.00990179362783178\n",
      "Test loss: 0.04696797869271702\n",
      "Starting epoch 4282\n",
      "Training loss: 0.009826937705644818\n",
      "Test loss: 0.046865036366162474\n",
      "Starting epoch 4283\n",
      "Training loss: 0.010130769710560313\n",
      "Test loss: 0.04667430667689553\n",
      "Starting epoch 4284\n",
      "Training loss: 0.009898568076066306\n",
      "Test loss: 0.047353029527046064\n",
      "Starting epoch 4285\n",
      "Training loss: 0.009887466085005979\n",
      "Test loss: 0.04714350033275507\n",
      "Starting epoch 4286\n",
      "Training loss: 0.009822920651831588\n",
      "Test loss: 0.04659814497938863\n",
      "Starting epoch 4287\n",
      "Training loss: 0.009855999351769198\n",
      "Test loss: 0.04642901155683729\n",
      "Starting epoch 4288\n",
      "Training loss: 0.010083235998744847\n",
      "Test loss: 0.04660275799256784\n",
      "Starting epoch 4289\n",
      "Training loss: 0.00985959069956033\n",
      "Test loss: 0.04719633788422302\n",
      "Starting epoch 4290\n",
      "Training loss: 0.0098365596266555\n",
      "Test loss: 0.04692025248099257\n",
      "Starting epoch 4291\n",
      "Training loss: 0.009785624846938203\n",
      "Test loss: 0.04661423588792483\n",
      "Starting epoch 4292\n",
      "Training loss: 0.009916489760650963\n",
      "Test loss: 0.046473034416083935\n",
      "Starting epoch 4293\n",
      "Training loss: 0.009888849389113363\n",
      "Test loss: 0.04673250143726667\n",
      "Starting epoch 4294\n",
      "Training loss: 0.009864200242474431\n",
      "Test loss: 0.04656789189687482\n",
      "Starting epoch 4295\n",
      "Training loss: 0.010146055889667058\n",
      "Test loss: 0.04641762265452632\n",
      "Starting epoch 4296\n",
      "Training loss: 0.00992293909314226\n",
      "Test loss: 0.04708217594910551\n",
      "Starting epoch 4297\n",
      "Training loss: 0.009874493021090499\n",
      "Test loss: 0.046844585281279355\n",
      "Starting epoch 4298\n",
      "Training loss: 0.009847523400285205\n",
      "Test loss: 0.04692057464961653\n",
      "Starting epoch 4299\n",
      "Training loss: 0.00995772873952252\n",
      "Test loss: 0.046837767103204024\n",
      "Starting epoch 4300\n",
      "Training loss: 0.009900950093860508\n",
      "Test loss: 0.04646052254570855\n",
      "Starting epoch 4301\n",
      "Training loss: 0.009795492408094837\n",
      "Test loss: 0.04671029981087755\n",
      "Starting epoch 4302\n",
      "Training loss: 0.010125860098566189\n",
      "Test loss: 0.04674947986172305\n",
      "Starting epoch 4303\n",
      "Training loss: 0.00983073985295706\n",
      "Test loss: 0.0460016131401062\n",
      "Starting epoch 4304\n",
      "Training loss: 0.009932422353962406\n",
      "Test loss: 0.04650494815022857\n",
      "Starting epoch 4305\n",
      "Training loss: 0.009904572602788934\n",
      "Test loss: 0.04660357631467007\n",
      "Starting epoch 4306\n",
      "Training loss: 0.009873713458292797\n",
      "Test loss: 0.04700344980314926\n",
      "Starting epoch 4307\n",
      "Training loss: 0.009842575512460022\n",
      "Test loss: 0.04707421803915942\n",
      "Starting epoch 4308\n",
      "Training loss: 0.009897580416109717\n",
      "Test loss: 0.04703904536587221\n",
      "Starting epoch 4309\n",
      "Training loss: 0.009819626243265926\n",
      "Test loss: 0.04701253275076548\n",
      "Starting epoch 4310\n",
      "Training loss: 0.009820984493269295\n",
      "Test loss: 0.04672237802986746\n",
      "Starting epoch 4311\n",
      "Training loss: 0.009956469347120309\n",
      "Test loss: 0.046680281283678834\n",
      "Starting epoch 4312\n",
      "Training loss: 0.00984009851503079\n",
      "Test loss: 0.046976531821268576\n",
      "Starting epoch 4313\n",
      "Training loss: 0.009892401010653034\n",
      "Test loss: 0.04715580051695859\n",
      "Starting epoch 4314\n",
      "Training loss: 0.009821060159411586\n",
      "Test loss: 0.046615116850093556\n",
      "Starting epoch 4315\n",
      "Training loss: 0.01008524753336535\n",
      "Test loss: 0.04646490072762525\n",
      "Starting epoch 4316\n",
      "Training loss: 0.009803970497040475\n",
      "Test loss: 0.04702498998354982\n",
      "Starting epoch 4317\n",
      "Training loss: 0.010065339688883453\n",
      "Test loss: 0.046857991290313226\n",
      "Starting epoch 4318\n",
      "Training loss: 0.009900070177238495\n",
      "Test loss: 0.046092339135982374\n",
      "Starting epoch 4319\n",
      "Training loss: 0.009874620444339807\n",
      "Test loss: 0.04659274482616672\n",
      "Starting epoch 4320\n",
      "Training loss: 0.009872187188536417\n",
      "Test loss: 0.047022826693676134\n",
      "Starting epoch 4321\n",
      "Training loss: 0.009833823904761525\n",
      "Test loss: 0.0467734193360364\n",
      "Starting epoch 4322\n",
      "Training loss: 0.009903552270204317\n",
      "Test loss: 0.04674786177498323\n",
      "Starting epoch 4323\n",
      "Training loss: 0.009929670143078585\n",
      "Test loss: 0.04686519572580302\n",
      "Starting epoch 4324\n",
      "Training loss: 0.009826791961295683\n",
      "Test loss: 0.04728665961711495\n",
      "Starting epoch 4325\n",
      "Training loss: 0.009953992258085579\n",
      "Test loss: 0.04742100448520095\n",
      "Starting epoch 4326\n",
      "Training loss: 0.010447860054183201\n",
      "Test loss: 0.04678924547301398\n",
      "Starting epoch 4327\n",
      "Training loss: 0.009832592826092341\n",
      "Test loss: 0.04746031223071946\n",
      "Starting epoch 4328\n",
      "Training loss: 0.009843355617256926\n",
      "Test loss: 0.04734908582435714\n",
      "Starting epoch 4329\n",
      "Training loss: 0.010124100105012537\n",
      "Test loss: 0.04709815578880133\n",
      "Starting epoch 4330\n",
      "Training loss: 0.01027687090891795\n",
      "Test loss: 0.047291834321286946\n",
      "Starting epoch 4331\n",
      "Training loss: 0.009775896617745767\n",
      "Test loss: 0.04623266192222083\n",
      "Starting epoch 4332\n",
      "Training loss: 0.009931845590472221\n",
      "Test loss: 0.04645345242763007\n",
      "Starting epoch 4333\n",
      "Training loss: 0.009831094366238743\n",
      "Test loss: 0.04635800669590632\n",
      "Starting epoch 4334\n",
      "Training loss: 0.010001496732479236\n",
      "Test loss: 0.0465485903399962\n",
      "Starting epoch 4335\n",
      "Training loss: 0.0099717022576293\n",
      "Test loss: 0.046373830694291324\n",
      "Starting epoch 4336\n",
      "Training loss: 0.009828986233619393\n",
      "Test loss: 0.04707089218276518\n",
      "Starting epoch 4337\n",
      "Training loss: 0.00983748282687586\n",
      "Test loss: 0.0468368459906843\n",
      "Starting epoch 4338\n",
      "Training loss: 0.010107962170340976\n",
      "Test loss: 0.0471745690813771\n",
      "Starting epoch 4339\n",
      "Training loss: 0.009912628329313193\n",
      "Test loss: 0.04761665521396531\n",
      "Starting epoch 4340\n",
      "Training loss: 0.009849099182813872\n",
      "Test loss: 0.047547439113259315\n",
      "Starting epoch 4341\n",
      "Training loss: 0.010005437601052347\n",
      "Test loss: 0.04708630619225679\n",
      "Starting epoch 4342\n",
      "Training loss: 0.009761395726780423\n",
      "Test loss: 0.04629688406432116\n",
      "Starting epoch 4343\n",
      "Training loss: 0.00980940578719143\n",
      "Test loss: 0.04635627553970725\n",
      "Starting epoch 4344\n",
      "Training loss: 0.009866635620471884\n",
      "Test loss: 0.04662539492602701\n",
      "Starting epoch 4345\n",
      "Training loss: 0.009820966553859046\n",
      "Test loss: 0.046666029978681495\n",
      "Starting epoch 4346\n",
      "Training loss: 0.009995039567717762\n",
      "Test loss: 0.046677656609702994\n",
      "Starting epoch 4347\n",
      "Training loss: 0.009972602701516913\n",
      "Test loss: 0.047342939095364675\n",
      "Starting epoch 4348\n",
      "Training loss: 0.009823787728416139\n",
      "Test loss: 0.046799602883833426\n",
      "Starting epoch 4349\n",
      "Training loss: 0.009803461842238903\n",
      "Test loss: 0.04658245391867779\n",
      "Starting epoch 4350\n",
      "Training loss: 0.009836131218271177\n",
      "Test loss: 0.046682361237428804\n",
      "Starting epoch 4351\n",
      "Training loss: 0.009875425152847023\n",
      "Test loss: 0.04675581775329731\n",
      "Starting epoch 4352\n",
      "Training loss: 0.009971091447428602\n",
      "Test loss: 0.046643271766327044\n",
      "Starting epoch 4353\n",
      "Training loss: 0.009866716034832548\n",
      "Test loss: 0.0463333059516218\n",
      "Starting epoch 4354\n",
      "Training loss: 0.009845331005874227\n",
      "Test loss: 0.04656964554278939\n",
      "Starting epoch 4355\n",
      "Training loss: 0.009834928010575106\n",
      "Test loss: 0.046710504563870256\n",
      "Starting epoch 4356\n",
      "Training loss: 0.009957801840710835\n",
      "Test loss: 0.04679438747741558\n",
      "Starting epoch 4357\n",
      "Training loss: 0.009845573042870545\n",
      "Test loss: 0.04663473547056869\n",
      "Starting epoch 4358\n",
      "Training loss: 0.00984115901662678\n",
      "Test loss: 0.04663381256439068\n",
      "Starting epoch 4359\n",
      "Training loss: 0.00987438578158617\n",
      "Test loss: 0.04704714442292849\n",
      "Starting epoch 4360\n",
      "Training loss: 0.009793336564278016\n",
      "Test loss: 0.047231977322587264\n",
      "Starting epoch 4361\n",
      "Training loss: 0.009823769636330058\n",
      "Test loss: 0.0468972489858667\n",
      "Starting epoch 4362\n",
      "Training loss: 0.009888884260273371\n",
      "Test loss: 0.046569139455203655\n",
      "Starting epoch 4363\n",
      "Training loss: 0.00995937139406556\n",
      "Test loss: 0.04693594381765083\n",
      "Starting epoch 4364\n",
      "Training loss: 0.009897607760351212\n",
      "Test loss: 0.04659445363062399\n",
      "Starting epoch 4365\n",
      "Training loss: 0.009842032643004518\n",
      "Test loss: 0.04676017234170878\n",
      "Starting epoch 4366\n",
      "Training loss: 0.010150951379146731\n",
      "Test loss: 0.04705393424740544\n",
      "Starting epoch 4367\n",
      "Training loss: 0.009907473474130278\n",
      "Test loss: 0.047599754123776046\n",
      "Starting epoch 4368\n",
      "Training loss: 0.009797789935083663\n",
      "Test loss: 0.046987518185266745\n",
      "Starting epoch 4369\n",
      "Training loss: 0.009792507061215698\n",
      "Test loss: 0.04673161661183393\n",
      "Starting epoch 4370\n",
      "Training loss: 0.009784883743182558\n",
      "Test loss: 0.04690795819516535\n",
      "Starting epoch 4371\n",
      "Training loss: 0.009897181443625787\n",
      "Test loss: 0.04685542925640389\n",
      "Starting epoch 4372\n",
      "Training loss: 0.009781598937926723\n",
      "Test loss: 0.047054953459236354\n",
      "Starting epoch 4373\n",
      "Training loss: 0.009835787468635645\n",
      "Test loss: 0.04685091254887758\n",
      "Starting epoch 4374\n",
      "Training loss: 0.010077994484759744\n",
      "Test loss: 0.046872543515982454\n",
      "Starting epoch 4375\n",
      "Training loss: 0.009786436219745483\n",
      "Test loss: 0.04731342500006711\n",
      "Starting epoch 4376\n",
      "Training loss: 0.010041033604838809\n",
      "Test loss: 0.04703883881922121\n",
      "Starting epoch 4377\n",
      "Training loss: 0.00979353872234704\n",
      "Test loss: 0.0462287410541817\n",
      "Starting epoch 4378\n",
      "Training loss: 0.010010180796389698\n",
      "Test loss: 0.0462812600588357\n",
      "Starting epoch 4379\n",
      "Training loss: 0.00981209109552571\n",
      "Test loss: 0.047059799923940944\n",
      "Starting epoch 4380\n",
      "Training loss: 0.00978342347518831\n",
      "Test loss: 0.04682306283050113\n",
      "Starting epoch 4381\n",
      "Training loss: 0.009862713882180511\n",
      "Test loss: 0.04685933349861039\n",
      "Starting epoch 4382\n",
      "Training loss: 0.009849642159142455\n",
      "Test loss: 0.04641032315514706\n",
      "Starting epoch 4383\n",
      "Training loss: 0.010117630871226553\n",
      "Test loss: 0.04646487271896115\n",
      "Starting epoch 4384\n",
      "Training loss: 0.009864069231343075\n",
      "Test loss: 0.04612517957058218\n",
      "Starting epoch 4385\n",
      "Training loss: 0.009915341928479125\n",
      "Test loss: 0.046545868946446314\n",
      "Starting epoch 4386\n",
      "Training loss: 0.009925818818880886\n",
      "Test loss: 0.0466043441383927\n",
      "Starting epoch 4387\n",
      "Training loss: 0.009854638826895933\n",
      "Test loss: 0.04634085476950363\n",
      "Starting epoch 4388\n",
      "Training loss: 0.009966776874221739\n",
      "Test loss: 0.04678318973768641\n",
      "Starting epoch 4389\n",
      "Training loss: 0.009791205591354215\n",
      "Test loss: 0.04723570316478058\n",
      "Starting epoch 4390\n",
      "Training loss: 0.00986241909568427\n",
      "Test loss: 0.04693535011675623\n",
      "Starting epoch 4391\n",
      "Training loss: 0.009832827862901766\n",
      "Test loss: 0.046541669992385085\n",
      "Starting epoch 4392\n",
      "Training loss: 0.009932791982151445\n",
      "Test loss: 0.046497256529551965\n",
      "Starting epoch 4393\n",
      "Training loss: 0.009805295830134486\n",
      "Test loss: 0.04613531891394545\n",
      "Starting epoch 4394\n",
      "Training loss: 0.00978088910218145\n",
      "Test loss: 0.046573331786526576\n",
      "Starting epoch 4395\n",
      "Training loss: 0.009801784118057275\n",
      "Test loss: 0.0467693281394464\n",
      "Starting epoch 4396\n",
      "Training loss: 0.009803111741288763\n",
      "Test loss: 0.046974797905595216\n",
      "Starting epoch 4397\n",
      "Training loss: 0.009809941572488332\n",
      "Test loss: 0.046737848332634675\n",
      "Starting epoch 4398\n",
      "Training loss: 0.009806293078133316\n",
      "Test loss: 0.04651786787090478\n",
      "Starting epoch 4399\n",
      "Training loss: 0.009825851050678824\n",
      "Test loss: 0.04653006226376251\n",
      "Starting epoch 4400\n",
      "Training loss: 0.009780977791572203\n",
      "Test loss: 0.04672854021191597\n",
      "Starting epoch 4401\n",
      "Training loss: 0.00990699455294697\n",
      "Test loss: 0.04689554828736517\n",
      "Starting epoch 4402\n",
      "Training loss: 0.010147050755922912\n",
      "Test loss: 0.04649786209618604\n",
      "Starting epoch 4403\n",
      "Training loss: 0.009866496639662102\n",
      "Test loss: 0.047247618712760786\n",
      "Starting epoch 4404\n",
      "Training loss: 0.009946228661498085\n",
      "Test loss: 0.04680716977627189\n",
      "Starting epoch 4405\n",
      "Training loss: 0.009949526505269965\n",
      "Test loss: 0.046524512822981236\n",
      "Starting epoch 4406\n",
      "Training loss: 0.009834117118696698\n",
      "Test loss: 0.046166938488130214\n",
      "Starting epoch 4407\n",
      "Training loss: 0.00983147507869318\n",
      "Test loss: 0.04639958369511145\n",
      "Starting epoch 4408\n",
      "Training loss: 0.009877777108769924\n",
      "Test loss: 0.046539711869425245\n",
      "Starting epoch 4409\n",
      "Training loss: 0.009859612822288373\n",
      "Test loss: 0.046724145887074645\n",
      "Starting epoch 4410\n",
      "Training loss: 0.009932249647061357\n",
      "Test loss: 0.04664733247072608\n",
      "Starting epoch 4411\n",
      "Training loss: 0.00984610152262889\n",
      "Test loss: 0.047249419476698945\n",
      "Starting epoch 4412\n",
      "Training loss: 0.009828588329866285\n",
      "Test loss: 0.046769927359289594\n",
      "Starting epoch 4413\n",
      "Training loss: 0.009852412339971691\n",
      "Test loss: 0.04682124461288805\n",
      "Starting epoch 4414\n",
      "Training loss: 0.009814915628951103\n",
      "Test loss: 0.04695346882497823\n",
      "Starting epoch 4415\n",
      "Training loss: 0.009860258442578746\n",
      "Test loss: 0.0470848683681753\n",
      "Starting epoch 4416\n",
      "Training loss: 0.009796060530132935\n",
      "Test loss: 0.046620487614914226\n",
      "Starting epoch 4417\n",
      "Training loss: 0.009881003271238725\n",
      "Test loss: 0.04661807348882711\n",
      "Starting epoch 4418\n",
      "Training loss: 0.010087695293372771\n",
      "Test loss: 0.04704695871030843\n",
      "Starting epoch 4419\n",
      "Training loss: 0.009954527432679152\n",
      "Test loss: 0.04742705518448794\n",
      "Starting epoch 4420\n",
      "Training loss: 0.009942444087174094\n",
      "Test loss: 0.04693159999118911\n",
      "Starting epoch 4421\n",
      "Training loss: 0.010113957429640606\n",
      "Test loss: 0.047005082239155414\n",
      "Starting epoch 4422\n",
      "Training loss: 0.009847513934383626\n",
      "Test loss: 0.04646263989033522\n",
      "Starting epoch 4423\n",
      "Training loss: 0.009828133218478962\n",
      "Test loss: 0.04635481619172626\n",
      "Starting epoch 4424\n",
      "Training loss: 0.009874769211670414\n",
      "Test loss: 0.04648341652419832\n",
      "Starting epoch 4425\n",
      "Training loss: 0.009840686225378122\n",
      "Test loss: 0.0464601574672593\n",
      "Starting epoch 4426\n",
      "Training loss: 0.009865017012372369\n",
      "Test loss: 0.04665392416494864\n",
      "Starting epoch 4427\n",
      "Training loss: 0.00981903755579327\n",
      "Test loss: 0.046845755643314786\n",
      "Starting epoch 4428\n",
      "Training loss: 0.009869948915037953\n",
      "Test loss: 0.046756043547281516\n",
      "Starting epoch 4429\n",
      "Training loss: 0.009843523110278318\n",
      "Test loss: 0.046610652848526286\n",
      "Starting epoch 4430\n",
      "Training loss: 0.009824448875838616\n",
      "Test loss: 0.0464965313397072\n",
      "Starting epoch 4431\n",
      "Training loss: 0.009917153564632917\n",
      "Test loss: 0.046304799892284254\n",
      "Starting epoch 4432\n",
      "Training loss: 0.00983166574027206\n",
      "Test loss: 0.04619628977444437\n",
      "Starting epoch 4433\n",
      "Training loss: 0.009845791598323916\n",
      "Test loss: 0.0465712238241125\n",
      "Starting epoch 4434\n",
      "Training loss: 0.009796062820270414\n",
      "Test loss: 0.04684367237819566\n",
      "Starting epoch 4435\n",
      "Training loss: 0.009842929521911457\n",
      "Test loss: 0.04667762528966974\n",
      "Starting epoch 4436\n",
      "Training loss: 0.009866192020842286\n",
      "Test loss: 0.04668770261384823\n",
      "Starting epoch 4437\n",
      "Training loss: 0.009838973538430988\n",
      "Test loss: 0.04661870554641441\n",
      "Starting epoch 4438\n",
      "Training loss: 0.009920328260078782\n",
      "Test loss: 0.04665233760520264\n",
      "Starting epoch 4439\n",
      "Training loss: 0.009825736444565605\n",
      "Test loss: 0.04717056742972798\n",
      "Starting epoch 4440\n",
      "Training loss: 0.00981769571844183\n",
      "Test loss: 0.04675792930302797\n",
      "Starting epoch 4441\n",
      "Training loss: 0.009818116187682895\n",
      "Test loss: 0.046860787741563936\n",
      "Starting epoch 4442\n",
      "Training loss: 0.010233847806077511\n",
      "Test loss: 0.04684124611042164\n",
      "Starting epoch 4443\n",
      "Training loss: 0.009821967175993764\n",
      "Test loss: 0.04741716371090324\n",
      "Starting epoch 4444\n",
      "Training loss: 0.009834439157828933\n",
      "Test loss: 0.04733885383164441\n",
      "Starting epoch 4445\n",
      "Training loss: 0.009831077617699982\n",
      "Test loss: 0.04699814333407967\n",
      "Starting epoch 4446\n",
      "Training loss: 0.009787036197595909\n",
      "Test loss: 0.04691174502174059\n",
      "Starting epoch 4447\n",
      "Training loss: 0.009798302986949194\n",
      "Test loss: 0.04693930444342119\n",
      "Starting epoch 4448\n",
      "Training loss: 0.009843150214826474\n",
      "Test loss: 0.04684670973155233\n",
      "Starting epoch 4449\n",
      "Training loss: 0.009887901913435733\n",
      "Test loss: 0.04712176612681813\n",
      "Starting epoch 4450\n",
      "Training loss: 0.009809351388792523\n",
      "Test loss: 0.04738874457500599\n",
      "Starting epoch 4451\n",
      "Training loss: 0.009945436228127753\n",
      "Test loss: 0.046937637582973195\n",
      "Starting epoch 4452\n",
      "Training loss: 0.009893687686226407\n",
      "Test loss: 0.047196536497385415\n",
      "Starting epoch 4453\n",
      "Training loss: 0.009994420482486974\n",
      "Test loss: 0.04729119039796017\n",
      "Starting epoch 4454\n",
      "Training loss: 0.009827794705624462\n",
      "Test loss: 0.04745126336261078\n",
      "Starting epoch 4455\n",
      "Training loss: 0.010265462986025655\n",
      "Test loss: 0.04729447513818741\n",
      "Starting epoch 4456\n",
      "Training loss: 0.010217948816716671\n",
      "Test loss: 0.04662912173403634\n",
      "Starting epoch 4457\n",
      "Training loss: 0.009879826736010488\n",
      "Test loss: 0.04730103468453443\n",
      "Starting epoch 4458\n",
      "Training loss: 0.009921414228003533\n",
      "Test loss: 0.047306930301366026\n",
      "Starting epoch 4459\n",
      "Training loss: 0.009878285488754999\n",
      "Test loss: 0.04656417723055239\n",
      "Starting epoch 4460\n",
      "Training loss: 0.00980615608210935\n",
      "Test loss: 0.04632017154384543\n",
      "Starting epoch 4461\n",
      "Training loss: 0.0098233058376879\n",
      "Test loss: 0.0462923509379228\n",
      "Starting epoch 4462\n",
      "Training loss: 0.010073720126367007\n",
      "Test loss: 0.04668402568333679\n",
      "Starting epoch 4463\n",
      "Training loss: 0.009790802176003574\n",
      "Test loss: 0.0463225572473473\n",
      "Starting epoch 4464\n",
      "Training loss: 0.009869571775197983\n",
      "Test loss: 0.04657870613866382\n",
      "Starting epoch 4465\n",
      "Training loss: 0.009786006964010293\n",
      "Test loss: 0.04663970390403712\n",
      "Starting epoch 4466\n",
      "Training loss: 0.009795689466791074\n",
      "Test loss: 0.04694822582381743\n",
      "Starting epoch 4467\n",
      "Training loss: 0.010008223904449432\n",
      "Test loss: 0.04701873646290214\n",
      "Starting epoch 4468\n",
      "Training loss: 0.01035776922143385\n",
      "Test loss: 0.04672477815162252\n",
      "Starting epoch 4469\n",
      "Training loss: 0.009817988947644586\n",
      "Test loss: 0.04751317685952893\n",
      "Starting epoch 4470\n",
      "Training loss: 0.009888205478792308\n",
      "Test loss: 0.04755834089937033\n",
      "Starting epoch 4471\n",
      "Training loss: 0.009843396847365333\n",
      "Test loss: 0.047061289488165466\n",
      "Starting epoch 4472\n",
      "Training loss: 0.009929432319935228\n",
      "Test loss: 0.046574923037378875\n",
      "Starting epoch 4473\n",
      "Training loss: 0.009795620090893058\n",
      "Test loss: 0.04620787059819257\n",
      "Starting epoch 4474\n",
      "Training loss: 0.009839063922523475\n",
      "Test loss: 0.04650545065049772\n",
      "Starting epoch 4475\n",
      "Training loss: 0.009978404772452643\n",
      "Test loss: 0.04710482219579043\n",
      "Starting epoch 4476\n",
      "Training loss: 0.009918783539448117\n",
      "Test loss: 0.04759171439541711\n",
      "Starting epoch 4477\n",
      "Training loss: 0.009969044201931015\n",
      "Test loss: 0.046819157622478624\n",
      "Starting epoch 4478\n",
      "Training loss: 0.009786888163109294\n",
      "Test loss: 0.046967013704556006\n",
      "Starting epoch 4479\n",
      "Training loss: 0.009865952838884025\n",
      "Test loss: 0.04663029064734777\n",
      "Starting epoch 4480\n",
      "Training loss: 0.009858570672327378\n",
      "Test loss: 0.04664231795403692\n",
      "Starting epoch 4481\n",
      "Training loss: 0.00982582059185036\n",
      "Test loss: 0.046538264801104866\n",
      "Starting epoch 4482\n",
      "Training loss: 0.010142147159356563\n",
      "Test loss: 0.04672762917147742\n",
      "Starting epoch 4483\n",
      "Training loss: 0.009968212683547716\n",
      "Test loss: 0.04625913155851541\n",
      "Starting epoch 4484\n",
      "Training loss: 0.00987256835901835\n",
      "Test loss: 0.046888824414323876\n",
      "Starting epoch 4485\n",
      "Training loss: 0.009804953759933104\n",
      "Test loss: 0.04669587562481562\n",
      "Starting epoch 4486\n",
      "Training loss: 0.009835468009724969\n",
      "Test loss: 0.046523408550355166\n",
      "Starting epoch 4487\n",
      "Training loss: 0.009830452410168335\n",
      "Test loss: 0.04669492236442036\n",
      "Starting epoch 4488\n",
      "Training loss: 0.009898849350751424\n",
      "Test loss: 0.04696675969494714\n",
      "Starting epoch 4489\n",
      "Training loss: 0.009846430256596355\n",
      "Test loss: 0.04719011733929316\n",
      "Starting epoch 4490\n",
      "Training loss: 0.009797519996579065\n",
      "Test loss: 0.0470330497180974\n",
      "Starting epoch 4491\n",
      "Training loss: 0.009805052296915015\n",
      "Test loss: 0.046756713202706086\n",
      "Starting epoch 4492\n",
      "Training loss: 0.009867111068280017\n",
      "Test loss: 0.0466764926634453\n",
      "Starting epoch 4493\n",
      "Training loss: 0.00999601238758349\n",
      "Test loss: 0.0467380645374457\n",
      "Starting epoch 4494\n",
      "Training loss: 0.009940299220749588\n",
      "Test loss: 0.04645540537657561\n",
      "Starting epoch 4495\n",
      "Training loss: 0.00981560734207513\n",
      "Test loss: 0.04692372652115645\n",
      "Starting epoch 4496\n",
      "Training loss: 0.009801367465712007\n",
      "Test loss: 0.0471248431476178\n",
      "Starting epoch 4497\n",
      "Training loss: 0.009904439377858013\n",
      "Test loss: 0.04725394522150358\n",
      "Starting epoch 4498\n",
      "Training loss: 0.009932650298979438\n",
      "Test loss: 0.04672789408100976\n",
      "Starting epoch 4499\n",
      "Training loss: 0.009825698069495256\n",
      "Test loss: 0.04718329426315096\n",
      "Starting epoch 4500\n",
      "Training loss: 0.00984948363582619\n",
      "Test loss: 0.04700917198702141\n",
      "Starting epoch 4501\n",
      "Training loss: 0.00988501345463952\n",
      "Test loss: 0.04673051889295931\n",
      "Starting epoch 4502\n",
      "Training loss: 0.009792877865008643\n",
      "Test loss: 0.04631846398115158\n",
      "Starting epoch 4503\n",
      "Training loss: 0.010083510280876864\n",
      "Test loss: 0.046592833267317876\n",
      "Starting epoch 4504\n",
      "Training loss: 0.009855161405733375\n",
      "Test loss: 0.04629406829675039\n",
      "Starting epoch 4505\n",
      "Training loss: 0.009822184968067974\n",
      "Test loss: 0.04631479843347161\n",
      "Starting epoch 4506\n",
      "Training loss: 0.010054945044952338\n",
      "Test loss: 0.04697531275451183\n",
      "Starting epoch 4507\n",
      "Training loss: 0.009755372474367013\n",
      "Test loss: 0.04762727008373649\n",
      "Starting epoch 4508\n",
      "Training loss: 0.009850091453580583\n",
      "Test loss: 0.04721684436555262\n",
      "Starting epoch 4509\n",
      "Training loss: 0.010073746859905173\n",
      "Test loss: 0.04718676968305199\n",
      "Starting epoch 4510\n",
      "Training loss: 0.00978581045494705\n",
      "Test loss: 0.04744378829167949\n",
      "Starting epoch 4511\n",
      "Training loss: 0.009868081063642854\n",
      "Test loss: 0.047108890005835784\n",
      "Starting epoch 4512\n",
      "Training loss: 0.00982260846029051\n",
      "Test loss: 0.04713022198390077\n",
      "Starting epoch 4513\n",
      "Training loss: 0.009792732700827669\n",
      "Test loss: 0.04688212247910323\n",
      "Starting epoch 4514\n",
      "Training loss: 0.009793421146688892\n",
      "Test loss: 0.04694470721814367\n",
      "Starting epoch 4515\n",
      "Training loss: 0.009790874498545146\n",
      "Test loss: 0.04702330187514976\n",
      "Starting epoch 4516\n",
      "Training loss: 0.009845449375446702\n",
      "Test loss: 0.046886813982079424\n",
      "Starting epoch 4517\n",
      "Training loss: 0.009806826435884491\n",
      "Test loss: 0.04663228588523688\n",
      "Starting epoch 4518\n",
      "Training loss: 0.009833420519946052\n",
      "Test loss: 0.0466401856392622\n",
      "Starting epoch 4519\n",
      "Training loss: 0.009832460303470248\n",
      "Test loss: 0.04684903113930314\n",
      "Starting epoch 4520\n",
      "Training loss: 0.009841976916325873\n",
      "Test loss: 0.046886994744892475\n",
      "Starting epoch 4521\n",
      "Training loss: 0.010033625254376989\n",
      "Test loss: 0.04659628012665996\n",
      "Starting epoch 4522\n",
      "Training loss: 0.00984809159866122\n",
      "Test loss: 0.04705037038635324\n",
      "Starting epoch 4523\n",
      "Training loss: 0.01003218665108329\n",
      "Test loss: 0.047287516985778454\n",
      "Starting epoch 4524\n",
      "Training loss: 0.009894423736412018\n",
      "Test loss: 0.04746234858477557\n",
      "Starting epoch 4525\n",
      "Training loss: 0.009882886863512094\n",
      "Test loss: 0.04669930006342905\n",
      "Starting epoch 4526\n",
      "Training loss: 0.009815247653082747\n",
      "Test loss: 0.046810116756845405\n",
      "Starting epoch 4527\n",
      "Training loss: 0.01006203369222215\n",
      "Test loss: 0.04657334999905692\n",
      "Starting epoch 4528\n",
      "Training loss: 0.009894782356673577\n",
      "Test loss: 0.04710410190401254\n",
      "Starting epoch 4529\n",
      "Training loss: 0.009827255439318594\n",
      "Test loss: 0.04663880321162718\n",
      "Starting epoch 4530\n",
      "Training loss: 0.009860043322331593\n",
      "Test loss: 0.04637703161548685\n",
      "Starting epoch 4531\n",
      "Training loss: 0.010032040723523155\n",
      "Test loss: 0.046591868003209434\n",
      "Starting epoch 4532\n",
      "Training loss: 0.009814390240878355\n",
      "Test loss: 0.04733617865928897\n",
      "Starting epoch 4533\n",
      "Training loss: 0.009817683397502195\n",
      "Test loss: 0.047210664522868616\n",
      "Starting epoch 4534\n",
      "Training loss: 0.009773623045594966\n",
      "Test loss: 0.0468747999381136\n",
      "Starting epoch 4535\n",
      "Training loss: 0.009832298535792554\n",
      "Test loss: 0.046529080994703154\n",
      "Starting epoch 4536\n",
      "Training loss: 0.010050497384222805\n",
      "Test loss: 0.04686766407556004\n",
      "Starting epoch 4537\n",
      "Training loss: 0.0097932599210104\n",
      "Test loss: 0.046380290002734574\n",
      "Starting epoch 4538\n",
      "Training loss: 0.009837865905805689\n",
      "Test loss: 0.04655900977000042\n",
      "Starting epoch 4539\n",
      "Training loss: 0.009781468445893194\n",
      "Test loss: 0.04645919427275658\n",
      "Starting epoch 4540\n",
      "Training loss: 0.009858000076940803\n",
      "Test loss: 0.04683277838759952\n",
      "Starting epoch 4541\n",
      "Training loss: 0.009785160040635555\n",
      "Test loss: 0.04670337353039671\n",
      "Starting epoch 4542\n",
      "Training loss: 0.009792934614615362\n",
      "Test loss: 0.04673905449884909\n",
      "Starting epoch 4543\n",
      "Training loss: 0.009902374650977675\n",
      "Test loss: 0.04690483295255237\n",
      "Starting epoch 4544\n",
      "Training loss: 0.00977013404404775\n",
      "Test loss: 0.04654103172598062\n",
      "Starting epoch 4545\n",
      "Training loss: 0.00994966066152346\n",
      "Test loss: 0.0467313835742297\n",
      "Starting epoch 4546\n",
      "Training loss: 0.009810186998888117\n",
      "Test loss: 0.047151212201074315\n",
      "Starting epoch 4547\n",
      "Training loss: 0.009934727407869746\n",
      "Test loss: 0.047139369779162936\n",
      "Starting epoch 4548\n",
      "Training loss: 0.009923973441368243\n",
      "Test loss: 0.046711896304731014\n",
      "Starting epoch 4549\n",
      "Training loss: 0.01001640945123356\n",
      "Test loss: 0.04613825533952978\n",
      "Starting epoch 4550\n",
      "Training loss: 0.010022387931459263\n",
      "Test loss: 0.04688531615667873\n",
      "Starting epoch 4551\n",
      "Training loss: 0.009849608906346266\n",
      "Test loss: 0.04669566286934747\n",
      "Starting epoch 4552\n",
      "Training loss: 0.009774669607887502\n",
      "Test loss: 0.046919432641179475\n",
      "Starting epoch 4553\n",
      "Training loss: 0.009881986778412686\n",
      "Test loss: 0.046941738575696945\n",
      "Starting epoch 4554\n",
      "Training loss: 0.009784737548439717\n",
      "Test loss: 0.047207983348656585\n",
      "Starting epoch 4555\n",
      "Training loss: 0.009967425654901833\n",
      "Test loss: 0.04715371311262802\n",
      "Starting epoch 4556\n",
      "Training loss: 0.009769076664672523\n",
      "Test loss: 0.04648042980719496\n",
      "Starting epoch 4557\n",
      "Training loss: 0.009858754127606994\n",
      "Test loss: 0.046297728877376626\n",
      "Starting epoch 4558\n",
      "Training loss: 0.009781748560241988\n",
      "Test loss: 0.0463526727700675\n",
      "Starting epoch 4559\n",
      "Training loss: 0.009869161672645906\n",
      "Test loss: 0.04668622070716487\n",
      "Starting epoch 4560\n",
      "Training loss: 0.009959019690019185\n",
      "Test loss: 0.046445327086581126\n",
      "Starting epoch 4561\n",
      "Training loss: 0.009975038102415741\n",
      "Test loss: 0.04711126439549305\n",
      "Starting epoch 4562\n",
      "Training loss: 0.009821798682945674\n",
      "Test loss: 0.04664914130612656\n",
      "Starting epoch 4563\n",
      "Training loss: 0.009800889147598236\n",
      "Test loss: 0.04658475545821367\n",
      "Starting epoch 4564\n",
      "Training loss: 0.009811788614167542\n",
      "Test loss: 0.046826655252112284\n",
      "Starting epoch 4565\n",
      "Training loss: 0.009817919243493529\n",
      "Test loss: 0.047173805396865914\n",
      "Starting epoch 4566\n",
      "Training loss: 0.009818436211494148\n",
      "Test loss: 0.047209282164220455\n",
      "Starting epoch 4567\n",
      "Training loss: 0.009800810763826136\n",
      "Test loss: 0.04683438171115187\n",
      "Starting epoch 4568\n",
      "Training loss: 0.009941187900964354\n",
      "Test loss: 0.046631821189765575\n",
      "Starting epoch 4569\n",
      "Training loss: 0.009780116944161595\n",
      "Test loss: 0.04636656341177446\n",
      "Starting epoch 4570\n",
      "Training loss: 0.009866348177683159\n",
      "Test loss: 0.046574840460110595\n",
      "Starting epoch 4571\n",
      "Training loss: 0.009784290536505277\n",
      "Test loss: 0.046524673286411494\n",
      "Starting epoch 4572\n",
      "Training loss: 0.009847982084286994\n",
      "Test loss: 0.04652096482890623\n",
      "Starting epoch 4573\n",
      "Training loss: 0.009831981687638604\n",
      "Test loss: 0.046683901024085504\n",
      "Starting epoch 4574\n",
      "Training loss: 0.009798092431709414\n",
      "Test loss: 0.046619115949228955\n",
      "Starting epoch 4575\n",
      "Training loss: 0.009827885929434026\n",
      "Test loss: 0.046881396875337315\n",
      "Starting epoch 4576\n",
      "Training loss: 0.00986877024234807\n",
      "Test loss: 0.046991069697671466\n",
      "Starting epoch 4577\n",
      "Training loss: 0.009828151768592537\n",
      "Test loss: 0.04671779371522091\n",
      "Starting epoch 4578\n",
      "Training loss: 0.009881090708687658\n",
      "Test loss: 0.0464620198364611\n",
      "Starting epoch 4579\n",
      "Training loss: 0.009865800590544451\n",
      "Test loss: 0.04672638837386061\n",
      "Starting epoch 4580\n",
      "Training loss: 0.00982898736342055\n",
      "Test loss: 0.04689188349854063\n",
      "Starting epoch 4581\n",
      "Training loss: 0.009820854466897054\n",
      "Test loss: 0.04717174613917315\n",
      "Starting epoch 4582\n",
      "Training loss: 0.010025281352220012\n",
      "Test loss: 0.04702368751168251\n",
      "Starting epoch 4583\n",
      "Training loss: 0.009969822283773149\n",
      "Test loss: 0.04745570466750198\n",
      "Starting epoch 4584\n",
      "Training loss: 0.009795767316197763\n",
      "Test loss: 0.04681188433810517\n",
      "Starting epoch 4585\n",
      "Training loss: 0.009912152637223729\n",
      "Test loss: 0.04676091091500388\n",
      "Starting epoch 4586\n",
      "Training loss: 0.009797349679054783\n",
      "Test loss: 0.04652902677103325\n",
      "Starting epoch 4587\n",
      "Training loss: 0.009812767289151422\n",
      "Test loss: 0.046686484305946914\n",
      "Starting epoch 4588\n",
      "Training loss: 0.00985015744007513\n",
      "Test loss: 0.04667465899277617\n",
      "Starting epoch 4589\n",
      "Training loss: 0.009826153638910075\n",
      "Test loss: 0.046439772195838114\n",
      "Starting epoch 4590\n",
      "Training loss: 0.009909744160707857\n",
      "Test loss: 0.04649325570574513\n",
      "Starting epoch 4591\n",
      "Training loss: 0.009870318405818744\n",
      "Test loss: 0.046501990131757875\n",
      "Starting epoch 4592\n",
      "Training loss: 0.009889575316891318\n",
      "Test loss: 0.046936124149296016\n",
      "Starting epoch 4593\n",
      "Training loss: 0.009846783075176302\n",
      "Test loss: 0.046632090514456784\n",
      "Starting epoch 4594\n",
      "Training loss: 0.009806002719236201\n",
      "Test loss: 0.04687859131782143\n",
      "Starting epoch 4595\n",
      "Training loss: 0.009852050574587995\n",
      "Test loss: 0.04701309595946913\n",
      "Starting epoch 4596\n",
      "Training loss: 0.009839920998841036\n",
      "Test loss: 0.04660895177059703\n",
      "Starting epoch 4597\n",
      "Training loss: 0.009788423303331508\n",
      "Test loss: 0.046870657691249144\n",
      "Starting epoch 4598\n",
      "Training loss: 0.009905065196092988\n",
      "Test loss: 0.0467111229620598\n",
      "Starting epoch 4599\n",
      "Training loss: 0.009866092079242722\n",
      "Test loss: 0.04644317110931432\n",
      "Starting epoch 4600\n",
      "Training loss: 0.009869908975040326\n",
      "Test loss: 0.046611230337509406\n",
      "Starting epoch 4601\n",
      "Training loss: 0.009802494564505874\n",
      "Test loss: 0.047075748443603516\n",
      "Starting epoch 4602\n",
      "Training loss: 0.009769565654826945\n",
      "Test loss: 0.04683218620441578\n",
      "Starting epoch 4603\n",
      "Training loss: 0.009944476355172571\n",
      "Test loss: 0.04679519890083207\n",
      "Starting epoch 4604\n",
      "Training loss: 0.009873896654023499\n",
      "Test loss: 0.04652331410734742\n",
      "Starting epoch 4605\n",
      "Training loss: 0.009835658701839016\n",
      "Test loss: 0.04683913614738871\n",
      "Starting epoch 4606\n",
      "Training loss: 0.009947603476829216\n",
      "Test loss: 0.04707792318529553\n",
      "Starting epoch 4607\n",
      "Training loss: 0.00983561541060688\n",
      "Test loss: 0.046523152816074866\n",
      "Starting epoch 4608\n",
      "Training loss: 0.009859705450715588\n",
      "Test loss: 0.04664533530120497\n",
      "Starting epoch 4609\n",
      "Training loss: 0.009882641788266722\n",
      "Test loss: 0.04661446216481703\n",
      "Starting epoch 4610\n",
      "Training loss: 0.009857876333179043\n",
      "Test loss: 0.04642965065108405\n",
      "Starting epoch 4611\n",
      "Training loss: 0.009813466597898085\n",
      "Test loss: 0.04694839484161801\n",
      "Starting epoch 4612\n",
      "Training loss: 0.009855674304923073\n",
      "Test loss: 0.046761006116867065\n",
      "Starting epoch 4613\n",
      "Training loss: 0.009833472521334399\n",
      "Test loss: 0.04660025570127699\n",
      "Starting epoch 4614\n",
      "Training loss: 0.009863817896388594\n",
      "Test loss: 0.047044775207285526\n",
      "Starting epoch 4615\n",
      "Training loss: 0.009824180853415708\n",
      "Test loss: 0.047169758627812065\n",
      "Starting epoch 4616\n",
      "Training loss: 0.00990572425185657\n",
      "Test loss: 0.04706432573773243\n",
      "Starting epoch 4617\n",
      "Training loss: 0.009942404047937179\n",
      "Test loss: 0.04747295379638672\n",
      "Starting epoch 4618\n",
      "Training loss: 0.00984187468458883\n",
      "Test loss: 0.04689941251719439\n",
      "Starting epoch 4619\n",
      "Training loss: 0.009828557382475157\n",
      "Test loss: 0.04683215143503966\n",
      "Starting epoch 4620\n",
      "Training loss: 0.009803571814640623\n",
      "Test loss: 0.046688050445583135\n",
      "Starting epoch 4621\n",
      "Training loss: 0.010171198240313374\n",
      "Test loss: 0.0466072255814517\n",
      "Starting epoch 4622\n",
      "Training loss: 0.00995969647144685\n",
      "Test loss: 0.046211911020455534\n",
      "Starting epoch 4623\n",
      "Training loss: 0.009802956469967717\n",
      "Test loss: 0.046897821680263234\n",
      "Starting epoch 4624\n",
      "Training loss: 0.009851572668698967\n",
      "Test loss: 0.047010507296632836\n",
      "Starting epoch 4625\n",
      "Training loss: 0.009758035346987794\n",
      "Test loss: 0.046714884953366384\n",
      "Starting epoch 4626\n",
      "Training loss: 0.009894694430661983\n",
      "Test loss: 0.04675707152044332\n",
      "Starting epoch 4627\n",
      "Training loss: 0.010100573423455973\n",
      "Test loss: 0.04712540159622828\n",
      "Starting epoch 4628\n",
      "Training loss: 0.00987796060985229\n",
      "Test loss: 0.04645701125264168\n",
      "Starting epoch 4629\n",
      "Training loss: 0.009773211248341154\n",
      "Test loss: 0.046294015383830774\n",
      "Starting epoch 4630\n",
      "Training loss: 0.009854307856227531\n",
      "Test loss: 0.046701194511519536\n",
      "Starting epoch 4631\n",
      "Training loss: 0.009798477510692643\n",
      "Test loss: 0.04693479339281718\n",
      "Starting epoch 4632\n",
      "Training loss: 0.0097807253878869\n",
      "Test loss: 0.04692974893583192\n",
      "Starting epoch 4633\n",
      "Training loss: 0.00978410898967356\n",
      "Test loss: 0.046730066960056625\n",
      "Starting epoch 4634\n",
      "Training loss: 0.00985187196676604\n",
      "Test loss: 0.04669954393196989\n",
      "Starting epoch 4635\n",
      "Training loss: 0.009855429962521693\n",
      "Test loss: 0.04667563419099207\n",
      "Starting epoch 4636\n",
      "Training loss: 0.009963015598229697\n",
      "Test loss: 0.04653702855662063\n",
      "Starting epoch 4637\n",
      "Training loss: 0.009859885470788986\n",
      "Test loss: 0.04698164057400492\n",
      "Starting epoch 4638\n",
      "Training loss: 0.009834368423116012\n",
      "Test loss: 0.04722735665186688\n",
      "Starting epoch 4639\n",
      "Training loss: 0.009888167813664576\n",
      "Test loss: 0.04716565446169288\n",
      "Starting epoch 4640\n",
      "Training loss: 0.009977062309130293\n",
      "Test loss: 0.04673772843347655\n",
      "Starting epoch 4641\n",
      "Training loss: 0.00982290132307127\n",
      "Test loss: 0.04702868422976247\n",
      "Starting epoch 4642\n",
      "Training loss: 0.009853908708166392\n",
      "Test loss: 0.04721088666054937\n",
      "Starting epoch 4643\n",
      "Training loss: 0.009879013126502271\n",
      "Test loss: 0.04720006207073176\n",
      "Starting epoch 4644\n",
      "Training loss: 0.00980329257054407\n",
      "Test loss: 0.046716508076146794\n",
      "Starting epoch 4645\n",
      "Training loss: 0.009856760288115407\n",
      "Test loss: 0.04667272832658556\n",
      "Starting epoch 4646\n",
      "Training loss: 0.009821296753514498\n",
      "Test loss: 0.0465075857937336\n",
      "Starting epoch 4647\n",
      "Training loss: 0.00982952108759372\n",
      "Test loss: 0.04637404807187893\n",
      "Starting epoch 4648\n",
      "Training loss: 0.009795328174702456\n",
      "Test loss: 0.046759758282590796\n",
      "Starting epoch 4649\n",
      "Training loss: 0.009840501915113847\n",
      "Test loss: 0.04687602597254294\n",
      "Starting epoch 4650\n",
      "Training loss: 0.009783091802211081\n",
      "Test loss: 0.04695338397114365\n",
      "Starting epoch 4651\n",
      "Training loss: 0.009778005681687692\n",
      "Test loss: 0.04675959078250108\n",
      "Starting epoch 4652\n",
      "Training loss: 0.009912166255907934\n",
      "Test loss: 0.0468312519154063\n",
      "Starting epoch 4653\n",
      "Training loss: 0.009798704928978056\n",
      "Test loss: 0.04731580946180555\n",
      "Starting epoch 4654\n",
      "Training loss: 0.009957110203924726\n",
      "Test loss: 0.047006501367798555\n",
      "Starting epoch 4655\n",
      "Training loss: 0.009895936280611108\n",
      "Test loss: 0.04725704756047991\n",
      "Starting epoch 4656\n",
      "Training loss: 0.010023716852435322\n",
      "Test loss: 0.0468991876890262\n",
      "Starting epoch 4657\n",
      "Training loss: 0.009826293169352853\n",
      "Test loss: 0.04730028948850102\n",
      "Starting epoch 4658\n",
      "Training loss: 0.009790649576509585\n",
      "Test loss: 0.04712563918696509\n",
      "Starting epoch 4659\n",
      "Training loss: 0.009868869512173974\n",
      "Test loss: 0.04684846116988747\n",
      "Starting epoch 4660\n",
      "Training loss: 0.009774865964274914\n",
      "Test loss: 0.0464972746041086\n",
      "Starting epoch 4661\n",
      "Training loss: 0.009799669802066733\n",
      "Test loss: 0.046710023311553175\n",
      "Starting epoch 4662\n",
      "Training loss: 0.009795151910454523\n",
      "Test loss: 0.04704097879153711\n",
      "Starting epoch 4663\n",
      "Training loss: 0.009827861211216841\n",
      "Test loss: 0.04693508093003874\n",
      "Starting epoch 4664\n",
      "Training loss: 0.009968582754496668\n",
      "Test loss: 0.0467180198541394\n",
      "Starting epoch 4665\n",
      "Training loss: 0.009809355068280071\n",
      "Test loss: 0.046218355634698165\n",
      "Starting epoch 4666\n",
      "Training loss: 0.00985113792426762\n",
      "Test loss: 0.04645040148386249\n",
      "Starting epoch 4667\n",
      "Training loss: 0.010199436414070794\n",
      "Test loss: 0.04694101752506362\n",
      "Starting epoch 4668\n",
      "Training loss: 0.009831433886753732\n",
      "Test loss: 0.04755601231698637\n",
      "Starting epoch 4669\n",
      "Training loss: 0.009963239512604768\n",
      "Test loss: 0.04759124611262922\n",
      "Starting epoch 4670\n",
      "Training loss: 0.0100412677637622\n",
      "Test loss: 0.0474287170778822\n",
      "Starting epoch 4671\n",
      "Training loss: 0.009824404783058361\n",
      "Test loss: 0.04661853376913954\n",
      "Starting epoch 4672\n",
      "Training loss: 0.009836899925817232\n",
      "Test loss: 0.046478664571488346\n",
      "Starting epoch 4673\n",
      "Training loss: 0.009834101347283263\n",
      "Test loss: 0.046831966274314456\n",
      "Starting epoch 4674\n",
      "Training loss: 0.009843382694315716\n",
      "Test loss: 0.04677496553847083\n",
      "Starting epoch 4675\n",
      "Training loss: 0.009781799500533303\n",
      "Test loss: 0.04712316724989149\n",
      "Starting epoch 4676\n",
      "Training loss: 0.00995506439357996\n",
      "Test loss: 0.04711873560316033\n",
      "Starting epoch 4677\n",
      "Training loss: 0.009836079858121325\n",
      "Test loss: 0.047287840810086995\n",
      "Starting epoch 4678\n",
      "Training loss: 0.009852233289389825\n",
      "Test loss: 0.04738832679059771\n",
      "Starting epoch 4679\n",
      "Training loss: 0.009916878290107994\n",
      "Test loss: 0.04727626847172225\n",
      "Starting epoch 4680\n",
      "Training loss: 0.009845833691050772\n",
      "Test loss: 0.046489334768719144\n",
      "Starting epoch 4681\n",
      "Training loss: 0.009926600946632565\n",
      "Test loss: 0.04657731701930364\n",
      "Starting epoch 4682\n",
      "Training loss: 0.009876893298914198\n",
      "Test loss: 0.04643752439706414\n",
      "Starting epoch 4683\n",
      "Training loss: 0.009788849452113519\n",
      "Test loss: 0.04697497168348895\n",
      "Starting epoch 4684\n",
      "Training loss: 0.00994613345284931\n",
      "Test loss: 0.04730382561683655\n",
      "Starting epoch 4685\n",
      "Training loss: 0.009775037129150062\n",
      "Test loss: 0.0475026321493917\n",
      "Starting epoch 4686\n",
      "Training loss: 0.009790972333218231\n",
      "Test loss: 0.0472230137222343\n",
      "Starting epoch 4687\n",
      "Training loss: 0.009802288788019633\n",
      "Test loss: 0.04697980614448035\n",
      "Starting epoch 4688\n",
      "Training loss: 0.00984450289216198\n",
      "Test loss: 0.04706525643942533\n",
      "Starting epoch 4689\n",
      "Training loss: 0.009788658210366476\n",
      "Test loss: 0.04722299178441366\n",
      "Starting epoch 4690\n",
      "Training loss: 0.009834194235259393\n",
      "Test loss: 0.047169198316556436\n",
      "Starting epoch 4691\n",
      "Training loss: 0.009809689581027774\n",
      "Test loss: 0.0472787665548148\n",
      "Starting epoch 4692\n",
      "Training loss: 0.009884422477029387\n",
      "Test loss: 0.04704649677431142\n",
      "Starting epoch 4693\n",
      "Training loss: 0.009865444092476954\n",
      "Test loss: 0.047069069274045805\n",
      "Starting epoch 4694\n",
      "Training loss: 0.009846158982178227\n",
      "Test loss: 0.04721834179427889\n",
      "Starting epoch 4695\n",
      "Training loss: 0.009926207348337917\n",
      "Test loss: 0.046651950795893314\n",
      "Starting epoch 4696\n",
      "Training loss: 0.009811609036854057\n",
      "Test loss: 0.04687773491497393\n",
      "Starting epoch 4697\n",
      "Training loss: 0.009829644770285145\n",
      "Test loss: 0.04696849540427879\n",
      "Starting epoch 4698\n",
      "Training loss: 0.009855560653033803\n",
      "Test loss: 0.04718832768223904\n",
      "Starting epoch 4699\n",
      "Training loss: 0.009901727855083396\n",
      "Test loss: 0.04742294805193389\n",
      "Starting epoch 4700\n",
      "Training loss: 0.009847569928244978\n",
      "Test loss: 0.04691780869055678\n",
      "Starting epoch 4701\n",
      "Training loss: 0.009866642964179398\n",
      "Test loss: 0.04640277964925325\n",
      "Starting epoch 4702\n",
      "Training loss: 0.009794744006434425\n",
      "Test loss: 0.04670381352857307\n",
      "Starting epoch 4703\n",
      "Training loss: 0.009797113932302742\n",
      "Test loss: 0.04683261419887896\n",
      "Starting epoch 4704\n",
      "Training loss: 0.009800443379971826\n",
      "Test loss: 0.04679271440815042\n",
      "Starting epoch 4705\n",
      "Training loss: 0.009867767192667624\n",
      "Test loss: 0.04677085523252134\n",
      "Starting epoch 4706\n",
      "Training loss: 0.009982592243029446\n",
      "Test loss: 0.046608232237674574\n",
      "Starting epoch 4707\n",
      "Training loss: 0.009932878946305299\n",
      "Test loss: 0.046300560925845745\n",
      "Starting epoch 4708\n",
      "Training loss: 0.010072026646039525\n",
      "Test loss: 0.04698862211295852\n",
      "Starting epoch 4709\n",
      "Training loss: 0.00984607836933898\n",
      "Test loss: 0.04662187259506296\n",
      "Starting epoch 4710\n",
      "Training loss: 0.010004117290993205\n",
      "Test loss: 0.04643523430934659\n",
      "Starting epoch 4711\n",
      "Training loss: 0.009826048979627305\n",
      "Test loss: 0.04620188198707722\n",
      "Starting epoch 4712\n",
      "Training loss: 0.009907029492811102\n",
      "Test loss: 0.046460694874878285\n",
      "Starting epoch 4713\n",
      "Training loss: 0.009847203407009116\n",
      "Test loss: 0.046974484567289\n",
      "Starting epoch 4714\n",
      "Training loss: 0.00982186561603038\n",
      "Test loss: 0.046969173545086826\n",
      "Starting epoch 4715\n",
      "Training loss: 0.00979015545644721\n",
      "Test loss: 0.04672310197794879\n",
      "Starting epoch 4716\n",
      "Training loss: 0.009843460329976237\n",
      "Test loss: 0.046771244456370674\n",
      "Starting epoch 4717\n",
      "Training loss: 0.009792584269383892\n",
      "Test loss: 0.046823071108924016\n",
      "Starting epoch 4718\n",
      "Training loss: 0.01010742296510544\n",
      "Test loss: 0.047064300212595195\n",
      "Starting epoch 4719\n",
      "Training loss: 0.009857368731840711\n",
      "Test loss: 0.04652634787338751\n",
      "Starting epoch 4720\n",
      "Training loss: 0.010034537278726453\n",
      "Test loss: 0.046673262698782816\n",
      "Starting epoch 4721\n",
      "Training loss: 0.009823839119101157\n",
      "Test loss: 0.047409978729707224\n",
      "Starting epoch 4722\n",
      "Training loss: 0.009938182965775982\n",
      "Test loss: 0.04693695447511143\n",
      "Starting epoch 4723\n",
      "Training loss: 0.009766646386047855\n",
      "Test loss: 0.046408445884784065\n",
      "Starting epoch 4724\n",
      "Training loss: 0.009920871129534284\n",
      "Test loss: 0.046283350636561714\n",
      "Starting epoch 4725\n",
      "Training loss: 0.009968583029313166\n",
      "Test loss: 0.046908322169824886\n",
      "Starting epoch 4726\n",
      "Training loss: 0.009948417971857259\n",
      "Test loss: 0.04673545862789507\n",
      "Starting epoch 4727\n",
      "Training loss: 0.00978926024170684\n",
      "Test loss: 0.04702049990495046\n",
      "Starting epoch 4728\n",
      "Training loss: 0.009911453122364694\n",
      "Test loss: 0.046986250689736116\n",
      "Starting epoch 4729\n",
      "Training loss: 0.00993527316289847\n",
      "Test loss: 0.046512936966286764\n",
      "Starting epoch 4730\n",
      "Training loss: 0.009839158917426085\n",
      "Test loss: 0.04687668617676805\n",
      "Starting epoch 4731\n",
      "Training loss: 0.010060838912232\n",
      "Test loss: 0.046584485167706455\n",
      "Starting epoch 4732\n",
      "Training loss: 0.009792649171880035\n",
      "Test loss: 0.04711715366553377\n",
      "Starting epoch 4733\n",
      "Training loss: 0.009829088190539938\n",
      "Test loss: 0.04676285813804026\n",
      "Starting epoch 4734\n",
      "Training loss: 0.009926959429485877\n",
      "Test loss: 0.046582628731374386\n",
      "Starting epoch 4735\n",
      "Training loss: 0.00976819932827207\n",
      "Test loss: 0.04632242037742226\n",
      "Starting epoch 4736\n",
      "Training loss: 0.009855423229517506\n",
      "Test loss: 0.04648771295668902\n",
      "Starting epoch 4737\n",
      "Training loss: 0.009929016690518036\n",
      "Test loss: 0.046608983228603997\n",
      "Starting epoch 4738\n",
      "Training loss: 0.009889780360533566\n",
      "Test loss: 0.04708280014219107\n",
      "Starting epoch 4739\n",
      "Training loss: 0.009816824412736737\n",
      "Test loss: 0.04673319751465762\n",
      "Starting epoch 4740\n",
      "Training loss: 0.009928845006545058\n",
      "Test loss: 0.04666461105699892\n",
      "Starting epoch 4741\n",
      "Training loss: 0.009990377931809817\n",
      "Test loss: 0.047178843783007726\n",
      "Starting epoch 4742\n",
      "Training loss: 0.009845825950386093\n",
      "Test loss: 0.0478247882867301\n",
      "Starting epoch 4743\n",
      "Training loss: 0.009794258733936509\n",
      "Test loss: 0.047236362679137125\n",
      "Starting epoch 4744\n",
      "Training loss: 0.009872568442990056\n",
      "Test loss: 0.046834958648240124\n",
      "Starting epoch 4745\n",
      "Training loss: 0.009836593993985262\n",
      "Test loss: 0.047012905279795326\n",
      "Starting epoch 4746\n",
      "Training loss: 0.009818300268933421\n",
      "Test loss: 0.04713537061104068\n",
      "Starting epoch 4747\n",
      "Training loss: 0.009894504975222174\n",
      "Test loss: 0.04713788373326814\n",
      "Starting epoch 4748\n",
      "Training loss: 0.00980454932165439\n",
      "Test loss: 0.046989438158494455\n",
      "Starting epoch 4749\n",
      "Training loss: 0.009793558356459023\n",
      "Test loss: 0.046880623739626676\n",
      "Starting epoch 4750\n",
      "Training loss: 0.009877802979689642\n",
      "Test loss: 0.0471458713075629\n",
      "Starting epoch 4751\n",
      "Training loss: 0.009804191647982989\n",
      "Test loss: 0.04673560018892641\n",
      "Starting epoch 4752\n",
      "Training loss: 0.009853118893186578\n",
      "Test loss: 0.04692741014339306\n",
      "Starting epoch 4753\n",
      "Training loss: 0.009825940060688824\n",
      "Test loss: 0.04710967342058817\n",
      "Starting epoch 4754\n",
      "Training loss: 0.009985880506392873\n",
      "Test loss: 0.04731658446016135\n",
      "Starting epoch 4755\n",
      "Training loss: 0.009817637823766372\n",
      "Test loss: 0.0466567056460513\n",
      "Starting epoch 4756\n",
      "Training loss: 0.009817185170459942\n",
      "Test loss: 0.046830463878534456\n",
      "Starting epoch 4757\n",
      "Training loss: 0.009824993844586806\n",
      "Test loss: 0.046803876757621765\n",
      "Starting epoch 4758\n",
      "Training loss: 0.009870501326732948\n",
      "Test loss: 0.046891570643142415\n",
      "Starting epoch 4759\n",
      "Training loss: 0.010003078331957098\n",
      "Test loss: 0.04703297493634401\n",
      "Starting epoch 4760\n",
      "Training loss: 0.009789198255319088\n",
      "Test loss: 0.04655127986161797\n",
      "Starting epoch 4761\n",
      "Training loss: 0.009885620325803757\n",
      "Test loss: 0.04646132017175356\n",
      "Starting epoch 4762\n",
      "Training loss: 0.009926497356080618\n",
      "Test loss: 0.04688021857981329\n",
      "Starting epoch 4763\n",
      "Training loss: 0.009907550987650136\n",
      "Test loss: 0.04680453075302972\n",
      "Starting epoch 4764\n",
      "Training loss: 0.00981213353940698\n",
      "Test loss: 0.046979250869265306\n",
      "Starting epoch 4765\n",
      "Training loss: 0.009923074028042496\n",
      "Test loss: 0.04712804462070818\n",
      "Starting epoch 4766\n",
      "Training loss: 0.00980867653108034\n",
      "Test loss: 0.04672560978818823\n",
      "Starting epoch 4767\n",
      "Training loss: 0.009777727429984047\n",
      "Test loss: 0.0468261664112409\n",
      "Starting epoch 4768\n",
      "Training loss: 0.00986313413767541\n",
      "Test loss: 0.046844890548123255\n",
      "Starting epoch 4769\n",
      "Training loss: 0.00978302537295662\n",
      "Test loss: 0.0472602365469491\n",
      "Starting epoch 4770\n",
      "Training loss: 0.009861981511482449\n",
      "Test loss: 0.04706529058791973\n",
      "Starting epoch 4771\n",
      "Training loss: 0.010214713570035871\n",
      "Test loss: 0.04683271395387473\n",
      "Starting epoch 4772\n",
      "Training loss: 0.010306512891146981\n",
      "Test loss: 0.047650050923780156\n",
      "Starting epoch 4773\n",
      "Training loss: 0.009891240033092068\n",
      "Test loss: 0.04674299882241973\n",
      "Starting epoch 4774\n",
      "Training loss: 0.009861670709291443\n",
      "Test loss: 0.04706114185629068\n",
      "Starting epoch 4775\n",
      "Training loss: 0.00993703257628396\n",
      "Test loss: 0.046803564592092124\n",
      "Starting epoch 4776\n",
      "Training loss: 0.009753936657407244\n",
      "Test loss: 0.047219426543624314\n",
      "Starting epoch 4777\n",
      "Training loss: 0.009810413676695745\n",
      "Test loss: 0.04718672511754213\n",
      "Starting epoch 4778\n",
      "Training loss: 0.009887111693864963\n",
      "Test loss: 0.04700063858871107\n",
      "Starting epoch 4779\n",
      "Training loss: 0.009866454378991831\n",
      "Test loss: 0.04666562654353954\n",
      "Starting epoch 4780\n",
      "Training loss: 0.009902706797249982\n",
      "Test loss: 0.04640785314970546\n",
      "Starting epoch 4781\n",
      "Training loss: 0.009807808492638048\n",
      "Test loss: 0.04694577485874847\n",
      "Starting epoch 4782\n",
      "Training loss: 0.009871684930852203\n",
      "Test loss: 0.047294241548688325\n",
      "Starting epoch 4783\n",
      "Training loss: 0.00978735960317684\n",
      "Test loss: 0.04703449354403549\n",
      "Starting epoch 4784\n",
      "Training loss: 0.009854760448463628\n",
      "Test loss: 0.046809679449156476\n",
      "Starting epoch 4785\n",
      "Training loss: 0.00979846969369005\n",
      "Test loss: 0.047176271539043496\n",
      "Starting epoch 4786\n",
      "Training loss: 0.009837095274544154\n",
      "Test loss: 0.04693282271424929\n",
      "Starting epoch 4787\n",
      "Training loss: 0.009796707348928589\n",
      "Test loss: 0.047112307200829186\n",
      "Starting epoch 4788\n",
      "Training loss: 0.010111423896350821\n",
      "Test loss: 0.047060415976577334\n",
      "Starting epoch 4789\n",
      "Training loss: 0.009785637351088837\n",
      "Test loss: 0.047683048717401644\n",
      "Starting epoch 4790\n",
      "Training loss: 0.009896795143235902\n",
      "Test loss: 0.04738062385607649\n",
      "Starting epoch 4791\n",
      "Training loss: 0.009780560849142856\n",
      "Test loss: 0.0471483969853984\n",
      "Starting epoch 4792\n",
      "Training loss: 0.00986098221762747\n",
      "Test loss: 0.04677596094983595\n",
      "Starting epoch 4793\n",
      "Training loss: 0.009904279220910346\n",
      "Test loss: 0.04642216737071673\n",
      "Starting epoch 4794\n",
      "Training loss: 0.00981172137573117\n",
      "Test loss: 0.04684872842497296\n",
      "Starting epoch 4795\n",
      "Training loss: 0.00983243676849076\n",
      "Test loss: 0.0470920298938398\n",
      "Starting epoch 4796\n",
      "Training loss: 0.010005587314973112\n",
      "Test loss: 0.046827960345480174\n",
      "Starting epoch 4797\n",
      "Training loss: 0.010104687670703794\n",
      "Test loss: 0.04726967160348539\n",
      "Starting epoch 4798\n",
      "Training loss: 0.00976100718205581\n",
      "Test loss: 0.046686609034185055\n",
      "Starting epoch 4799\n",
      "Training loss: 0.009816664561140732\n",
      "Test loss: 0.04656827118661669\n",
      "Starting epoch 4800\n",
      "Training loss: 0.010050580333002278\n",
      "Test loss: 0.04697431982667358\n",
      "Starting epoch 4801\n",
      "Training loss: 0.009852843420183072\n",
      "Test loss: 0.04748355666244471\n",
      "Starting epoch 4802\n",
      "Training loss: 0.009811283043417775\n",
      "Test loss: 0.04756020975333673\n",
      "Starting epoch 4803\n",
      "Training loss: 0.010025999524065705\n",
      "Test loss: 0.0474599120379598\n",
      "Starting epoch 4804\n",
      "Training loss: 0.009893558949964945\n",
      "Test loss: 0.04756403472964411\n",
      "Starting epoch 4805\n",
      "Training loss: 0.009896688896124481\n",
      "Test loss: 0.046810908898435255\n",
      "Starting epoch 4806\n",
      "Training loss: 0.009853192131783142\n",
      "Test loss: 0.04694531292275146\n",
      "Starting epoch 4807\n",
      "Training loss: 0.010085899840857162\n",
      "Test loss: 0.04693338357739978\n",
      "Starting epoch 4808\n",
      "Training loss: 0.009862329688717107\n",
      "Test loss: 0.04753508743036677\n",
      "Starting epoch 4809\n",
      "Training loss: 0.010006620953256478\n",
      "Test loss: 0.047684011773930654\n",
      "Starting epoch 4810\n",
      "Training loss: 0.00981253130575184\n",
      "Test loss: 0.04691085467735926\n",
      "Starting epoch 4811\n",
      "Training loss: 0.009897066310780947\n",
      "Test loss: 0.0466750838138439\n",
      "Starting epoch 4812\n",
      "Training loss: 0.00980828441374126\n",
      "Test loss: 0.0470348653142099\n",
      "Starting epoch 4813\n",
      "Training loss: 0.009759136490890236\n",
      "Test loss: 0.04707669273570732\n",
      "Starting epoch 4814\n",
      "Training loss: 0.009782024246991658\n",
      "Test loss: 0.0468588268591298\n",
      "Starting epoch 4815\n",
      "Training loss: 0.009772085111405029\n",
      "Test loss: 0.04689646201829115\n",
      "Starting epoch 4816\n",
      "Training loss: 0.009932943070154698\n",
      "Test loss: 0.04700181081339165\n",
      "Starting epoch 4817\n",
      "Training loss: 0.00984787171492811\n",
      "Test loss: 0.04736573952767584\n",
      "Starting epoch 4818\n",
      "Training loss: 0.009835210048639383\n",
      "Test loss: 0.04684510206182798\n",
      "Starting epoch 4819\n",
      "Training loss: 0.00977242469299035\n",
      "Test loss: 0.04680238443392294\n",
      "Starting epoch 4820\n",
      "Training loss: 0.00978643323493297\n",
      "Test loss: 0.046845610081045715\n",
      "Starting epoch 4821\n",
      "Training loss: 0.009766770603104692\n",
      "Test loss: 0.04700616360814483\n",
      "Starting epoch 4822\n",
      "Training loss: 0.0098577067103298\n",
      "Test loss: 0.04691058093750918\n",
      "Starting epoch 4823\n",
      "Training loss: 0.009824780793097175\n",
      "Test loss: 0.047061421942931635\n",
      "Starting epoch 4824\n",
      "Training loss: 0.009811398481614277\n",
      "Test loss: 0.04665366256678546\n",
      "Starting epoch 4825\n",
      "Training loss: 0.009899061921312183\n",
      "Test loss: 0.04641798662918585\n",
      "Starting epoch 4826\n",
      "Training loss: 0.00984109585463512\n",
      "Test loss: 0.04688584045679481\n",
      "Starting epoch 4827\n",
      "Training loss: 0.009887565171620885\n",
      "Test loss: 0.04673116640360267\n",
      "Starting epoch 4828\n",
      "Training loss: 0.009822380347330063\n",
      "Test loss: 0.04644654511853501\n",
      "Starting epoch 4829\n",
      "Training loss: 0.010185857456116403\n",
      "Test loss: 0.046405736081026216\n",
      "Starting epoch 4830\n",
      "Training loss: 0.009968466797202338\n",
      "Test loss: 0.046018731124975065\n",
      "Starting epoch 4831\n",
      "Training loss: 0.009821052395845534\n",
      "Test loss: 0.046934799601634346\n",
      "Starting epoch 4832\n",
      "Training loss: 0.009943144731834287\n",
      "Test loss: 0.047039161194805744\n",
      "Starting epoch 4833\n",
      "Training loss: 0.010008833355835228\n",
      "Test loss: 0.04663878920729513\n",
      "Starting epoch 4834\n",
      "Training loss: 0.009826756891657094\n",
      "Test loss: 0.04721512438522445\n",
      "Starting epoch 4835\n",
      "Training loss: 0.009797964122940282\n",
      "Test loss: 0.0468770072416023\n",
      "Starting epoch 4836\n",
      "Training loss: 0.009783712780622185\n",
      "Test loss: 0.04666516729802997\n",
      "Starting epoch 4837\n",
      "Training loss: 0.00985857651981174\n",
      "Test loss: 0.04673440299100346\n",
      "Starting epoch 4838\n",
      "Training loss: 0.009808637934630035\n",
      "Test loss: 0.04699817686169236\n",
      "Starting epoch 4839\n",
      "Training loss: 0.009971871269775218\n",
      "Test loss: 0.046815902891534346\n",
      "Starting epoch 4840\n",
      "Training loss: 0.009810268817866435\n",
      "Test loss: 0.047135882493522435\n",
      "Starting epoch 4841\n",
      "Training loss: 0.009891530819481513\n",
      "Test loss: 0.0471831515983299\n",
      "Starting epoch 4842\n",
      "Training loss: 0.00988458692416793\n",
      "Test loss: 0.04663267248758563\n",
      "Starting epoch 4843\n",
      "Training loss: 0.009954060733196188\n",
      "Test loss: 0.04722999995229421\n",
      "Starting epoch 4844\n",
      "Training loss: 0.009811103542442204\n",
      "Test loss: 0.04669454045317791\n",
      "Starting epoch 4845\n",
      "Training loss: 0.009790931385560114\n",
      "Test loss: 0.04663829174306658\n",
      "Starting epoch 4846\n",
      "Training loss: 0.009833629792708843\n",
      "Test loss: 0.04687266520879887\n",
      "Starting epoch 4847\n",
      "Training loss: 0.009877556064700494\n",
      "Test loss: 0.04689070306442402\n",
      "Starting epoch 4848\n",
      "Training loss: 0.009818773464773034\n",
      "Test loss: 0.04666769366573404\n",
      "Starting epoch 4849\n",
      "Training loss: 0.010019420096498043\n",
      "Test loss: 0.046720022680582826\n",
      "Starting epoch 4850\n",
      "Training loss: 0.010061058750162359\n",
      "Test loss: 0.04734514046598364\n",
      "Starting epoch 4851\n",
      "Training loss: 0.009759341183378071\n",
      "Test loss: 0.04676393805830567\n",
      "Starting epoch 4852\n",
      "Training loss: 0.009864344414262498\n",
      "Test loss: 0.046630745822632755\n",
      "Starting epoch 4853\n",
      "Training loss: 0.009879992435090855\n",
      "Test loss: 0.04666650943734028\n",
      "Starting epoch 4854\n",
      "Training loss: 0.009809644694333194\n",
      "Test loss: 0.04701675950653023\n",
      "Starting epoch 4855\n",
      "Training loss: 0.009829286867599995\n",
      "Test loss: 0.047098258096310824\n",
      "Starting epoch 4856\n",
      "Training loss: 0.009842810923325234\n",
      "Test loss: 0.04703014288787489\n",
      "Starting epoch 4857\n",
      "Training loss: 0.009899470649781774\n",
      "Test loss: 0.04706665611377469\n",
      "Starting epoch 4858\n",
      "Training loss: 0.009769480270867954\n",
      "Test loss: 0.04739377675233064\n",
      "Starting epoch 4859\n",
      "Training loss: 0.00978156815847901\n",
      "Test loss: 0.04700338067831816\n",
      "Starting epoch 4860\n",
      "Training loss: 0.009808357163775162\n",
      "Test loss: 0.04695646713177363\n",
      "Starting epoch 4861\n",
      "Training loss: 0.009791262821889803\n",
      "Test loss: 0.04664735164907244\n",
      "Starting epoch 4862\n",
      "Training loss: 0.009822173578450914\n",
      "Test loss: 0.046405762847926885\n",
      "Starting epoch 4863\n",
      "Training loss: 0.009887606195616917\n",
      "Test loss: 0.046683027788444804\n",
      "Starting epoch 4864\n",
      "Training loss: 0.009814424409729535\n",
      "Test loss: 0.04645417803139598\n",
      "Starting epoch 4865\n",
      "Training loss: 0.009883522880492641\n",
      "Test loss: 0.04643388458148197\n",
      "Starting epoch 4866\n",
      "Training loss: 0.009883394800737256\n",
      "Test loss: 0.04649774661218679\n",
      "Starting epoch 4867\n",
      "Training loss: 0.009839100946412712\n",
      "Test loss: 0.04636131268408564\n",
      "Starting epoch 4868\n",
      "Training loss: 0.009849479086086398\n",
      "Test loss: 0.046635256321341904\n",
      "Starting epoch 4869\n",
      "Training loss: 0.009820943194456765\n",
      "Test loss: 0.04674786812177411\n",
      "Starting epoch 4870\n",
      "Training loss: 0.009792324857877904\n",
      "Test loss: 0.046696307206595386\n",
      "Starting epoch 4871\n",
      "Training loss: 0.009837373312501634\n",
      "Test loss: 0.046937283404447416\n",
      "Starting epoch 4872\n",
      "Training loss: 0.009880300481483096\n",
      "Test loss: 0.04666937901465981\n",
      "Starting epoch 4873\n",
      "Training loss: 0.009928568754894812\n",
      "Test loss: 0.0470058880056496\n",
      "Starting epoch 4874\n",
      "Training loss: 0.009821478124769007\n",
      "Test loss: 0.046531579146782555\n",
      "Starting epoch 4875\n",
      "Training loss: 0.009857090480137066\n",
      "Test loss: 0.04666187172686612\n",
      "Starting epoch 4876\n",
      "Training loss: 0.00998048809524931\n",
      "Test loss: 0.04692282486293051\n",
      "Starting epoch 4877\n",
      "Training loss: 0.009839558118923765\n",
      "Test loss: 0.04675176215392572\n",
      "Starting epoch 4878\n",
      "Training loss: 0.009996223507723848\n",
      "Test loss: 0.047126438882615834\n",
      "Starting epoch 4879\n",
      "Training loss: 0.01000711175261951\n",
      "Test loss: 0.04687017919840636\n",
      "Starting epoch 4880\n",
      "Training loss: 0.009829824164387633\n",
      "Test loss: 0.047454676142445314\n",
      "Starting epoch 4881\n",
      "Training loss: 0.009761146323175216\n",
      "Test loss: 0.04713486231587551\n",
      "Starting epoch 4882\n",
      "Training loss: 0.009809810317075644\n",
      "Test loss: 0.046748105022642344\n",
      "Starting epoch 4883\n",
      "Training loss: 0.009814710493703357\n",
      "Test loss: 0.04683658660010055\n",
      "Starting epoch 4884\n",
      "Training loss: 0.009780589170509675\n",
      "Test loss: 0.046765002249567596\n",
      "Starting epoch 4885\n",
      "Training loss: 0.009784586544408173\n",
      "Test loss: 0.046859868698649935\n",
      "Starting epoch 4886\n",
      "Training loss: 0.009820747792293302\n",
      "Test loss: 0.0468832361339419\n",
      "Starting epoch 4887\n",
      "Training loss: 0.009832221007005114\n",
      "Test loss: 0.04682501253706438\n",
      "Starting epoch 4888\n",
      "Training loss: 0.01015777872173024\n",
      "Test loss: 0.046717595722940236\n",
      "Starting epoch 4889\n",
      "Training loss: 0.00979492080504777\n",
      "Test loss: 0.047364374415742025\n",
      "Starting epoch 4890\n",
      "Training loss: 0.009838522366080129\n",
      "Test loss: 0.04710396737964065\n",
      "Starting epoch 4891\n",
      "Training loss: 0.009906967796507429\n",
      "Test loss: 0.047027096842174175\n",
      "Starting epoch 4892\n",
      "Training loss: 0.00977390286985968\n",
      "Test loss: 0.04658285224879229\n",
      "Starting epoch 4893\n",
      "Training loss: 0.010012421039406394\n",
      "Test loss: 0.04665698276625739\n",
      "Starting epoch 4894\n",
      "Training loss: 0.009787755087018013\n",
      "Test loss: 0.04641675728338736\n",
      "Starting epoch 4895\n",
      "Training loss: 0.009877630845322961\n",
      "Test loss: 0.046417792362195474\n",
      "Starting epoch 4896\n",
      "Training loss: 0.009772817130948677\n",
      "Test loss: 0.04704026201808894\n",
      "Starting epoch 4897\n",
      "Training loss: 0.009761917023140877\n",
      "Test loss: 0.04697056983908018\n",
      "Starting epoch 4898\n",
      "Training loss: 0.009841267248524016\n",
      "Test loss: 0.04692445736792353\n",
      "Starting epoch 4899\n",
      "Training loss: 0.009815408100114494\n",
      "Test loss: 0.047155640881370614\n",
      "Starting epoch 4900\n",
      "Training loss: 0.009799149765281892\n",
      "Test loss: 0.046907965231824805\n",
      "Starting epoch 4901\n",
      "Training loss: 0.009779741437953026\n",
      "Test loss: 0.0466749627419092\n",
      "Starting epoch 4902\n",
      "Training loss: 0.009828739555277785\n",
      "Test loss: 0.04683724087145594\n",
      "Starting epoch 4903\n",
      "Training loss: 0.009878579878294077\n",
      "Test loss: 0.04689896134314714\n",
      "Starting epoch 4904\n",
      "Training loss: 0.009873964701641778\n",
      "Test loss: 0.04669206906799917\n",
      "Starting epoch 4905\n",
      "Training loss: 0.009829094412080089\n",
      "Test loss: 0.047024582133249\n",
      "Starting epoch 4906\n",
      "Training loss: 0.009804257748984411\n",
      "Test loss: 0.04681119012335936\n",
      "Starting epoch 4907\n",
      "Training loss: 0.009900559724659705\n",
      "Test loss: 0.04662867866594483\n",
      "Starting epoch 4908\n",
      "Training loss: 0.00980738503095068\n",
      "Test loss: 0.04620241139222075\n",
      "Starting epoch 4909\n",
      "Training loss: 0.00981408165248691\n",
      "Test loss: 0.046514168795612126\n",
      "Starting epoch 4910\n",
      "Training loss: 0.009845085503136525\n",
      "Test loss: 0.04701370359570892\n",
      "Starting epoch 4911\n",
      "Training loss: 0.009825126558053688\n",
      "Test loss: 0.04703309842281871\n",
      "Starting epoch 4912\n",
      "Training loss: 0.009786838825914215\n",
      "Test loss: 0.046668418027736525\n",
      "Starting epoch 4913\n",
      "Training loss: 0.009885885226005902\n",
      "Test loss: 0.046807652822247255\n",
      "Starting epoch 4914\n",
      "Training loss: 0.010033142951424004\n",
      "Test loss: 0.04642062316890116\n",
      "Starting epoch 4915\n",
      "Training loss: 0.009927037339962896\n",
      "Test loss: 0.046930288826977765\n",
      "Starting epoch 4916\n",
      "Training loss: 0.00979721193491924\n",
      "Test loss: 0.046509832419731\n",
      "Starting epoch 4917\n",
      "Training loss: 0.009916304732810279\n",
      "Test loss: 0.046584787054194346\n",
      "Starting epoch 4918\n",
      "Training loss: 0.00982234750675862\n",
      "Test loss: 0.04648711739314927\n",
      "Starting epoch 4919\n",
      "Training loss: 0.009925022263263092\n",
      "Test loss: 0.04699301581691812\n",
      "Starting epoch 4920\n",
      "Training loss: 0.009827541431686918\n",
      "Test loss: 0.04671945326306202\n",
      "Starting epoch 4921\n",
      "Training loss: 0.009874303199228694\n",
      "Test loss: 0.046799937194144284\n",
      "Starting epoch 4922\n",
      "Training loss: 0.009989970913309543\n",
      "Test loss: 0.04716968798527011\n",
      "Starting epoch 4923\n",
      "Training loss: 0.009851688427514717\n",
      "Test loss: 0.0465095122517259\n",
      "Starting epoch 4924\n",
      "Training loss: 0.009800080606927638\n",
      "Test loss: 0.04627505724352819\n",
      "Starting epoch 4925\n",
      "Training loss: 0.009844638987398539\n",
      "Test loss: 0.04663276492997452\n",
      "Starting epoch 4926\n",
      "Training loss: 0.009869431176024382\n",
      "Test loss: 0.04670658686922656\n",
      "Starting epoch 4927\n",
      "Training loss: 0.009779266280228974\n",
      "Test loss: 0.047112903109303224\n",
      "Starting epoch 4928\n",
      "Training loss: 0.009826241259570004\n",
      "Test loss: 0.047090566268673646\n",
      "Starting epoch 4929\n",
      "Training loss: 0.009845056456559505\n",
      "Test loss: 0.04683237922964273\n",
      "Starting epoch 4930\n",
      "Training loss: 0.009852455486161787\n",
      "Test loss: 0.04671170603897837\n",
      "Starting epoch 4931\n",
      "Training loss: 0.009839272951004936\n",
      "Test loss: 0.0470564468867249\n",
      "Starting epoch 4932\n",
      "Training loss: 0.00983948286500622\n",
      "Test loss: 0.0471699267487835\n",
      "Starting epoch 4933\n",
      "Training loss: 0.00985383529399262\n",
      "Test loss: 0.04725906873742739\n",
      "Starting epoch 4934\n",
      "Training loss: 0.009785700543615663\n",
      "Test loss: 0.04673016374861753\n",
      "Starting epoch 4935\n",
      "Training loss: 0.009842072827283476\n",
      "Test loss: 0.04674687167560613\n",
      "Starting epoch 4936\n",
      "Training loss: 0.009976226805907781\n",
      "Test loss: 0.04656641233574461\n",
      "Starting epoch 4937\n",
      "Training loss: 0.009839642395982976\n",
      "Test loss: 0.04638255125394574\n",
      "Starting epoch 4938\n",
      "Training loss: 0.010231262546215878\n",
      "Test loss: 0.046532034322067546\n",
      "Starting epoch 4939\n",
      "Training loss: 0.009777859754127557\n",
      "Test loss: 0.04751568411787351\n",
      "Starting epoch 4940\n",
      "Training loss: 0.009934685910578634\n",
      "Test loss: 0.047298316188432554\n",
      "Starting epoch 4941\n",
      "Training loss: 0.010064042982507924\n",
      "Test loss: 0.046850806998985785\n",
      "Starting epoch 4942\n",
      "Training loss: 0.009812311437286314\n",
      "Test loss: 0.04736951655811734\n",
      "Starting epoch 4943\n",
      "Training loss: 0.009851478315034851\n",
      "Test loss: 0.047473151719680536\n",
      "Starting epoch 4944\n",
      "Training loss: 0.009865256942442207\n",
      "Test loss: 0.04743247782742536\n",
      "Starting epoch 4945\n",
      "Training loss: 0.009808447593670399\n",
      "Test loss: 0.047316852060181123\n",
      "Starting epoch 4946\n",
      "Training loss: 0.009806037010228048\n",
      "Test loss: 0.04716099798679352\n",
      "Starting epoch 4947\n",
      "Training loss: 0.009811727055272118\n",
      "Test loss: 0.04696121853258875\n",
      "Starting epoch 4948\n",
      "Training loss: 0.009816512007449494\n",
      "Test loss: 0.04697494622733858\n",
      "Starting epoch 4949\n",
      "Training loss: 0.009949685822500557\n",
      "Test loss: 0.047192290839221746\n",
      "Starting epoch 4950\n",
      "Training loss: 0.00985452262532027\n",
      "Test loss: 0.04681433392343698\n",
      "Starting epoch 4951\n",
      "Training loss: 0.009853121687154302\n",
      "Test loss: 0.046619476543532476\n",
      "Starting epoch 4952\n",
      "Training loss: 0.00982845235677039\n",
      "Test loss: 0.04692078561142639\n",
      "Starting epoch 4953\n",
      "Training loss: 0.009792743143854572\n",
      "Test loss: 0.0473132285254973\n",
      "Starting epoch 4954\n",
      "Training loss: 0.00993443785815454\n",
      "Test loss: 0.04704900637820915\n",
      "Starting epoch 4955\n",
      "Training loss: 0.009974307670700745\n",
      "Test loss: 0.04683215543627739\n",
      "Starting epoch 4956\n",
      "Training loss: 0.01002786687163056\n",
      "Test loss: 0.04707075724447215\n",
      "Starting epoch 4957\n",
      "Training loss: 0.00978720484331983\n",
      "Test loss: 0.04749793862854993\n",
      "Starting epoch 4958\n",
      "Training loss: 0.009983972264606445\n",
      "Test loss: 0.04714501552559711\n",
      "Starting epoch 4959\n",
      "Training loss: 0.0098690945563502\n",
      "Test loss: 0.047449781386940566\n",
      "Starting epoch 4960\n",
      "Training loss: 0.009799569142890758\n",
      "Test loss: 0.04688497674134043\n",
      "Starting epoch 4961\n",
      "Training loss: 0.00987897770570927\n",
      "Test loss: 0.0468064677660112\n",
      "Starting epoch 4962\n",
      "Training loss: 0.009766442472206765\n",
      "Test loss: 0.047134346708103465\n",
      "Starting epoch 4963\n",
      "Training loss: 0.009873232040859636\n",
      "Test loss: 0.04701684139392994\n",
      "Starting epoch 4964\n",
      "Training loss: 0.00979839982922937\n",
      "Test loss: 0.04721476951683009\n",
      "Starting epoch 4965\n",
      "Training loss: 0.00980297753923252\n",
      "Test loss: 0.04704258218407631\n",
      "Starting epoch 4966\n",
      "Training loss: 0.009879605157575647\n",
      "Test loss: 0.04710089967206672\n",
      "Starting epoch 4967\n",
      "Training loss: 0.009925340821386361\n",
      "Test loss: 0.04739622468197787\n",
      "Starting epoch 4968\n",
      "Training loss: 0.009813579486408194\n",
      "Test loss: 0.04760927762146349\n",
      "Starting epoch 4969\n",
      "Training loss: 0.009919674212082487\n",
      "Test loss: 0.04731741685558249\n",
      "Starting epoch 4970\n",
      "Training loss: 0.00983909787762849\n",
      "Test loss: 0.047309948752323784\n",
      "Starting epoch 4971\n",
      "Training loss: 0.00981152401168327\n",
      "Test loss: 0.0471793193784025\n",
      "Starting epoch 4972\n",
      "Training loss: 0.009835832446935724\n",
      "Test loss: 0.04693762820076059\n",
      "Starting epoch 4973\n",
      "Training loss: 0.009878757883046494\n",
      "Test loss: 0.04691954260623014\n",
      "Starting epoch 4974\n",
      "Training loss: 0.009802447540349648\n",
      "Test loss: 0.04724175400204129\n",
      "Starting epoch 4975\n",
      "Training loss: 0.010027497380849768\n",
      "Test loss: 0.046886644429630704\n",
      "Starting epoch 4976\n",
      "Training loss: 0.009851145955016379\n",
      "Test loss: 0.04710375407227763\n",
      "Starting epoch 4977\n",
      "Training loss: 0.009916439469231934\n",
      "Test loss: 0.046773772341785605\n",
      "Starting epoch 4978\n",
      "Training loss: 0.009934065527603274\n",
      "Test loss: 0.04728809343995871\n",
      "Starting epoch 4979\n",
      "Training loss: 0.009918650825981234\n",
      "Test loss: 0.047428890648815364\n",
      "Starting epoch 4980\n",
      "Training loss: 0.009796543581197496\n",
      "Test loss: 0.047231229229105845\n",
      "Starting epoch 4981\n",
      "Training loss: 0.009911568392617781\n",
      "Test loss: 0.04703960015817925\n",
      "Starting epoch 4982\n",
      "Training loss: 0.009985181594603374\n",
      "Test loss: 0.04686430041436796\n",
      "Starting epoch 4983\n",
      "Training loss: 0.009875998588004073\n",
      "Test loss: 0.04746400868451154\n",
      "Starting epoch 4984\n",
      "Training loss: 0.009840678248065905\n",
      "Test loss: 0.04697612134946717\n",
      "Starting epoch 4985\n",
      "Training loss: 0.009877158435763882\n",
      "Test loss: 0.046461166331061614\n",
      "Starting epoch 4986\n",
      "Training loss: 0.009932555044527914\n",
      "Test loss: 0.04692498891165963\n",
      "Starting epoch 4987\n",
      "Training loss: 0.00984198374093556\n",
      "Test loss: 0.04669170730091907\n",
      "Starting epoch 4988\n",
      "Training loss: 0.009786249382696191\n",
      "Test loss: 0.046733287887440786\n",
      "Starting epoch 4989\n",
      "Training loss: 0.009837028768951775\n",
      "Test loss: 0.04671330736191184\n",
      "Starting epoch 4990\n",
      "Training loss: 0.00982674499820979\n",
      "Test loss: 0.046714008820277673\n",
      "Starting epoch 4991\n",
      "Training loss: 0.00978807637040488\n",
      "Test loss: 0.047165351609388985\n",
      "Starting epoch 4992\n",
      "Training loss: 0.009797664756169085\n",
      "Test loss: 0.047166827652189464\n",
      "Starting epoch 4993\n",
      "Training loss: 0.009836090804978471\n",
      "Test loss: 0.04715360914943395\n",
      "Starting epoch 4994\n",
      "Training loss: 0.00996412535304906\n",
      "Test loss: 0.0468477209409078\n",
      "Starting epoch 4995\n",
      "Training loss: 0.00977460546877052\n",
      "Test loss: 0.04652426757470325\n",
      "Starting epoch 4996\n",
      "Training loss: 0.009907972388213775\n",
      "Test loss: 0.04668669982088937\n",
      "Starting epoch 4997\n",
      "Training loss: 0.009786408119758621\n",
      "Test loss: 0.0473275711690938\n",
      "Starting epoch 4998\n",
      "Training loss: 0.009870303000827304\n",
      "Test loss: 0.047428024311860405\n",
      "Starting epoch 4999\n",
      "Training loss: 0.009785764148367232\n",
      "Test loss: 0.04683015247186025\n",
      "Starting epoch 5000\n",
      "Training loss: 0.009826011833597402\n",
      "Test loss: 0.0466184072472431\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_subsampler = torch.utils.data.SubsetRandomSampler(range(len(dataset_train_part)), gen)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset_train_part, \n",
    "                      batch_size=best_params[8], sampler=train_subsampler)\n",
    "\n",
    "test_subsampler =  torch.utils.data.SubsetRandomSampler(range(len(dataset_test_part)), gen)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "                      dataset_test_part, \n",
    "                      batch_size=best_params[8], sampler=test_subsampler)\n",
    "\n",
    "\n",
    "best_net = fit_model(learning_rate=best_params[1],epochs=best_params[2],hidden_size=best_params[3],input_size=17,loss_function=best_params[4],momentum=best_params[5],opt=best_params[6],output_size=1,trainloader=trainloader,weight_decay=best_params[7],testloader=testloader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set 0.942\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94       204\n",
      "           1       0.97      0.92      0.94       228\n",
      "\n",
      "    accuracy                           0.94       432\n",
      "   macro avg       0.94      0.94      0.94       432\n",
      "weighted avg       0.94      0.94      0.94       432\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "test_data = torch.from_numpy(dataset_test_part[:, 1:]).to(torch.float32)\n",
    "val_labels = torch.from_numpy(dataset_test_part[:, [0]]).to(torch.float32)\n",
    "\n",
    "#Print accuracy on test set\n",
    "test_outputs = best_net(test_data).round().int().view(-1)\n",
    "\n",
    "y_pred = best_net(test_data)\n",
    "y_pred = y_pred.round().int().view(-1)\n",
    "print(\"accuracy on test set {:.3f}\".format(accuracy_score( val_labels,y_pred)))\n",
    "print(classification_report(val_labels, \n",
    "                            y_pred, \n",
    "                            target_names=['0', '1']))\n",
    "\n",
    "#print the confusion matrix\n",
    "cm = confusion_matrix(val_labels, test_outputs)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
