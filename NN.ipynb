{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from monk_helpers import CV,SEED\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets Path\n",
    "TR_PATH = \"./monks/datasets/monks-3.train\"\n",
    "TS_PATH = \"./monks/datasets/monks-3.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_graph(train_losses,validation_losses,epochs):\n",
    "    num_epochs = list(range(1, epochs + 1))  \n",
    "    # Plotting\n",
    "    plt.plot(num_epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(num_epochs, validation_losses, label='Test Loss')\n",
    "\n",
    "    plt.title('Training and Validation Losses Across Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, units, output_size,):\n",
    "    super().__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.l1 = nn.Linear(input_size, units)\n",
    "    self.l2 = nn.Linear(units, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.tanh(self.l1(x))\n",
    "    out = torch.sigmoid(self.l2(out))\n",
    "    return out\n",
    "\n",
    "\n",
    "def reset_weights(net):\n",
    "  for param in net.parameters():\n",
    "    torch.nn.init.uniform_(param, a=-0.7, b=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ds(path):\n",
    "  \"\"\"\n",
    "  parse CSV data set and\n",
    "  returns a tuple (input, target)\n",
    "  \"\"\"\n",
    "  names = ['class', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'id']\n",
    "  data = pd.read_csv(path, dtype=object, delim_whitespace=True, header=None, skipinitialspace=True, names=names)\n",
    "\n",
    "  X = data.drop(['class','id'], axis=1)\n",
    "  X = pd.get_dummies(X).astype(float).to_numpy()\n",
    "  y = data.drop(['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'id'], axis=1)\n",
    "  y = y.astype(float).to_numpy()\n",
    "\n",
    "\n",
    "  return np.concatenate((y, X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(input_size,hidden_size,output_size,learning_rate,momentum,weight_decay,opt,epochs,trainloader,loss_function,testloader):\n",
    "    # Init the neural network\n",
    "    network = Net(input_size, hidden_size, output_size)\n",
    "    network.apply(reset_weights) #reset weights with random initialization\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    if (opt.__name__ == \"RMSprop\") or (opt.__name__ == \"SGD\"):\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "      # Print epoch\n",
    "      print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss and accuracy value for train\n",
    "      train_loss = 0.0\n",
    "      epoch_train_accuracy = []\n",
    "\n",
    "\n",
    "      # Set current loss and accuracy value for test\n",
    "      test_loss = 0.0\n",
    "      epoch_test_accuracy = []\n",
    "\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[:, 1:].to(torch.float32)\n",
    "        targets = data[:, [0]].to(torch.float32)\n",
    "\n",
    "        # Early stopping\n",
    "        #tolerance_stopper = ToleranceStopper(patience=10, min_delta=1e-4)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "      # Print loss values\n",
    "      epoch_train_loss = train_loss / len(trainloader.sampler.indices)    \n",
    "      print(f'Training loss: {epoch_train_loss}')\n",
    "      train_losses.append(epoch_train_loss)\n",
    "      # Update accuracy\n",
    "      for output, target in zip(outputs, targets):\n",
    "        output = 0 if output.item() < 0.5 else 1\n",
    "        if output == target.item():\n",
    "          epoch_train_accuracy.append(1)\n",
    "        else:\n",
    "          epoch_train_accuracy.append(0)\n",
    "      \n",
    "      with torch.no_grad():\n",
    "        # Iterate over the testing data and generate predictions\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "\n",
    "          inputs = data[:, 1:].to(torch.float32)\n",
    "          targets = data[:, [0]].to(torch.float32)\n",
    "        \n",
    "          outputs = network(inputs)\n",
    "\n",
    "          loss = loss_function(outputs, targets)\n",
    "          \n",
    "          test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_test_loss = test_loss / len(testloader.sampler.indices)    \n",
    "        print(f'Test loss: {epoch_test_loss}')\n",
    "        test_losses.append(epoch_test_loss)\n",
    "\n",
    "        # Update accuracy\n",
    "        for output, target in zip(outputs, targets):\n",
    "          output = 0 if output.item() < 0.5 else 1\n",
    "          if output == target.item():\n",
    "            epoch_test_accuracy.append(1)\n",
    "          else:\n",
    "            epoch_test_accuracy.append(0)\n",
    "\n",
    "    plot_graph(train_losses,test_losses,epochs)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_model(input_size,hidden_size,output_size,learning_rate,momentum,weight_decay,opt,epochs,trainloader,loss_function,validationloader):\n",
    "    \n",
    "    # Init the neural network\n",
    "    network = Net(input_size, hidden_size, output_size)\n",
    "    network.apply(reset_weights) #reset weights with random initialization\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    if (opt.__name__ == \"RMSprop\") or (opt.__name__ == \"SGD\"):\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    \n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "      # Print epoch\n",
    "      #print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss value\n",
    "      train_loss = 0.0\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[:, 1:].to(torch.float32)\n",
    "        targets = data[:, [0]].to(torch.float32)\n",
    "\n",
    "        # Early stopping\n",
    "        #tolerance_stopper = ToleranceStopper(patience=10, min_delta=1e-4)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        #print(\"loss per item\", loss.item())\n",
    "        #print(\"inputs size\",inputs.size(0))\n",
    "\n",
    "        # Print loss values\n",
    "      #print(\"train loaders length\",len(trainloader.sampler.indices))\n",
    "      avg_train_loss = train_loss / len(trainloader.sampler.indices)    \n",
    "      #print(f'Training loss: {avg_train_loss}')\n",
    "      # Print about testing\n",
    "      #print('Starting validation')\n",
    "\n",
    "      # Evaluationfor this fold\n",
    "      valid_loss = 0.0 \n",
    "      with torch.no_grad():\n",
    "        # Iterate over the validation data and generate predictions\n",
    "        for i, data in enumerate(validationloader, 0):\n",
    "\n",
    "          # Get inputs\n",
    "          inputs = data[:, 1:].to(torch.float32)\n",
    "          targets = data[:, [0]].to(torch.float32)\n",
    "          \n",
    "          # Generate outputs\n",
    "          outputs = network(inputs)\n",
    "\n",
    "          loss = loss_function(outputs, targets)\n",
    "\n",
    "          # Calculate loss\n",
    "          valid_loss += loss.item() * inputs.size(0)\n",
    "          #print(\"loss per item\", loss.item())\n",
    "          #print(\"inputs size\",inputs.size(0))\n",
    "        \n",
    "        #print(\"validation loaders length\",len(validationloader.sampler.indices))\n",
    "        avg_valid_loss = valid_loss / len(validationloader.sampler.indices) #used to find the best parameters of the model\n",
    "        # Print validation results\n",
    "        #print(f'Validation loss: {avg_valid_loss:.4f}')\n",
    "        \n",
    "\n",
    "    return avg_valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Actual iter 0.0% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 1.0416666666666665% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 2.083333333333333% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 3.125% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 4.166666666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 5.208333333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 6.25% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 7.291666666666667% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 8.333333333333332% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 9.375% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 10.416666666666668% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 11.458333333333332% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 12.5% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 13.541666666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 14.583333333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 15.625% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 16.666666666666664% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 17.708333333333336% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 18.75% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 19.791666666666664% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 20.833333333333336% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 21.875% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 22.916666666666664% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 23.958333333333336% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 25.0% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 26.041666666666668% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 27.083333333333332% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 28.125% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 29.166666666666668% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 30.208333333333332% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 31.25% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 32.29166666666667% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 33.33333333333333% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 34.375% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 35.41666666666667% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 36.45833333333333% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 37.5% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 38.54166666666667% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 39.58333333333333% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 40.625% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 41.66666666666667% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 42.70833333333333% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 43.75% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 44.79166666666667% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 45.83333333333333% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 46.875% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 47.91666666666667% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 48.95833333333333% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 50.0% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 51.041666666666664% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 52.083333333333336% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 53.125% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 54.166666666666664% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 55.208333333333336% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 56.25% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 57.291666666666664% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 58.333333333333336% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 59.375% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 60.416666666666664% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 61.458333333333336% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 62.5% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 63.541666666666664% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 64.58333333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 65.625% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 66.66666666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 67.70833333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 68.75% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 69.79166666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 70.83333333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 71.875% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 72.91666666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 73.95833333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 75.0% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 76.04166666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 77.08333333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 78.125% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 79.16666666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 80.20833333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 81.25% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 82.29166666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 83.33333333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 84.375% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 85.41666666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 86.45833333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 87.5% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 88.54166666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 89.58333333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 90.625% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 91.66666666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 92.70833333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 93.75% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 94.79166666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 95.83333333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 96.875% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 97.91666666666666% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " \n",
      " Actual iter 98.95833333333334% \n",
      " \n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Best hidden size: 4 \n",
      "Best learning rate: 0.5 \n",
      "Best batch size: 64 \n",
      "Best weight decay: 0.001 \n",
      "Best momentum: 0.9\n"
     ]
    }
   ],
   "source": [
    "input_size = 17  \n",
    "output_size = 1\n",
    "params_grid = {\n",
    "    \"hidden_size\": [4, 5],\n",
    "    \"learning_rate\": [ 0.01, 0.1, 0.5],\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"weight_decay\": [ 0.001, 0.01],\n",
    "    \"momentum\": [0.9, 0.99],\n",
    "    \"epochs\":[400],\n",
    "    \"optimizer\":[torch.optim.Adam,torch.optim.SGD]\n",
    "}\n",
    "\n",
    "params_grid_o = {\n",
    "    \"hidden_size\": [4],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"batch_size\": [64],\n",
    "    \"weight_decay\": [0.01],\n",
    "    \"momentum\": [0.09],\n",
    "    \"epochs\":[400, 600],\n",
    "    \"optimizer\":[torch.optim.SGD]\n",
    "}\n",
    "\n",
    "\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "# Set fixed random number seed\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "dataset_train_part = read_ds(TR_PATH)\n",
    "dataset_test_part = read_ds(TS_PATH)\n",
    "\n",
    "dataset = dataset_train_part\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = CV \n",
    "  \n",
    "# K-fold Cross Validation model evaluation\n",
    "best_params = None\n",
    "\n",
    "actual_it = 0\n",
    "total_iterations = len(params_grid[\"epochs\"]) * len(params_grid[\"optimizer\"]) * len(params_grid[\"hidden_size\"]) * len(params_grid[\"learning_rate\"]) * len(params_grid[\"batch_size\"]) * len(params_grid[\"weight_decay\"]) * len(params_grid[\"momentum\"])\n",
    "\n",
    "\n",
    "for epochs, opt, hidden_size, learning_rate, batch_size, weight_decay, momentum in product(params_grid[\"epochs\"],params_grid[\"optimizer\"], params_grid[\"hidden_size\"], params_grid[\"learning_rate\"], params_grid[\"batch_size\"], params_grid[\"weight_decay\"], params_grid[\"momentum\"]):\n",
    "    validation_avg_loss_fold = 0\n",
    "    num_iterations = 0\n",
    "    #print the actual percentage of the grid search\n",
    "    print(f'\\n \\n Actual iter {(actual_it/total_iterations)*100}% \\n \\n')\n",
    "    \n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(np.zeros(len(dataset)),dataset[:, 0])):\n",
    "\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids, gen) \n",
    "        validation_subsampler = torch.utils.data.SubsetRandomSampler(val_ids, gen) \n",
    "        # Print\n",
    "        print(f'FOLD {fold}')\n",
    "\n",
    "        print('--------------------------------')\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        \n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=batch_size, sampler=train_subsampler)\n",
    "        validationloader = torch.utils.data.DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size=batch_size, sampler=validation_subsampler)    \n",
    "        \n",
    "\n",
    "        validation_loss = k_fold_model(learning_rate=learning_rate,epochs=epochs,hidden_size=hidden_size,input_size=input_size,loss_function=loss_function,momentum=momentum\n",
    "                                                    ,opt=opt,output_size=output_size,trainloader=trainloader,weight_decay=weight_decay,validationloader=validationloader)   \n",
    "        validation_avg_loss_fold  += validation_loss\n",
    "        num_iterations += 1\n",
    "\n",
    "    actual_it = actual_it + 1\n",
    "\n",
    "    #validation average over all folds\n",
    "    validation_avg_loss_fold /= num_iterations\n",
    "\n",
    "    #best \n",
    "    if best_params is None or validation_avg_loss_fold < best_params[0]:\n",
    "        best_params = (validation_avg_loss_fold,learning_rate,epochs,hidden_size, loss_function,momentum,opt,weight_decay,batch_size)\n",
    "\n",
    "\n",
    "print(f\"Best hidden size: {best_params[3]} \\nBest learning rate: {best_params[1]} \\nBest batch size: {best_params[8]} \\nBest weight decay: {best_params[7]} \\nBest momentum: {best_params[5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Training loss: 0.25890486137788804\n",
      "Test loss: 0.2590479883882735\n",
      "Starting epoch 2\n",
      "Training loss: 0.24486826824360206\n",
      "Test loss: 0.2384619779056973\n",
      "Starting epoch 3\n",
      "Training loss: 0.22806063611976435\n",
      "Test loss: 0.21962473403524468\n",
      "Starting epoch 4\n",
      "Training loss: 0.21352756023406982\n",
      "Test loss: 0.20268265847806577\n",
      "Starting epoch 5\n",
      "Training loss: 0.19875342533236645\n",
      "Test loss: 0.1786872849420265\n",
      "Starting epoch 6\n",
      "Training loss: 0.17324520329960058\n",
      "Test loss: 0.1503821042952714\n",
      "Starting epoch 7\n",
      "Training loss: 0.14050442688777798\n",
      "Test loss: 0.1272507205053612\n",
      "Starting epoch 8\n",
      "Training loss: 0.11469092635346241\n",
      "Test loss: 0.09834668536980946\n",
      "Starting epoch 9\n",
      "Training loss: 0.08870321817573953\n",
      "Test loss: 0.06628837312261264\n",
      "Starting epoch 10\n",
      "Training loss: 0.0704715933467521\n",
      "Test loss: 0.04671730542624438\n",
      "Starting epoch 11\n",
      "Training loss: 0.06268355867168943\n",
      "Test loss: 0.03791800831203108\n",
      "Starting epoch 12\n",
      "Training loss: 0.058958384712211424\n",
      "Test loss: 0.034904172988953416\n",
      "Starting epoch 13\n",
      "Training loss: 0.05393863799142056\n",
      "Test loss: 0.03628213510469154\n",
      "Starting epoch 14\n",
      "Training loss: 0.04974496966136283\n",
      "Test loss: 0.0432220705681377\n",
      "Starting epoch 15\n",
      "Training loss: 0.047966077801634054\n",
      "Test loss: 0.04863354853457875\n",
      "Starting epoch 16\n",
      "Training loss: 0.048110124761941\n",
      "Test loss: 0.04349373832896904\n",
      "Starting epoch 17\n",
      "Training loss: 0.04558049655351482\n",
      "Test loss: 0.0387856003448919\n",
      "Starting epoch 18\n",
      "Training loss: 0.044469816610217094\n",
      "Test loss: 0.03853664147081198\n",
      "Starting epoch 19\n",
      "Training loss: 0.044026769819806834\n",
      "Test loss: 0.0388833968727677\n",
      "Starting epoch 20\n",
      "Training loss: 0.044073208555823466\n",
      "Test loss: 0.04133826811556463\n",
      "Starting epoch 21\n",
      "Training loss: 0.04357840462786253\n",
      "Test loss: 0.04283060092065069\n",
      "Starting epoch 22\n",
      "Training loss: 0.043378984952559235\n",
      "Test loss: 0.04268086118692601\n",
      "Starting epoch 23\n",
      "Training loss: 0.04303996356540039\n",
      "Test loss: 0.03874651163264557\n",
      "Starting epoch 24\n",
      "Training loss: 0.04297015077022255\n",
      "Test loss: 0.04075899858165671\n",
      "Starting epoch 25\n",
      "Training loss: 0.042001570406995835\n",
      "Test loss: 0.04154634275646121\n",
      "Starting epoch 26\n",
      "Training loss: 0.043509151664425115\n",
      "Test loss: 0.03969333596803524\n",
      "Starting epoch 27\n",
      "Training loss: 0.04225001227660257\n",
      "Test loss: 0.04432004745359774\n",
      "Starting epoch 28\n",
      "Training loss: 0.04178213541869257\n",
      "Test loss: 0.04400959431573197\n",
      "Starting epoch 29\n",
      "Training loss: 0.04216631483591971\n",
      "Test loss: 0.03979428867912955\n",
      "Starting epoch 30\n",
      "Training loss: 0.04153916117597799\n",
      "Test loss: 0.04209871286595309\n",
      "Starting epoch 31\n",
      "Training loss: 0.04070989445584719\n",
      "Test loss: 0.04269977272660644\n",
      "Starting epoch 32\n",
      "Training loss: 0.04082932292682226\n",
      "Test loss: 0.04200727120041847\n",
      "Starting epoch 33\n",
      "Training loss: 0.041061815851535954\n",
      "Test loss: 0.04336371321093153\n",
      "Starting epoch 34\n",
      "Training loss: 0.04074050681512864\n",
      "Test loss: 0.04010001514796858\n",
      "Starting epoch 35\n",
      "Training loss: 0.04028758021896003\n",
      "Test loss: 0.041143481654149515\n",
      "Starting epoch 36\n",
      "Training loss: 0.041493123977399266\n",
      "Test loss: 0.039930231800233876\n",
      "Starting epoch 37\n",
      "Training loss: 0.03969178275495279\n",
      "Test loss: 0.043980852597289614\n",
      "Starting epoch 38\n",
      "Training loss: 0.03979983553290367\n",
      "Test loss: 0.04294188134372234\n",
      "Starting epoch 39\n",
      "Training loss: 0.03980053448286213\n",
      "Test loss: 0.03879838685194651\n",
      "Starting epoch 40\n",
      "Training loss: 0.03963272039946474\n",
      "Test loss: 0.038626807362393094\n",
      "Starting epoch 41\n",
      "Training loss: 0.03913494248370655\n",
      "Test loss: 0.037838613614439964\n",
      "Starting epoch 42\n",
      "Training loss: 0.03860712698737129\n",
      "Test loss: 0.03654131983165388\n",
      "Starting epoch 43\n",
      "Training loss: 0.03872431955132328\n",
      "Test loss: 0.03663611329264111\n",
      "Starting epoch 44\n",
      "Training loss: 0.038151299916818496\n",
      "Test loss: 0.038402316746888335\n",
      "Starting epoch 45\n",
      "Training loss: 0.03781059781303171\n",
      "Test loss: 0.03894644706613488\n",
      "Starting epoch 46\n",
      "Training loss: 0.038465844619958126\n",
      "Test loss: 0.038200086879509466\n",
      "Starting epoch 47\n",
      "Training loss: 0.037093550272163794\n",
      "Test loss: 0.03442267107742804\n",
      "Starting epoch 48\n",
      "Training loss: 0.03722353914722067\n",
      "Test loss: 0.03180320609222959\n",
      "Starting epoch 49\n",
      "Training loss: 0.03736750280759374\n",
      "Test loss: 0.03227168990782014\n",
      "Starting epoch 50\n",
      "Training loss: 0.036488167575148285\n",
      "Test loss: 0.035170002116097346\n",
      "Starting epoch 51\n",
      "Training loss: 0.03625845267880158\n",
      "Test loss: 0.03979059363956804\n",
      "Starting epoch 52\n",
      "Training loss: 0.03672125153854245\n",
      "Test loss: 0.035991627585004876\n",
      "Starting epoch 53\n",
      "Training loss: 0.03599982708692551\n",
      "Test loss: 0.031288969054542204\n",
      "Starting epoch 54\n",
      "Training loss: 0.03506289798094601\n",
      "Test loss: 0.03095133740592886\n",
      "Starting epoch 55\n",
      "Training loss: 0.035125425696128705\n",
      "Test loss: 0.03359036851260397\n",
      "Starting epoch 56\n",
      "Training loss: 0.0341217470828627\n",
      "Test loss: 0.03289141381780306\n",
      "Starting epoch 57\n",
      "Training loss: 0.034820734446898836\n",
      "Test loss: 0.030092189674852072\n",
      "Starting epoch 58\n",
      "Training loss: 0.0339040894244538\n",
      "Test loss: 0.030329557756582897\n",
      "Starting epoch 59\n",
      "Training loss: 0.03304754081563872\n",
      "Test loss: 0.03314317838737258\n",
      "Starting epoch 60\n",
      "Training loss: 0.03290912142542542\n",
      "Test loss: 0.03278393905471872\n",
      "Starting epoch 61\n",
      "Training loss: 0.032288162556828044\n",
      "Test loss: 0.030063858738651982\n",
      "Starting epoch 62\n",
      "Training loss: 0.03304053406368514\n",
      "Test loss: 0.02722505631822127\n",
      "Starting epoch 63\n",
      "Training loss: 0.03194597794017831\n",
      "Test loss: 0.0295467817535003\n",
      "Starting epoch 64\n",
      "Training loss: 0.03138226728703155\n",
      "Test loss: 0.031770188085458895\n",
      "Starting epoch 65\n",
      "Training loss: 0.03129882221949882\n",
      "Test loss: 0.030094478693273332\n",
      "Starting epoch 66\n",
      "Training loss: 0.030650892218605418\n",
      "Test loss: 0.029031321199403867\n",
      "Starting epoch 67\n",
      "Training loss: 0.030530869777573914\n",
      "Test loss: 0.027862744060931383\n",
      "Starting epoch 68\n",
      "Training loss: 0.03020185048951477\n",
      "Test loss: 0.02837852740453349\n",
      "Starting epoch 69\n",
      "Training loss: 0.029780671000480652\n",
      "Test loss: 0.029258620614806812\n",
      "Starting epoch 70\n",
      "Training loss: 0.029742417765445398\n",
      "Test loss: 0.030293634092366253\n",
      "Starting epoch 71\n",
      "Training loss: 0.02951962097746427\n",
      "Test loss: 0.02943651474736355\n",
      "Starting epoch 72\n",
      "Training loss: 0.02901819256729767\n",
      "Test loss: 0.026741661752263706\n",
      "Starting epoch 73\n",
      "Training loss: 0.028980001685072164\n",
      "Test loss: 0.02696961599091689\n",
      "Starting epoch 74\n",
      "Training loss: 0.028620408878463212\n",
      "Test loss: 0.0271214471647033\n",
      "Starting epoch 75\n",
      "Training loss: 0.028481249254746516\n",
      "Test loss: 0.028916104110302748\n",
      "Starting epoch 76\n",
      "Training loss: 0.028006792740255106\n",
      "Test loss: 0.028670835826132033\n",
      "Starting epoch 77\n",
      "Training loss: 0.02776907530964398\n",
      "Test loss: 0.027773739563094244\n",
      "Starting epoch 78\n",
      "Training loss: 0.02840195443542277\n",
      "Test loss: 0.02612740994879493\n",
      "Starting epoch 79\n",
      "Training loss: 0.027384789813248836\n",
      "Test loss: 0.027769067290204542\n",
      "Starting epoch 80\n",
      "Training loss: 0.026934229753545074\n",
      "Test loss: 0.029819951530683925\n",
      "Starting epoch 81\n",
      "Training loss: 0.027592727761776722\n",
      "Test loss: 0.030436081052930268\n",
      "Starting epoch 82\n",
      "Training loss: 0.02708350494503975\n",
      "Test loss: 0.026570304227923905\n",
      "Starting epoch 83\n",
      "Training loss: 0.02644932734184578\n",
      "Test loss: 0.025268269771779026\n",
      "Starting epoch 84\n",
      "Training loss: 0.0266664839792447\n",
      "Test loss: 0.025957692621482745\n",
      "Starting epoch 85\n",
      "Training loss: 0.026190124039889358\n",
      "Test loss: 0.026506819244888093\n",
      "Starting epoch 86\n",
      "Training loss: 0.02599264638590031\n",
      "Test loss: 0.02798484017451604\n",
      "Starting epoch 87\n",
      "Training loss: 0.0261242074556038\n",
      "Test loss: 0.02672742665917785\n",
      "Starting epoch 88\n",
      "Training loss: 0.025539458347637146\n",
      "Test loss: 0.02695222019597336\n",
      "Starting epoch 89\n",
      "Training loss: 0.025752076253172804\n",
      "Test loss: 0.028452116030233877\n",
      "Starting epoch 90\n",
      "Training loss: 0.02528998982466635\n",
      "Test loss: 0.02702592613382472\n",
      "Starting epoch 91\n",
      "Training loss: 0.025202277192815405\n",
      "Test loss: 0.025319671879212063\n",
      "Starting epoch 92\n",
      "Training loss: 0.025385182289803614\n",
      "Test loss: 0.026744779923723802\n",
      "Starting epoch 93\n",
      "Training loss: 0.024689866077215947\n",
      "Test loss: 0.026442517147020058\n",
      "Starting epoch 94\n",
      "Training loss: 0.024584391215419183\n",
      "Test loss: 0.02722468889421887\n",
      "Starting epoch 95\n",
      "Training loss: 0.024483927357636513\n",
      "Test loss: 0.027251823632805434\n",
      "Starting epoch 96\n",
      "Training loss: 0.024288091121516267\n",
      "Test loss: 0.026153816220661003\n",
      "Starting epoch 97\n",
      "Training loss: 0.02463050959173773\n",
      "Test loss: 0.025121550731084966\n",
      "Starting epoch 98\n",
      "Training loss: 0.024173507558517767\n",
      "Test loss: 0.02727038944485011\n",
      "Starting epoch 99\n",
      "Training loss: 0.023842131138825027\n",
      "Test loss: 0.028193881804192508\n",
      "Starting epoch 100\n",
      "Training loss: 0.023865503045257\n",
      "Test loss: 0.02772115646964974\n",
      "Starting epoch 101\n",
      "Training loss: 0.023676348483709038\n",
      "Test loss: 0.02716851006779406\n",
      "Starting epoch 102\n",
      "Training loss: 0.023704040429142655\n",
      "Test loss: 0.026765262639081036\n",
      "Starting epoch 103\n",
      "Training loss: 0.023513898253440857\n",
      "Test loss: 0.02497216396861606\n",
      "Starting epoch 104\n",
      "Training loss: 0.023393424014087585\n",
      "Test loss: 0.025424923057909363\n",
      "Starting epoch 105\n",
      "Training loss: 0.02346156168057293\n",
      "Test loss: 0.028177115514322563\n",
      "Starting epoch 106\n",
      "Training loss: 0.02318255707133012\n",
      "Test loss: 0.027628534990880225\n",
      "Starting epoch 107\n",
      "Training loss: 0.022993495443561038\n",
      "Test loss: 0.026856492997871503\n",
      "Starting epoch 108\n",
      "Training loss: 0.022711601428931853\n",
      "Test loss: 0.026740516087523213\n",
      "Starting epoch 109\n",
      "Training loss: 0.02263440603970504\n",
      "Test loss: 0.026888660051756434\n",
      "Starting epoch 110\n",
      "Training loss: 0.02249302021914818\n",
      "Test loss: 0.02678787304709355\n",
      "Starting epoch 111\n",
      "Training loss: 0.02265226480657937\n",
      "Test loss: 0.026369375625142345\n",
      "Starting epoch 112\n",
      "Training loss: 0.02246975465143313\n",
      "Test loss: 0.027921263404466486\n",
      "Starting epoch 113\n",
      "Training loss: 0.022424504130345878\n",
      "Test loss: 0.02722470593397264\n",
      "Starting epoch 114\n",
      "Training loss: 0.0224833340826826\n",
      "Test loss: 0.026787114088182098\n",
      "Starting epoch 115\n",
      "Training loss: 0.022170727980918573\n",
      "Test loss: 0.028486134967318288\n",
      "Starting epoch 116\n",
      "Training loss: 0.02215196326619289\n",
      "Test loss: 0.028253088049866534\n",
      "Starting epoch 117\n",
      "Training loss: 0.02179921064220491\n",
      "Test loss: 0.02670430768005274\n",
      "Starting epoch 118\n",
      "Training loss: 0.021827294414893526\n",
      "Test loss: 0.02557401607433955\n",
      "Starting epoch 119\n",
      "Training loss: 0.02192402838683519\n",
      "Test loss: 0.026628187529880692\n",
      "Starting epoch 120\n",
      "Training loss: 0.021637306136430286\n",
      "Test loss: 0.027301783500998107\n",
      "Starting epoch 121\n",
      "Training loss: 0.02152042527545671\n",
      "Test loss: 0.028185614281230502\n",
      "Starting epoch 122\n",
      "Training loss: 0.02152780199148616\n",
      "Test loss: 0.028040041215717793\n",
      "Starting epoch 123\n",
      "Training loss: 0.021586053134476552\n",
      "Test loss: 0.027878790817878866\n",
      "Starting epoch 124\n",
      "Training loss: 0.0212314493465619\n",
      "Test loss: 0.026495701391939765\n",
      "Starting epoch 125\n",
      "Training loss: 0.021280498037755977\n",
      "Test loss: 0.025867010708208436\n",
      "Starting epoch 126\n",
      "Training loss: 0.021374215997877668\n",
      "Test loss: 0.02682508145355516\n",
      "Starting epoch 127\n",
      "Training loss: 0.021137520273933647\n",
      "Test loss: 0.02808406024619385\n",
      "Starting epoch 128\n",
      "Training loss: 0.021071830627004632\n",
      "Test loss: 0.02846642059308511\n",
      "Starting epoch 129\n",
      "Training loss: 0.021081326834735323\n",
      "Test loss: 0.02867298566356853\n",
      "Starting epoch 130\n",
      "Training loss: 0.020976199753216056\n",
      "Test loss: 0.027243479775885742\n",
      "Starting epoch 131\n",
      "Training loss: 0.020848135509696164\n",
      "Test loss: 0.02704748118089305\n",
      "Starting epoch 132\n",
      "Training loss: 0.02082731941195785\n",
      "Test loss: 0.02756204090460583\n",
      "Starting epoch 133\n",
      "Training loss: 0.020772971884637583\n",
      "Test loss: 0.028009384249647457\n",
      "Starting epoch 134\n",
      "Training loss: 0.0206861250468942\n",
      "Test loss: 0.028737787364257708\n",
      "Starting epoch 135\n",
      "Training loss: 0.02066794792037518\n",
      "Test loss: 0.029542237451231038\n",
      "Starting epoch 136\n",
      "Training loss: 0.02068893678608488\n",
      "Test loss: 0.029080805886122916\n",
      "Starting epoch 137\n",
      "Training loss: 0.020450406776527402\n",
      "Test loss: 0.02771114875321035\n",
      "Starting epoch 138\n",
      "Training loss: 0.020738610921458143\n",
      "Test loss: 0.026311740520651692\n",
      "Starting epoch 139\n",
      "Training loss: 0.020673250581030964\n",
      "Test loss: 0.028023159890263168\n",
      "Starting epoch 140\n",
      "Training loss: 0.020443409498109192\n",
      "Test loss: 0.02964126394578704\n",
      "Starting epoch 141\n",
      "Training loss: 0.020303868184812734\n",
      "Test loss: 0.029537827328399376\n",
      "Starting epoch 142\n",
      "Training loss: 0.02046629155940208\n",
      "Test loss: 0.028344300541060942\n",
      "Starting epoch 143\n",
      "Training loss: 0.020186697300828872\n",
      "Test loss: 0.028551234622244483\n",
      "Starting epoch 144\n",
      "Training loss: 0.02020997841094361\n",
      "Test loss: 0.0286870119334371\n",
      "Starting epoch 145\n",
      "Training loss: 0.020263758411661524\n",
      "Test loss: 0.02974734593320776\n",
      "Starting epoch 146\n",
      "Training loss: 0.02019929681278643\n",
      "Test loss: 0.02955477733027052\n",
      "Starting epoch 147\n",
      "Training loss: 0.019994474794776715\n",
      "Test loss: 0.028096013116063894\n",
      "Starting epoch 148\n",
      "Training loss: 0.020017109460151585\n",
      "Test loss: 0.027420363423449022\n",
      "Starting epoch 149\n",
      "Training loss: 0.020415822990605088\n",
      "Test loss: 0.027464468997937662\n",
      "Starting epoch 150\n",
      "Training loss: 0.01988263883185191\n",
      "Test loss: 0.029986807007204602\n",
      "Starting epoch 151\n",
      "Training loss: 0.019899599528947813\n",
      "Test loss: 0.03201847764904852\n",
      "Starting epoch 152\n",
      "Training loss: 0.020932503884322332\n",
      "Test loss: 0.03265128819340909\n",
      "Starting epoch 153\n",
      "Training loss: 0.019807466412665415\n",
      "Test loss: 0.028670534629512717\n",
      "Starting epoch 154\n",
      "Training loss: 0.019593783455793976\n",
      "Test loss: 0.02651625092106837\n",
      "Starting epoch 155\n",
      "Training loss: 0.020157406931040716\n",
      "Test loss: 0.026703994928134814\n",
      "Starting epoch 156\n",
      "Training loss: 0.020004588713655708\n",
      "Test loss: 0.028463538805091823\n",
      "Starting epoch 157\n",
      "Training loss: 0.019656672157713623\n",
      "Test loss: 0.031643258373218554\n",
      "Starting epoch 158\n",
      "Training loss: 0.019805361150351704\n",
      "Test loss: 0.0326020992188542\n",
      "Starting epoch 159\n",
      "Training loss: 0.01977208516270411\n",
      "Test loss: 0.030791518105952827\n",
      "Starting epoch 160\n",
      "Training loss: 0.019457111501547157\n",
      "Test loss: 0.028953937744652783\n",
      "Starting epoch 161\n",
      "Training loss: 0.019589757210895665\n",
      "Test loss: 0.0279159237527185\n",
      "Starting epoch 162\n",
      "Training loss: 0.019630331469730276\n",
      "Test loss: 0.028589998716833414\n",
      "Starting epoch 163\n",
      "Training loss: 0.01992651023214958\n",
      "Test loss: 0.03176254089231844\n",
      "Starting epoch 164\n",
      "Training loss: 0.01958256475932774\n",
      "Test loss: 0.03235304134863394\n",
      "Starting epoch 165\n",
      "Training loss: 0.019509601910583308\n",
      "Test loss: 0.030979265424388426\n",
      "Starting epoch 166\n",
      "Training loss: 0.01923299449511239\n",
      "Test loss: 0.029125836436395294\n",
      "Starting epoch 167\n",
      "Training loss: 0.01990245905567388\n",
      "Test loss: 0.027870132484369807\n",
      "Starting epoch 168\n",
      "Training loss: 0.019743461161851883\n",
      "Test loss: 0.030768440001540713\n",
      "Starting epoch 169\n",
      "Training loss: 0.01923211886868125\n",
      "Test loss: 0.03239857884882777\n",
      "Starting epoch 170\n",
      "Training loss: 0.019327902616780312\n",
      "Test loss: 0.032174238866126095\n",
      "Starting epoch 171\n",
      "Training loss: 0.019209817494647424\n",
      "Test loss: 0.03129366688706257\n",
      "Starting epoch 172\n",
      "Training loss: 0.01920892379139779\n",
      "Test loss: 0.02958835068124312\n",
      "Starting epoch 173\n",
      "Training loss: 0.019170279813105942\n",
      "Test loss: 0.02961945782105128\n",
      "Starting epoch 174\n",
      "Training loss: 0.019218040874502698\n",
      "Test loss: 0.030758851380259904\n",
      "Starting epoch 175\n",
      "Training loss: 0.019083193686531216\n",
      "Test loss: 0.03112542236016856\n",
      "Starting epoch 176\n",
      "Training loss: 0.0190549761240111\n",
      "Test loss: 0.03190491213980648\n",
      "Starting epoch 177\n",
      "Training loss: 0.019045896858709756\n",
      "Test loss: 0.03164380440419471\n",
      "Starting epoch 178\n",
      "Training loss: 0.019012190248878277\n",
      "Test loss: 0.03152074692425905\n",
      "Starting epoch 179\n",
      "Training loss: 0.018974641124244598\n",
      "Test loss: 0.03098853580929615\n",
      "Starting epoch 180\n",
      "Training loss: 0.018993093090162415\n",
      "Test loss: 0.030694776249152643\n",
      "Starting epoch 181\n",
      "Training loss: 0.019050506874918938\n",
      "Test loss: 0.031196769395912136\n",
      "Starting epoch 182\n",
      "Training loss: 0.01893004558247621\n",
      "Test loss: 0.0316805098619726\n",
      "Starting epoch 183\n",
      "Training loss: 0.018987572492390383\n",
      "Test loss: 0.032464328601404475\n",
      "Starting epoch 184\n",
      "Training loss: 0.019146542415999977\n",
      "Test loss: 0.031338585540652275\n",
      "Starting epoch 185\n",
      "Training loss: 0.01890454479477933\n",
      "Test loss: 0.03159666023458595\n",
      "Starting epoch 186\n",
      "Training loss: 0.018858383608157517\n",
      "Test loss: 0.03253154829144478\n",
      "Starting epoch 187\n",
      "Training loss: 0.018939057212384022\n",
      "Test loss: 0.03202007714383028\n",
      "Starting epoch 188\n",
      "Training loss: 0.018862066485109876\n",
      "Test loss: 0.03212969391434281\n",
      "Starting epoch 189\n",
      "Training loss: 0.018843101887185066\n",
      "Test loss: 0.032075250452315365\n",
      "Starting epoch 190\n",
      "Training loss: 0.01879422427689443\n",
      "Test loss: 0.03227988354585789\n",
      "Starting epoch 191\n",
      "Training loss: 0.018931837233363606\n",
      "Test loss: 0.0328278398072278\n",
      "Starting epoch 192\n",
      "Training loss: 0.018898949088131795\n",
      "Test loss: 0.03144430838249348\n",
      "Starting epoch 193\n",
      "Training loss: 0.018826103769242764\n",
      "Test loss: 0.03138943402855485\n",
      "Starting epoch 194\n",
      "Training loss: 0.018835755492575833\n",
      "Test loss: 0.03270507824641687\n",
      "Starting epoch 195\n",
      "Training loss: 0.018707273558515015\n",
      "Test loss: 0.033046089457692926\n",
      "Starting epoch 196\n",
      "Training loss: 0.018710792049399166\n",
      "Test loss: 0.0330404895875189\n",
      "Starting epoch 197\n",
      "Training loss: 0.0187414997852728\n",
      "Test loss: 0.031991741723484464\n",
      "Starting epoch 198\n",
      "Training loss: 0.018657264498169304\n",
      "Test loss: 0.03178622187287719\n",
      "Starting epoch 199\n",
      "Training loss: 0.018654656611749382\n",
      "Test loss: 0.03194462632139524\n",
      "Starting epoch 200\n",
      "Training loss: 0.01875035315141326\n",
      "Test loss: 0.03321302406213902\n",
      "Starting epoch 201\n",
      "Training loss: 0.01862127037688357\n",
      "Test loss: 0.032938710448366625\n",
      "Starting epoch 202\n",
      "Training loss: 0.018623382188990466\n",
      "Test loss: 0.03283269703388214\n",
      "Starting epoch 203\n",
      "Training loss: 0.018580847282390126\n",
      "Test loss: 0.03220987292351546\n",
      "Starting epoch 204\n",
      "Training loss: 0.018575337181081536\n",
      "Test loss: 0.031940068359728215\n",
      "Starting epoch 205\n",
      "Training loss: 0.01870321341958202\n",
      "Test loss: 0.03280399449997478\n",
      "Starting epoch 206\n",
      "Training loss: 0.018575405320305317\n",
      "Test loss: 0.032627690169546336\n",
      "Starting epoch 207\n",
      "Training loss: 0.018642192996549803\n",
      "Test loss: 0.03217458728424929\n",
      "Starting epoch 208\n",
      "Training loss: 0.01880457411046888\n",
      "Test loss: 0.03368002362549305\n",
      "Starting epoch 209\n",
      "Training loss: 0.018564620162131357\n",
      "Test loss: 0.03301772764987416\n",
      "Starting epoch 210\n",
      "Training loss: 0.018765722447242894\n",
      "Test loss: 0.031711921371795515\n",
      "Starting epoch 211\n",
      "Training loss: 0.018662871846349023\n",
      "Test loss: 0.033300618261650754\n",
      "Starting epoch 212\n",
      "Training loss: 0.018560664545072884\n",
      "Test loss: 0.03372133619807385\n",
      "Starting epoch 213\n",
      "Training loss: 0.01854757243981127\n",
      "Test loss: 0.03244679973081306\n",
      "Starting epoch 214\n",
      "Training loss: 0.01851006011005308\n",
      "Test loss: 0.03257745711339845\n",
      "Starting epoch 215\n",
      "Training loss: 0.0184982495351893\n",
      "Test loss: 0.03206130600085965\n",
      "Starting epoch 216\n",
      "Training loss: 0.018566723882419166\n",
      "Test loss: 0.03333469721730108\n",
      "Starting epoch 217\n",
      "Training loss: 0.018427324038548548\n",
      "Test loss: 0.033493970417314105\n",
      "Starting epoch 218\n",
      "Training loss: 0.018416220963489813\n",
      "Test loss: 0.032994185746819886\n",
      "Starting epoch 219\n",
      "Training loss: 0.018442227825766704\n",
      "Test loss: 0.03290318821867307\n",
      "Starting epoch 220\n",
      "Training loss: 0.018465249051080376\n",
      "Test loss: 0.03271860643117516\n",
      "Starting epoch 221\n",
      "Training loss: 0.01848911707762812\n",
      "Test loss: 0.03300769627094269\n",
      "Starting epoch 222\n",
      "Training loss: 0.018454192023052544\n",
      "Test loss: 0.03427906486171263\n",
      "Starting epoch 223\n",
      "Training loss: 0.018468488893303714\n",
      "Test loss: 0.034389999246707666\n",
      "Starting epoch 224\n",
      "Training loss: 0.018438572888491583\n",
      "Test loss: 0.033561328770937746\n",
      "Starting epoch 225\n",
      "Training loss: 0.018387808891959855\n",
      "Test loss: 0.032716427343311136\n",
      "Starting epoch 226\n",
      "Training loss: 0.018507471979885805\n",
      "Test loss: 0.03119778026033331\n",
      "Starting epoch 227\n",
      "Training loss: 0.018514806832202146\n",
      "Test loss: 0.03235130377665714\n",
      "Starting epoch 228\n",
      "Training loss: 0.01845244761006754\n",
      "Test loss: 0.03385007478020809\n",
      "Starting epoch 229\n",
      "Training loss: 0.01832916289873299\n",
      "Test loss: 0.03410826278505502\n",
      "Starting epoch 230\n",
      "Training loss: 0.0184402983086031\n",
      "Test loss: 0.0345857296552923\n",
      "Starting epoch 231\n",
      "Training loss: 0.018378567622333277\n",
      "Test loss: 0.03328020815496092\n",
      "Starting epoch 232\n",
      "Training loss: 0.018370093533494433\n",
      "Test loss: 0.03250972540290267\n",
      "Starting epoch 233\n",
      "Training loss: 0.018383419522862942\n",
      "Test loss: 0.032652298747389404\n",
      "Starting epoch 234\n",
      "Training loss: 0.018360596486046665\n",
      "Test loss: 0.03419524369140466\n",
      "Starting epoch 235\n",
      "Training loss: 0.018364719237338324\n",
      "Test loss: 0.033916686144140035\n",
      "Starting epoch 236\n",
      "Training loss: 0.01826065029093965\n",
      "Test loss: 0.03392790540776871\n",
      "Starting epoch 237\n",
      "Training loss: 0.018269207557571717\n",
      "Test loss: 0.0337307410383666\n",
      "Starting epoch 238\n",
      "Training loss: 0.018319850436365994\n",
      "Test loss: 0.03339342110686832\n",
      "Starting epoch 239\n",
      "Training loss: 0.0183604666657868\n",
      "Test loss: 0.03257513846512194\n",
      "Starting epoch 240\n",
      "Training loss: 0.018314382000291934\n",
      "Test loss: 0.033413989262448415\n",
      "Starting epoch 241\n",
      "Training loss: 0.018296369702601043\n",
      "Test loss: 0.03367917308652842\n",
      "Starting epoch 242\n",
      "Training loss: 0.018267749854531445\n",
      "Test loss: 0.03323891800310877\n",
      "Starting epoch 243\n",
      "Training loss: 0.01825660025914673\n",
      "Test loss: 0.03333216401989813\n",
      "Starting epoch 244\n",
      "Training loss: 0.01823190584412364\n",
      "Test loss: 0.03326959273329488\n",
      "Starting epoch 245\n",
      "Training loss: 0.018299823993297874\n",
      "Test loss: 0.03336824021405644\n",
      "Starting epoch 246\n",
      "Training loss: 0.01821652552509894\n",
      "Test loss: 0.03250034967506373\n",
      "Starting epoch 247\n",
      "Training loss: 0.01828611663496885\n",
      "Test loss: 0.03234401359050362\n",
      "Starting epoch 248\n",
      "Training loss: 0.01821115475575455\n",
      "Test loss: 0.033079382997971994\n",
      "Starting epoch 249\n",
      "Training loss: 0.018180586962548435\n",
      "Test loss: 0.03386885314076035\n",
      "Starting epoch 250\n",
      "Training loss: 0.01845690498097998\n",
      "Test loss: 0.03531469570265876\n",
      "Starting epoch 251\n",
      "Training loss: 0.018309031506298017\n",
      "Test loss: 0.03391329171480956\n",
      "Starting epoch 252\n",
      "Training loss: 0.018149269103515345\n",
      "Test loss: 0.032530336882229206\n",
      "Starting epoch 253\n",
      "Training loss: 0.018434110021249193\n",
      "Test loss: 0.03151065304323479\n",
      "Starting epoch 254\n",
      "Training loss: 0.01827902009435853\n",
      "Test loss: 0.03271943206588427\n",
      "Starting epoch 255\n",
      "Training loss: 0.018203781612339567\n",
      "Test loss: 0.034639954842903\n",
      "Starting epoch 256\n",
      "Training loss: 0.018204415675069464\n",
      "Test loss: 0.03569237322167114\n",
      "Starting epoch 257\n",
      "Training loss: 0.01834172144776485\n",
      "Test loss: 0.03479415523233237\n",
      "Starting epoch 258\n",
      "Training loss: 0.018171179291532665\n",
      "Test loss: 0.03347742178097919\n",
      "Starting epoch 259\n",
      "Training loss: 0.01814374605529621\n",
      "Test loss: 0.03233103329936663\n",
      "Starting epoch 260\n",
      "Training loss: 0.01830146544170184\n",
      "Test loss: 0.03184325792999179\n",
      "Starting epoch 261\n",
      "Training loss: 0.018184532670945417\n",
      "Test loss: 0.03306036042394461\n",
      "Starting epoch 262\n",
      "Training loss: 0.01836915561532388\n",
      "Test loss: 0.0354221368001567\n",
      "Starting epoch 263\n",
      "Training loss: 0.018284471171190503\n",
      "Test loss: 0.03500609189547874\n",
      "Starting epoch 264\n",
      "Training loss: 0.018163383312401225\n",
      "Test loss: 0.03334254122994564\n",
      "Starting epoch 265\n",
      "Training loss: 0.018261492206547103\n",
      "Test loss: 0.03315678507917457\n",
      "Starting epoch 266\n",
      "Training loss: 0.01811287808613699\n",
      "Test loss: 0.03210497802744309\n",
      "Starting epoch 267\n",
      "Training loss: 0.018720643457452783\n",
      "Test loss: 0.031174183443740563\n",
      "Starting epoch 268\n",
      "Training loss: 0.018401849877516755\n",
      "Test loss: 0.03425023842740942\n",
      "Starting epoch 269\n",
      "Training loss: 0.018131540630195963\n",
      "Test loss: 0.03489547926518652\n",
      "Starting epoch 270\n",
      "Training loss: 0.018219629669042885\n",
      "Test loss: 0.03594519059967111\n",
      "Starting epoch 271\n",
      "Training loss: 0.01838505350541873\n",
      "Test loss: 0.03518638042388139\n",
      "Starting epoch 272\n",
      "Training loss: 0.018128905781224124\n",
      "Test loss: 0.032156461366900695\n",
      "Starting epoch 273\n",
      "Training loss: 0.018132159532215752\n",
      "Test loss: 0.03155509548054801\n",
      "Starting epoch 274\n",
      "Training loss: 0.018233414976017884\n",
      "Test loss: 0.03244495143493017\n",
      "Starting epoch 275\n",
      "Training loss: 0.018172101484092533\n",
      "Test loss: 0.03377401511426325\n",
      "Starting epoch 276\n",
      "Training loss: 0.01804324299035991\n",
      "Test loss: 0.03421853151586321\n",
      "Starting epoch 277\n",
      "Training loss: 0.01805683243714395\n",
      "Test loss: 0.033794251167111926\n",
      "Starting epoch 278\n",
      "Training loss: 0.018040034278738695\n",
      "Test loss: 0.033474364697381305\n",
      "Starting epoch 279\n",
      "Training loss: 0.018110849482358478\n",
      "Test loss: 0.03352771816706216\n",
      "Starting epoch 280\n",
      "Training loss: 0.018049845075021026\n",
      "Test loss: 0.032479718396509136\n",
      "Starting epoch 281\n",
      "Training loss: 0.018053592320104114\n",
      "Test loss: 0.03253602878087097\n",
      "Starting epoch 282\n",
      "Training loss: 0.018045865396251443\n",
      "Test loss: 0.03255845192405912\n",
      "Starting epoch 283\n",
      "Training loss: 0.018037990636390742\n",
      "Test loss: 0.03321807659058659\n",
      "Starting epoch 284\n",
      "Training loss: 0.018113030960447477\n",
      "Test loss: 0.03455333446187002\n",
      "Starting epoch 285\n",
      "Training loss: 0.01809319615608356\n",
      "Test loss: 0.03383479177675865\n",
      "Starting epoch 286\n",
      "Training loss: 0.018073149208651215\n",
      "Test loss: 0.03307682848363011\n",
      "Starting epoch 287\n",
      "Training loss: 0.018233110364831863\n",
      "Test loss: 0.03225795690108229\n",
      "Starting epoch 288\n",
      "Training loss: 0.01809586361661309\n",
      "Test loss: 0.03302589962603869\n",
      "Starting epoch 289\n",
      "Training loss: 0.018053923031223603\n",
      "Test loss: 0.03457473532331211\n",
      "Starting epoch 290\n",
      "Training loss: 0.018227349035441875\n",
      "Test loss: 0.03562964236846677\n",
      "Starting epoch 291\n",
      "Training loss: 0.018164376529758094\n",
      "Test loss: 0.03402374560634295\n",
      "Starting epoch 292\n",
      "Training loss: 0.018203726618504914\n",
      "Test loss: 0.03188808069184974\n",
      "Starting epoch 293\n",
      "Training loss: 0.018119567684584954\n",
      "Test loss: 0.0319049219014468\n",
      "Starting epoch 294\n",
      "Training loss: 0.018087088695315064\n",
      "Test loss: 0.03296816266245312\n",
      "Starting epoch 295\n",
      "Training loss: 0.018058511489605317\n",
      "Test loss: 0.035044633955867203\n",
      "Starting epoch 296\n",
      "Training loss: 0.01827720516040677\n",
      "Test loss: 0.03565299262603124\n",
      "Starting epoch 297\n",
      "Training loss: 0.01825480179128344\n",
      "Test loss: 0.033368779829254853\n",
      "Starting epoch 298\n",
      "Training loss: 0.018079528615611497\n",
      "Test loss: 0.03220896657418321\n",
      "Starting epoch 299\n",
      "Training loss: 0.018127245019327422\n",
      "Test loss: 0.03254094637102551\n",
      "Starting epoch 300\n",
      "Training loss: 0.01806049865716305\n",
      "Test loss: 0.03297424868301109\n",
      "Starting epoch 301\n",
      "Training loss: 0.018052034843407692\n",
      "Test loss: 0.033814194439737884\n",
      "Starting epoch 302\n",
      "Training loss: 0.018170101934524834\n",
      "Test loss: 0.034426932188647764\n",
      "Starting epoch 303\n",
      "Training loss: 0.018076782465958206\n",
      "Test loss: 0.033787154213145924\n",
      "Starting epoch 304\n",
      "Training loss: 0.018018035263922372\n",
      "Test loss: 0.03278258525662952\n",
      "Starting epoch 305\n",
      "Training loss: 0.018058107768903015\n",
      "Test loss: 0.03201081996990575\n",
      "Starting epoch 306\n",
      "Training loss: 0.018087988001767728\n",
      "Test loss: 0.03267311898094637\n",
      "Starting epoch 307\n",
      "Training loss: 0.018069895259181006\n",
      "Test loss: 0.0333002433180809\n",
      "Starting epoch 308\n",
      "Training loss: 0.018113596700742595\n",
      "Test loss: 0.03458385307479788\n",
      "Starting epoch 309\n",
      "Training loss: 0.018121099130052033\n",
      "Test loss: 0.034261001342976535\n",
      "Starting epoch 310\n",
      "Training loss: 0.01803238247139532\n",
      "Test loss: 0.03336132079776791\n",
      "Starting epoch 311\n",
      "Training loss: 0.018011139492031005\n",
      "Test loss: 0.03289577061379397\n",
      "Starting epoch 312\n",
      "Training loss: 0.018075409818623888\n",
      "Test loss: 0.03299489300008173\n",
      "Starting epoch 313\n",
      "Training loss: 0.018071288731498797\n",
      "Test loss: 0.03240589411170394\n",
      "Starting epoch 314\n",
      "Training loss: 0.018103397497143903\n",
      "Test loss: 0.03326456586795824\n",
      "Starting epoch 315\n",
      "Training loss: 0.01800583752605026\n",
      "Test loss: 0.03384387624208574\n",
      "Starting epoch 316\n",
      "Training loss: 0.018041251120386553\n",
      "Test loss: 0.034171922201359714\n",
      "Starting epoch 317\n",
      "Training loss: 0.018043993895903963\n",
      "Test loss: 0.033770755071330955\n",
      "Starting epoch 318\n",
      "Training loss: 0.01811484934487304\n",
      "Test loss: 0.03298009531917395\n",
      "Starting epoch 319\n",
      "Training loss: 0.018105205698091476\n",
      "Test loss: 0.03289207271127789\n",
      "Starting epoch 320\n",
      "Training loss: 0.018137142428609192\n",
      "Test loss: 0.0321092878088907\n",
      "Starting epoch 321\n",
      "Training loss: 0.018150887497868695\n",
      "Test loss: 0.03249538110362159\n",
      "Starting epoch 322\n",
      "Training loss: 0.018059514982045673\n",
      "Test loss: 0.03430646533767382\n",
      "Starting epoch 323\n",
      "Training loss: 0.018164977064875305\n",
      "Test loss: 0.035197357129719525\n",
      "Starting epoch 324\n",
      "Training loss: 0.018073673375317307\n",
      "Test loss: 0.0340438585176512\n",
      "Starting epoch 325\n",
      "Training loss: 0.018069752293532013\n",
      "Test loss: 0.03235954542954763\n",
      "Starting epoch 326\n",
      "Training loss: 0.01811426606212483\n",
      "Test loss: 0.03242298808914644\n",
      "Starting epoch 327\n",
      "Training loss: 0.01807065950859277\n",
      "Test loss: 0.03285485906181512\n",
      "Starting epoch 328\n",
      "Training loss: 0.01801204650861318\n",
      "Test loss: 0.03302127543698858\n",
      "Starting epoch 329\n",
      "Training loss: 0.01802433630237814\n",
      "Test loss: 0.03357763388366611\n",
      "Starting epoch 330\n",
      "Training loss: 0.018083977375607022\n",
      "Test loss: 0.03323317767569312\n",
      "Starting epoch 331\n",
      "Training loss: 0.018087057442572274\n",
      "Test loss: 0.03411416964674437\n",
      "Starting epoch 332\n",
      "Training loss: 0.018026826826886076\n",
      "Test loss: 0.033537887726668957\n",
      "Starting epoch 333\n",
      "Training loss: 0.01807109695538634\n",
      "Test loss: 0.03290803344161422\n",
      "Starting epoch 334\n",
      "Training loss: 0.018174451088807622\n",
      "Test loss: 0.03250736308594545\n",
      "Starting epoch 335\n",
      "Training loss: 0.017993773639079978\n",
      "Test loss: 0.03339818140698804\n",
      "Starting epoch 336\n",
      "Training loss: 0.01798203825706341\n",
      "Test loss: 0.03418881335744151\n",
      "Starting epoch 337\n",
      "Training loss: 0.018075396566361677\n",
      "Test loss: 0.03449437739672484\n",
      "Starting epoch 338\n",
      "Training loss: 0.0180499758876738\n",
      "Test loss: 0.033463966101408005\n",
      "Starting epoch 339\n",
      "Training loss: 0.018022113128519448\n",
      "Test loss: 0.033241468447226065\n",
      "Starting epoch 340\n",
      "Training loss: 0.01798755788534391\n",
      "Test loss: 0.03249566339784198\n",
      "Starting epoch 341\n",
      "Training loss: 0.018117661022993385\n",
      "Test loss: 0.03190023314069818\n",
      "Starting epoch 342\n",
      "Training loss: 0.018062404524840293\n",
      "Test loss: 0.032736186352041036\n",
      "Starting epoch 343\n",
      "Training loss: 0.018024196542921613\n",
      "Test loss: 0.034193851743583324\n",
      "Starting epoch 344\n",
      "Training loss: 0.018036634432243518\n",
      "Test loss: 0.03441557398548833\n",
      "Starting epoch 345\n",
      "Training loss: 0.018224824372617923\n",
      "Test loss: 0.0345942955809059\n",
      "Starting epoch 346\n",
      "Training loss: 0.01817756788957803\n",
      "Test loss: 0.032214553060906904\n",
      "Starting epoch 347\n",
      "Training loss: 0.018038834124559262\n",
      "Test loss: 0.031997482809755534\n",
      "Starting epoch 348\n",
      "Training loss: 0.01816531053942735\n",
      "Test loss: 0.033300800111006806\n",
      "Starting epoch 349\n",
      "Training loss: 0.018025578442411344\n",
      "Test loss: 0.03374116743604342\n",
      "Starting epoch 350\n",
      "Training loss: 0.018030816521190228\n",
      "Test loss: 0.033664012879685117\n",
      "Starting epoch 351\n",
      "Training loss: 0.01797073932944751\n",
      "Test loss: 0.03282936655536846\n",
      "Starting epoch 352\n",
      "Training loss: 0.018001501708001386\n",
      "Test loss: 0.0322164915226124\n",
      "Starting epoch 353\n",
      "Training loss: 0.017992554537829806\n",
      "Test loss: 0.03220122417917958\n",
      "Starting epoch 354\n",
      "Training loss: 0.018044879171447675\n",
      "Test loss: 0.032412840122426\n",
      "Starting epoch 355\n",
      "Training loss: 0.017960991328734844\n",
      "Test loss: 0.03366354459689723\n",
      "Starting epoch 356\n",
      "Training loss: 0.01805870048701763\n",
      "Test loss: 0.0337825868692663\n",
      "Starting epoch 357\n",
      "Training loss: 0.018001487356473188\n",
      "Test loss: 0.03437731028706939\n",
      "Starting epoch 358\n",
      "Training loss: 0.0180624822673739\n",
      "Test loss: 0.03417496265912497\n",
      "Starting epoch 359\n",
      "Training loss: 0.018089470300884522\n",
      "Test loss: 0.032458362342030915\n",
      "Starting epoch 360\n",
      "Training loss: 0.0181650631740445\n",
      "Test loss: 0.03184110691977872\n",
      "Starting epoch 361\n",
      "Training loss: 0.018177942846153605\n",
      "Test loss: 0.0338230622863328\n",
      "Starting epoch 362\n",
      "Training loss: 0.018029374452155144\n",
      "Test loss: 0.03432631885839833\n",
      "Starting epoch 363\n",
      "Training loss: 0.018043891267209757\n",
      "Test loss: 0.034055277360258276\n",
      "Starting epoch 364\n",
      "Training loss: 0.017965648445438166\n",
      "Test loss: 0.03329253576144024\n",
      "Starting epoch 365\n",
      "Training loss: 0.018040177427598687\n",
      "Test loss: 0.03222525492310524\n",
      "Starting epoch 366\n",
      "Training loss: 0.018018621157427302\n",
      "Test loss: 0.032497262513196026\n",
      "Starting epoch 367\n",
      "Training loss: 0.018008984549001593\n",
      "Test loss: 0.03309098072350025\n",
      "Starting epoch 368\n",
      "Training loss: 0.018012808086197884\n",
      "Test loss: 0.034269025187111564\n",
      "Starting epoch 369\n",
      "Training loss: 0.01809349782825982\n",
      "Test loss: 0.03382887222148754\n",
      "Starting epoch 370\n",
      "Training loss: 0.018128532977377782\n",
      "Test loss: 0.03298482850745872\n",
      "Starting epoch 371\n",
      "Training loss: 0.017997670323267333\n",
      "Test loss: 0.033126553796507696\n",
      "Starting epoch 372\n",
      "Training loss: 0.018019492447864813\n",
      "Test loss: 0.033889580931928426\n",
      "Starting epoch 373\n",
      "Training loss: 0.018007039565776217\n",
      "Test loss: 0.03352485762702094\n",
      "Starting epoch 374\n",
      "Training loss: 0.01802002577508082\n",
      "Test loss: 0.03335878232287036\n",
      "Starting epoch 375\n",
      "Training loss: 0.01801411749520263\n",
      "Test loss: 0.033214752182916356\n",
      "Starting epoch 376\n",
      "Training loss: 0.018036486260348657\n",
      "Test loss: 0.032753715498579875\n",
      "Starting epoch 377\n",
      "Training loss: 0.01805039428052355\n",
      "Test loss: 0.032842646180479614\n",
      "Starting epoch 378\n",
      "Training loss: 0.017975205128065875\n",
      "Test loss: 0.03344169397045065\n",
      "Starting epoch 379\n",
      "Training loss: 0.01810580940886599\n",
      "Test loss: 0.03463459208055779\n",
      "Starting epoch 380\n",
      "Training loss: 0.01801713262913657\n",
      "Test loss: 0.03404469408646778\n",
      "Starting epoch 381\n",
      "Training loss: 0.01811978546139158\n",
      "Test loss: 0.03233205458080327\n",
      "Starting epoch 382\n",
      "Training loss: 0.017988712725336434\n",
      "Test loss: 0.032439803083737694\n",
      "Starting epoch 383\n",
      "Training loss: 0.01800781125050099\n",
      "Test loss: 0.03321335754460759\n",
      "Starting epoch 384\n",
      "Training loss: 0.018014889179927406\n",
      "Test loss: 0.0335344270699554\n",
      "Starting epoch 385\n",
      "Training loss: 0.018033048718190583\n",
      "Test loss: 0.03317903892861472\n",
      "Starting epoch 386\n",
      "Training loss: 0.018070324461479655\n",
      "Test loss: 0.03403857515917884\n",
      "Starting epoch 387\n",
      "Training loss: 0.017981134767292954\n",
      "Test loss: 0.033654010820168036\n",
      "Starting epoch 388\n",
      "Training loss: 0.018038256307605836\n",
      "Test loss: 0.03279049459982802\n",
      "Starting epoch 389\n",
      "Training loss: 0.018021832724086573\n",
      "Test loss: 0.03271912997243581\n",
      "Starting epoch 390\n",
      "Training loss: 0.017983993362696446\n",
      "Test loss: 0.03339568939473894\n",
      "Starting epoch 391\n",
      "Training loss: 0.018010819849909328\n",
      "Test loss: 0.033698837928198\n",
      "Starting epoch 392\n",
      "Training loss: 0.017995845053161753\n",
      "Test loss: 0.0342716978242\n",
      "Starting epoch 393\n",
      "Training loss: 0.018001476852375953\n",
      "Test loss: 0.03405658700675876\n",
      "Starting epoch 394\n",
      "Training loss: 0.01800359067980383\n",
      "Test loss: 0.03334507277166402\n",
      "Starting epoch 395\n",
      "Training loss: 0.01796376706696436\n",
      "Test loss: 0.03290937868533311\n",
      "Starting epoch 396\n",
      "Training loss: 0.018041340237269637\n",
      "Test loss: 0.03245665222682335\n",
      "Starting epoch 397\n",
      "Training loss: 0.018000802116804434\n",
      "Test loss: 0.03275466103244711\n",
      "Starting epoch 398\n",
      "Training loss: 0.01796470878676313\n",
      "Test loss: 0.033888421676777025\n",
      "Starting epoch 399\n",
      "Training loss: 0.018077577540620428\n",
      "Test loss: 0.03463069749651132\n",
      "Starting epoch 400\n",
      "Training loss: 0.018078737281507155\n",
      "Test loss: 0.033345170595027784\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_subsampler = torch.utils.data.SubsetRandomSampler(range(len(dataset_train_part)), gen)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset_train_part, \n",
    "                      batch_size=best_params[8], sampler=train_subsampler)\n",
    "\n",
    "test_subsampler =  torch.utils.data.SubsetRandomSampler(range(len(dataset_test_part)), gen)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "                      dataset_test_part, \n",
    "                      batch_size=best_params[8], sampler=test_subsampler)\n",
    "\n",
    "\n",
    "best_net = fit_model(learning_rate=best_params[1],epochs=best_params[2],hidden_size=best_params[3],input_size=17,loss_function=best_params[4],momentum=best_params[5],opt=best_params[6],output_size=1,trainloader=trainloader,weight_decay=best_params[7],testloader=testloader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set 0.961\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       204\n",
      "           1       1.00      0.93      0.96       228\n",
      "\n",
      "    accuracy                           0.96       432\n",
      "   macro avg       0.96      0.96      0.96       432\n",
      "weighted avg       0.96      0.96      0.96       432\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "test_data = torch.from_numpy(dataset_test_part[:, 1:]).to(torch.float32)\n",
    "val_labels = torch.from_numpy(dataset_test_part[:, [0]]).to(torch.float32)\n",
    "\n",
    "#Print accuracy on test set\n",
    "test_outputs = best_net(test_data).round().int().view(-1)\n",
    "\n",
    "y_pred = best_net(test_data)\n",
    "y_pred = y_pred.round().int().view(-1)\n",
    "print(\"accuracy on test set {:.3f}\".format(accuracy_score( val_labels,y_pred)))\n",
    "print(classification_report(val_labels, \n",
    "                            y_pred, \n",
    "                            target_names=['0', '1']))\n",
    "\n",
    "#print the confusion matrix\n",
    "cm = confusion_matrix(val_labels, test_outputs)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
