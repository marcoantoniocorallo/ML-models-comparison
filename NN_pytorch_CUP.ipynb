{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from monk_helpers import CV,SEED\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets Path\n",
    "TR_PATH = \"./data/ML-CUP23-TR.csv\"\n",
    "TS_PATH = \"./data/ML-CUP23-TS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_graph(train_losses,validation_losses,epochs):\n",
    "    num_epochs = list(range(1, epochs + 1))  \n",
    "    # Plotting\n",
    "    plt.plot(num_epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(num_epochs, validation_losses, label='Test Loss')\n",
    "\n",
    "    plt.title('Training and Validation Losses Across Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, units, output_size):\n",
    "    super().__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.l1 = nn.Linear(input_size, units)\n",
    "    self.l2 = nn.Linear(units, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.tanh(self.l1(x))\n",
    "    out = self.l2(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def reset_weights(net):\n",
    "  for param in net.parameters():\n",
    "    torch.nn.init.uniform_(param, a=-0.7, b=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ds(path):\n",
    "  \"\"\"\n",
    "  parse CSV data set and\n",
    "  returns a tuple (input, target)\n",
    "  \"\"\"\n",
    "  data = pd.read_csv(path, dtype=object, delimiter=\",\", header=None, skiprows=1, names=[\"id\", \"INPUT_0\", \"INPUT_1\", \"INPUT_2\", \"INPUT_3\", \"INPUT_4\", \"INPUT_5\", \"INPUT_6\", \"INPUT_7\", \"INPUT_8\", \"INPUT_9\", \"TARGET_x\", \"TARGET_y\", \"TARGET_z\"])\n",
    "  y = data.drop([\"id\",\"INPUT_0\", \"INPUT_1\", \"INPUT_2\", \"INPUT_3\", \"INPUT_4\", \"INPUT_5\", \"INPUT_6\", \"INPUT_7\", \"INPUT_8\", \"INPUT_9\"], axis=1)\n",
    "  X = data.drop([\"id\",\"TARGET_x\", \"TARGET_y\", \"TARGET_z\"], axis=1).astype(float).to_numpy()\n",
    "\n",
    "  y = y.astype(float).to_numpy()\n",
    "\n",
    "  return np.concatenate((y, X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.897453  , -35.936382  ,  21.077147  , ...,   0.40764457,\n",
       "         -0.68854785,   0.6168897 ],\n",
       "       [ -9.330632  ,  19.901571  ,   6.0691543 , ...,   0.98098207,\n",
       "          0.6617593 ,  -0.8001547 ],\n",
       "       [ 14.8494005 ,   3.3740904 ,  19.667479  , ...,   0.5991635 ,\n",
       "         -0.6846301 ,   0.9229005 ],\n",
       "       ...,\n",
       "       [  7.265506  , -53.497242  ,   2.815666  , ...,  -0.9873102 ,\n",
       "          0.9376967 ,   0.14342013],\n",
       "       [  5.5452743 , -63.348396  ,  27.98934   , ...,  -0.64811015,\n",
       "         -0.9552309 ,   0.9012979 ],\n",
       "       [  6.16061   ,   8.321016  ,  13.021444  , ...,   0.8920809 ,\n",
       "         -0.19820416,   0.49458626]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_ds(TR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToleranceStopper:\n",
    "  def __init__(self, patience=1, min_delta=0):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.counter = 0\n",
    "    self.min_training_loss = np.inf\n",
    "\n",
    "  def tol_stop(self, training_loss):\n",
    "    if training_loss > (self.min_training_loss - self.min_delta):\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.patience:\n",
    "        return True\n",
    "    else: \n",
    "      self.counter = 0\n",
    "    if training_loss < self.min_training_loss:\n",
    "      self.min_training_loss = training_loss\n",
    "          \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(input_size,hidden_size,output_size,learning_rate,momentum,weight_decay,opt,epochs,trainloader,loss_function,testloader):\n",
    "    # Init the neural network\n",
    "    network = Net(input_size, 3, output_size)\n",
    "    network.apply(reset_weights) #reset weights with random initialization\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    if (opt.__name__ == \"RMSprop\") or (opt.__name__ == \"SGD\"):\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "      # Print epoch\n",
    "      print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss and accuracy value for train\n",
    "      train_loss = 0.0\n",
    "      epoch_train_accuracy = []\n",
    "\n",
    "\n",
    "      # Set current loss and accuracy value for test\n",
    "      test_loss = 0.0\n",
    "      epoch_test_accuracy = []\n",
    "\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[:, 3:].to(torch.float32)\n",
    "        targets = data[:, [0,1,2]].to(torch.float32)\n",
    "\n",
    "        # Early stopping\n",
    "        tolerance_stopper = ToleranceStopper(patience=10, min_delta=1e-4)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "      # Print loss values\n",
    "      epoch_train_loss = train_loss / len(trainloader.sampler.indices)    \n",
    "      print(f'Training loss: {epoch_train_loss}')\n",
    "      train_losses.append(epoch_train_loss)\n",
    "      # Update accuracy\n",
    "      for output, target in zip(outputs, targets):\n",
    "        output = 0 if output.item() < 0.5 else 1\n",
    "        if output == target.item():\n",
    "          epoch_train_accuracy.append(1)\n",
    "        else:\n",
    "          epoch_train_accuracy.append(0)\n",
    "      \n",
    "      with torch.no_grad():\n",
    "        # Iterate over the testing data and generate predictions\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "\n",
    "          inputs = data[:, 3:].to(torch.float32)\n",
    "          targets = data[:, [0,1,2]].to(torch.float32)\n",
    "        \n",
    "          outputs = network(inputs)\n",
    "\n",
    "          loss = loss_function(outputs, targets)\n",
    "          \n",
    "          test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_test_loss = test_loss / len(testloader.sampler.indices)    \n",
    "        print(f'Test loss: {epoch_test_loss}')\n",
    "        test_losses.append(epoch_test_loss)\n",
    "\n",
    "        # Update accuracy\n",
    "        for output, target in zip(outputs, targets):\n",
    "          output = 0 if output.item() < 0.5 else 1\n",
    "          if output == target.item():\n",
    "            epoch_test_accuracy.append(1)\n",
    "          else:\n",
    "            epoch_test_accuracy.append(0)\n",
    "        \n",
    "        if tolerance_stopper.tol_stop(epoch_test_loss):\n",
    "          break\n",
    "\n",
    "    plot_graph(train_losses,test_losses,epochs)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_model(input_size,hidden_size,output_size,learning_rate,momentum,weight_decay,opt,epochs,trainloader,loss_function,validationloader):\n",
    "    \n",
    "    # Init the neural network\n",
    "    network = Net(input_size, 3, output_size)\n",
    "    network.apply(reset_weights) #reset weights with random initialization\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    if (opt.__name__ == \"RMSprop\") or (opt.__name__ == \"SGD\"):\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "      optimizer = opt(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    \n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "      # Print epoch\n",
    "      #print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss value\n",
    "      train_loss = 0.0\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[:, 3:].to(torch.float32)\n",
    "        targets = data[:, [0,1,2]].to(torch.float32)\n",
    "\n",
    "        # Early stopping\n",
    "        tolerance_stopper = ToleranceStopper(patience=10, min_delta=1e-4)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad() \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        #print(\"loss per item\", loss.item())\n",
    "        #print(\"inputs size\",inputs.size(0))\n",
    "\n",
    "        # Print loss values\n",
    "      #print(\"train loaders length\",len(trainloader.sampler.indices))\n",
    "      avg_train_loss = train_loss / len(trainloader.sampler.indices)    \n",
    "      #print(f'Training loss: {avg_train_loss}')\n",
    "      # Print about testing\n",
    "      #print('Starting validation')\n",
    "\n",
    "      # Evaluationfor this fold\n",
    "      valid_loss = 0.0 \n",
    "      with torch.no_grad():\n",
    "        # Iterate over the validation data and generate predictions\n",
    "        for i, data in enumerate(validationloader, 0):\n",
    "\n",
    "          # Get inputs\n",
    "          inputs = data[:, 3:].to(torch.float32)\n",
    "          targets = data[:, [0,1,2]].to(torch.float32)\n",
    "          \n",
    "          # Generate outputs\n",
    "          outputs = network(inputs)\n",
    "\n",
    "          loss = loss_function(outputs, targets)\n",
    "\n",
    "          # Calculate loss\n",
    "          valid_loss += loss.item() * inputs.size(0)\n",
    "          #print(\"loss per item\", loss.item())\n",
    "          #print(\"inputs size\",inputs.size(0))\n",
    "        \n",
    "        #print(\"validation loaders length\",len(validationloader.sampler.indices))\n",
    "        avg_valid_loss = valid_loss / len(validationloader.sampler.indices) #used to find the best parameters of the model\n",
    "        # Early stopping\n",
    "        if tolerance_stopper.tol_stop(avg_valid_loss):\n",
    "          break\n",
    "        # Print validation results\n",
    "        #print(f'Validation loss: {avg_valid_loss:.4f}')\n",
    "        \n",
    "\n",
    "    return avg_valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Actual iter 0.0%\n",
      "Actual iter 50.0%\n",
      "Best loss: 48.003238372802734 \n",
      "Best hidden size: 4 \n",
      "Best learning rate: 0.001 \n",
      "Best batch size: 64 \n",
      "Best weight decay: 0.01 \n",
      "Best momentum: 0.09\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 3\n",
    "params_grid = {\n",
    "    \"hidden_size\": [2, 3, 4, 5],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 0.5],\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"weight_decay\": [0.0001, 0.001, 0.01],\n",
    "    \"momentum\": [0.01, 0.05, 0.1, 0.4],\n",
    "    \"epochs\":[5000],\n",
    "    \"optimizer\":[torch.optim.SGD]\n",
    "}\n",
    "\n",
    "params_grid = {\n",
    "    \"hidden_size\": [4],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"batch_size\": [64],\n",
    "    \"weight_decay\": [0.01],\n",
    "    \"momentum\": [0.09],\n",
    "    \"epochs\":[400, 600],\n",
    "    \"optimizer\":[torch.optim.SGD]\n",
    "}\n",
    "\n",
    "\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "# Set fixed random number seed\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "dataset = read_ds(TR_PATH)\n",
    "#shuffle dataset\n",
    "np.random.shuffle(dataset)\n",
    "#split dataset in train and test\n",
    "dataset_train_part, dataset_test_part = dataset[:int(len(dataset)*0.8)], dataset[int(len(dataset)*0.8):]\n",
    "\n",
    "#dataset_test_part = read_ds(TS_PATH)\n",
    "\n",
    "dataset = dataset_train_part\n",
    "\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "#kfold = CV \n",
    "\n",
    "\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "best_params = None\n",
    "\n",
    "actual_it = 0\n",
    "total_iterations = len(params_grid[\"epochs\"]) * len(params_grid[\"optimizer\"]) * len(params_grid[\"hidden_size\"]) * len(params_grid[\"learning_rate\"]) * len(params_grid[\"batch_size\"]) * len(params_grid[\"weight_decay\"]) * len(params_grid[\"momentum\"])\n",
    "print(total_iterations)\n",
    "\n",
    "for epochs, opt, hidden_size, learning_rate, batch_size, weight_decay, momentum in product(params_grid[\"epochs\"],params_grid[\"optimizer\"], params_grid[\"hidden_size\"], params_grid[\"learning_rate\"], params_grid[\"batch_size\"], params_grid[\"weight_decay\"], params_grid[\"momentum\"]):\n",
    "    validation_avg_loss_fold = 0\n",
    "    num_iterations = 0\n",
    "    #print the actual percentage of the grid search\n",
    "    print(f'Actual iter {(actual_it/total_iterations)*100}%')\n",
    "    kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    kf_split = kf.split(dataset)\n",
    "\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kf_split):\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids, gen) \n",
    "        validation_subsampler = torch.utils.data.SubsetRandomSampler(val_ids, gen) \n",
    "        # Print\n",
    "        #print(f'FOLD {fold}')\n",
    "\n",
    "        #print('--------------------------------')\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        \n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=batch_size, sampler=train_subsampler)\n",
    "        validationloader = torch.utils.data.DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size=batch_size, sampler=validation_subsampler)    \n",
    "        \n",
    "\n",
    "        validation_loss = k_fold_model(learning_rate=learning_rate,epochs=epochs,hidden_size=hidden_size,input_size=input_size,loss_function=loss_function,momentum=momentum\n",
    "                                                    ,opt=opt,output_size=output_size,trainloader=trainloader,weight_decay=weight_decay,validationloader=validationloader)   \n",
    "        validation_avg_loss_fold  += validation_loss\n",
    "        num_iterations += 1\n",
    "\n",
    "    actual_it = actual_it + 1\n",
    "\n",
    "    #validation average over all folds\n",
    "    validation_avg_loss_fold /= num_iterations\n",
    "\n",
    "    #best \n",
    "    if best_params is None or validation_avg_loss_fold < best_params[0]:\n",
    "        best_params = (validation_avg_loss_fold,learning_rate,epochs,hidden_size, loss_function,momentum,opt,weight_decay,batch_size)\n",
    "\n",
    "\n",
    "print(f\"Best loss: {best_params[0]} \\nBest hidden size: {best_params[3]} \\nBest learning rate: {best_params[1]} \\nBest batch size: {best_params[8]} \\nBest weight decay: {best_params[7]} \\nBest momentum: {best_params[5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss: 48.003238372802734 \n",
      "Best hidden size: 4 \n",
      "Best learning rate: 0.001 \n",
      "Best batch size: 64 \n",
      "Best weight decay: 0.01 \n",
      "Best momentum: 0.09\n"
     ]
    }
   ],
   "source": [
    "#Best parameters found\n",
    "print(f\"Best loss: {best_params[0]} \\nBest hidden size: {best_params[3]} \\nBest learning rate: {best_params[1]} \\nBest batch size: {best_params[8]} \\nBest weight decay: {best_params[7]} \\nBest momentum: {best_params[5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Training loss: 777.2550634765626\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 3 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[330], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m test_subsampler \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mSubsetRandomSampler(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset_test_part)), gen)\n\u001b[1;32m      8\u001b[0m testloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      9\u001b[0m                       dataset_test_part, \n\u001b[1;32m     10\u001b[0m                       batch_size\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;241m8\u001b[39m], sampler\u001b[38;5;241m=\u001b[39mtest_subsampler)\n\u001b[0;32m---> 13\u001b[0m best_net \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestloader\u001b[49m\u001b[43m)\u001b[49m   \n",
      "Cell \u001b[0;32mIn[326], line 64\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(input_size, hidden_size, output_size, learning_rate, momentum, weight_decay, opt, epochs, trainloader, loss_function, testloader)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Update accuracy\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, targets):\n\u001b[0;32m---> 64\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;241m==\u001b[39m target\u001b[38;5;241m.\u001b[39mitem():\n\u001b[1;32m     66\u001b[0m     epoch_train_accuracy\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 3 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "train_subsampler = torch.utils.data.SubsetRandomSampler(range(len(dataset_train_part)), gen)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset_train_part, \n",
    "                      batch_size=best_params[8], sampler=train_subsampler)\n",
    "\n",
    "test_subsampler =  torch.utils.data.SubsetRandomSampler(range(len(dataset_test_part)), gen)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "                      dataset_test_part, \n",
    "                      batch_size=best_params[8], sampler=test_subsampler)\n",
    "\n",
    "\n",
    "best_net = fit_model(learning_rate=best_params[1],epochs=best_params[2],hidden_size=best_params[3],input_size=10,loss_function=best_params[4],momentum=best_params[5],opt=best_params[6],output_size=3,trainloader=trainloader,weight_decay=best_params[7],testloader=testloader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[306], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m val_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(dataset_test_part[:, [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m]])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Print accuracy on test set\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m test_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbest_net\u001b[49m(test_data)\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_net(test_data)\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_net' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "test_data = torch.from_numpy(dataset_test_part[:, 3:]).to(torch.float32)\n",
    "val_labels = torch.from_numpy(dataset_test_part[:, [0,1,2]]).to(torch.float32)\n",
    "\n",
    "#Print accuracy on test set\n",
    "test_outputs = best_net(test_data).round().int().view(-1)\n",
    "\n",
    "y_pred = best_net(test_data)\n",
    "y_pred = y_pred.round().int().view(-1)\n",
    "print(\"accuracy on test set {:.3f}\".format(accuracy_score( val_labels,y_pred)))\n",
    "print(classification_report(val_labels, \n",
    "                            y_pred, \n",
    "                            target_names=['0', '1']))\n",
    "\n",
    "#print the confusion matrix\n",
    "cm = confusion_matrix(val_labels, test_outputs)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
