{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from itertools import product\n",
    "from monk_helpers import CV,SEED\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "SEEDS = list(range(40,46))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets Path\n",
    "TR_PATH_1 = \"./monks/datasets/monks-1.train\"\n",
    "TS_PATH_1 = \"./monks/datasets/monks-1.test\"\n",
    "# Datasets Path\n",
    "TR_PATH_2 = \"./monks/datasets/monks-2.train\"\n",
    "TS_PATH_2 = \"./monks/datasets/monks-2.test\"\n",
    "# Datasets Path\n",
    "TR_PATH_3 = \"./monks/datasets/monks-3.train\"\n",
    "TS_PATH_3 = \"./monks/datasets/monks-3.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_graph(train_losses,losses,epochs,title):\n",
    "    num_epochs = list(range(0, epochs))  \n",
    "    # Plotting\n",
    "    plt.plot(num_epochs, train_losses, label=' Training',linestyle='-')\n",
    "    plt.plot(num_epochs, losses, label=title+' MSE',linestyle='--')\n",
    "\n",
    "    plt.title('Training and '+title+' Losses Across Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_graph_accuracy(train_accuracies, accuracies, epochs):\n",
    "    num_epochs = list(range(0, epochs))\n",
    "    \n",
    "    # Plotting\n",
    "    print(\"TRAIN ACCURACY:\", train_accuracies[-1])\n",
    "    plt.plot(num_epochs, train_accuracies, label='Training Accuracy',linestyle='-')\n",
    "    plt.plot(num_epochs, accuracies, label='Test Accuracy',linestyle='--')\n",
    "\n",
    "    plt.title('Training and Test Accuracies Across Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, units, output_size):\n",
    "    super().__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.l1 = nn.Linear(input_size, units)\n",
    "    self.l2 = nn.Linear(units, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.tanh(self.l1(x))\n",
    "    out = torch.sigmoid(self.l2(out))\n",
    "    return out\n",
    "\n",
    "\n",
    "def reset_weights(net):\n",
    "  for param in net.parameters():\n",
    "    torch.nn.init.uniform_(param, a=-0.7, b=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ds(path):\n",
    "  \"\"\"\n",
    "  parse CSV data set and\n",
    "  returns a tuple (input, target)\n",
    "  \"\"\"\n",
    "  names = ['class', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'id']\n",
    "  data = pd.read_csv(path, dtype=object, delim_whitespace=True, header=None, skipinitialspace=True, names=names)\n",
    "\n",
    "  X = data.drop(['class','id'], axis=1)\n",
    "  X = pd.get_dummies(X).astype(float).to_numpy()\n",
    "  y = data.drop(['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'id'], axis=1)\n",
    "  y = y.astype(float).to_numpy()\n",
    "\n",
    "\n",
    "  return np.concatenate((y, X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "  def __init__(self, epochs_to_wait=1, min_delta=0):\n",
    "    self.min_training_loss = np.inf\n",
    "    self.epochs_to_wait = epochs_to_wait\n",
    "    self.min_delta = min_delta\n",
    "    self.counter = 0\n",
    "\n",
    "  def check_early_stop(self, training_loss):\n",
    "    if training_loss > (self.min_training_loss - self.min_delta):\n",
    "      self.counter +=1\n",
    "      if self.counter >= self.epochs_to_wait:\n",
    "        return True\n",
    "    else: \n",
    "      self.counter = 0\n",
    "    if training_loss < self.min_training_loss:\n",
    "      self.min_training_loss = training_loss\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Padding(validation_losses_fold,train_losses_fold):\n",
    "    max_epochs = max(map(len, validation_losses_fold))\n",
    "\n",
    "    for validation_loss_arr in validation_losses_fold:\n",
    "        while len(validation_loss_arr) < max_epochs:\n",
    "            validation_loss_arr.append(validation_loss_arr[-1])\n",
    "\n",
    "    for train_loss_arr in train_losses_fold:\n",
    "      while len(train_loss_arr) < max_epochs:\n",
    "            train_loss_arr.append(train_loss_arr[-1])\n",
    "\n",
    "    print(len(validation_losses_fold))\n",
    "    return validation_losses_fold,train_losses_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mean(validation_avg_loss_fold,train_losses_fold,n_folds):\n",
    "    max_epochs = max(map(len, validation_avg_loss_fold))\n",
    "    validation_avg_loss = []\n",
    "    train_avg_loss = []\n",
    "    \n",
    "    for i in range(0,max_epochs):\n",
    "        temp_loss = 0\n",
    "        for j in range(0,len(validation_avg_loss_fold)):\n",
    "            temp_loss += validation_avg_loss_fold[j][i]\n",
    "        validation_avg_loss.append(temp_loss/n_folds)\n",
    "\n",
    "    for i in range(0,max_epochs):\n",
    "        temp_loss = 0\n",
    "        for j in range(0,len(train_losses_fold)):\n",
    "            temp_loss += train_losses_fold[j][i]\n",
    "        train_avg_loss.append(temp_loss/n_folds)\n",
    "    \n",
    "    return validation_avg_loss,train_avg_loss\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(input_size,hidden_size,output_size,learning_rate,momentum,weight_decay,opt,epochs,trainloader,loss_function,testloader):\n",
    "    # Init the neural network\n",
    "    network = Net(input_size, hidden_size, output_size)\n",
    "    network.apply(reset_weights) #reset weights with random initialization\n",
    "\n",
    "    optimizer = opt(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    \n",
    "\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "\n",
    "    \n",
    "    epoch_train_accuracies = []\n",
    "    epoch_test_accuracies = []\n",
    "\n",
    "    early_stopper = EarlyStopper(epochs_to_wait=40, min_delta=1e-5)\n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "      print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss and accuracy value for train\n",
    "      train_loss = 0.0\n",
    "      epoch_train_accuracy = 0.0\n",
    "\n",
    "      # Set current loss and accuracy value for test\n",
    "      test_loss = 0.0\n",
    "      epoch_test_accuracy = 0.0\n",
    "\n",
    "\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[:, 1:].to(torch.float32)\n",
    "        targets = data[:, [0]].to(torch.float32)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Update accuracy\n",
    "        for output, target in zip(outputs, targets):\n",
    "          output = 0 if output.item() < 0.5 else 1\n",
    "          if output == target.item():\n",
    "            epoch_train_accuracy += 1\n",
    "\n",
    "      # Print loss values\n",
    "      epoch_train_loss = train_loss / len(trainloader.sampler)\n",
    "      train_losses.append(epoch_train_loss)\n",
    "\n",
    "      epoch_train_accuracy /= len(trainloader.sampler)\n",
    "\n",
    "      epoch_train_accuracies.append(epoch_train_accuracy)\n",
    "\n",
    "      \n",
    "      with torch.no_grad():\n",
    "        # Iterate over the testing data and generate predictions\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "\n",
    "          inputs = data[:, 1:].to(torch.float32)\n",
    "          targets = data[:, [0]].to(torch.float32)\n",
    "        \n",
    "          outputs = network(inputs)\n",
    "\n",
    "          loss = loss_function(outputs, targets)\n",
    "          \n",
    "          test_loss += loss.item() * inputs.size(0)\n",
    "          # Update accuracy\n",
    "          for output, target in zip(outputs, targets):\n",
    "            output = 0 if output.item() < 0.5 else 1\n",
    "            if output == target.item():\n",
    "              epoch_test_accuracy += 1\n",
    "\n",
    "        epoch_test_loss = test_loss / len(testloader.sampler)    \n",
    "        test_losses.append(epoch_test_loss)\n",
    "\n",
    "\n",
    "        \n",
    "        epoch_test_accuracy /= len(testloader.sampler)\n",
    "\n",
    "        epoch_test_accuracies.append(epoch_test_accuracy)\n",
    "        \n",
    "        print(epoch_test_loss)\n",
    "        if early_stopper.check_early_stop(epoch_test_loss):\n",
    "          print(\"Early stopping: \",epoch)\n",
    "          break\n",
    "\n",
    "    plot_graph(train_losses,test_losses,epoch+1,\"test\")\n",
    "    plot_graph_accuracy(epoch_train_accuracies,epoch_test_accuracies,epoch+1)\n",
    "\n",
    "    return network,train_losses[-1],test_losses[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_model(input_size,hidden_size,output_size,learning_rate,momentum,weight_decay,opt,epochs,trainloader,loss_function,validationloader):\n",
    "    \n",
    "    # Init the neural network\n",
    "    network = Net(input_size, hidden_size, output_size)\n",
    "    network.apply(reset_weights) #reset weights with random initialization\n",
    "    \n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    early_stopper = EarlyStopper(epochs_to_wait=40, min_delta=1e-5)\n",
    "    train_losses = []\n",
    "    validaition_losses = []\n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "      # Set current loss value\n",
    "      train_loss = 0.0\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[:, 1:].to(torch.float32)\n",
    "        targets = data[:, [0]].to(torch.float32)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "      avg_train_loss = train_loss / len(trainloader.sampler)    \n",
    "      train_losses.append(avg_train_loss) \n",
    "\n",
    "      # Evaluationfor this fold\n",
    "      valid_loss = 0.0 \n",
    "      with torch.no_grad():\n",
    "        # Iterate over the validation data and generate predictions\n",
    "        for i, data in enumerate(validationloader, 0):\n",
    "\n",
    "          # Get inputs\n",
    "          inputs = data[:, 1:].to(torch.float32)\n",
    "          targets = data[:, [0]].to(torch.float32)\n",
    "          \n",
    "          # Generate outputs\n",
    "          outputs = network(inputs)\n",
    "\n",
    "          loss = loss_function(outputs, targets)\n",
    "\n",
    "          # Calculate loss\n",
    "          valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "      \n",
    "        avg_valid_loss = valid_loss / len(validationloader.sampler) #used to find the best parameters of the model\n",
    "        validaition_losses.append(avg_valid_loss)\n",
    "\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopper.check_early_stop(avg_valid_loss):\n",
    "          print(\"Early stopping:\", epoch)\n",
    "          break\n",
    "      \n",
    "    return avg_valid_loss,avg_train_loss,validaition_losses,train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_folds(kfold,dataset,batch_size,input_size, hidden_size, output_size, learning_rate, epochs,\n",
    "    loss_function, momentum, opt, weight_decay):\n",
    "\n",
    "    validation_avg_loss_fold = 0\n",
    "    train_avg_loss_fold = 0\n",
    "    validation_losses_fold = []\n",
    "    train_losses_fold = []\n",
    "    num_iterations = 0\n",
    "    current_config = {\n",
    "        'input_size': input_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'output_size': output_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'epochs': epochs,\n",
    "        'momentum': momentum,\n",
    "        'opt': opt,\n",
    "        'weight_decay': weight_decay,\n",
    "        \"batch_size\":batch_size,\n",
    "        \"loss_function\":loss_function\n",
    "    }\n",
    " \n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(np.zeros(len(dataset)),dataset[:, 0])):\n",
    "\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids, gen) \n",
    "        validation_subsampler = torch.utils.data.SubsetRandomSampler(val_ids, gen) \n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=batch_size, sampler=train_subsampler)\n",
    "        validationloader = torch.utils.data.DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size=batch_size, sampler=validation_subsampler)    \n",
    "        \n",
    "\n",
    "        validation_loss,train_loss,validation_losses,train_losses = k_fold_model(learning_rate=learning_rate,epochs=epochs,hidden_size=hidden_size,input_size=input_size,loss_function=loss_function,momentum=momentum\n",
    "                                                    ,opt=opt,output_size=output_size,trainloader=trainloader,weight_decay=weight_decay,validationloader=validationloader)   \n",
    "        validation_avg_loss_fold  += validation_loss\n",
    "        train_avg_loss_fold += train_loss\n",
    "        validation_losses_fold.append(validation_losses)\n",
    "        train_losses_fold.append(train_losses)\n",
    "\n",
    "        num_iterations += 1\n",
    "\n",
    "\n",
    "    #validation and train average over all folds\n",
    "    validation_avg_loss_fold /= num_iterations\n",
    "    train_avg_loss_fold /= num_iterations\n",
    "    \n",
    "    validation_losses_fold,train_losses_fold = Padding(validation_losses_fold,train_losses_fold)\n",
    "\n",
    "    validation_losses_mean, train_losses_mean = Mean(validation_losses_fold, train_losses_fold,n_folds=num_iterations)\n",
    "    \n",
    "    plot_graph(train_losses_mean,validation_losses_mean,len(validation_losses),\"validation\") \n",
    "    \n",
    "\n",
    "\n",
    "    return (validation_avg_loss_fold,train_avg_loss_fold,current_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that executes the folds for each combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dogridsearch(dataset_train_part,params_grid,output_size,input_size,seeds):\n",
    "    \n",
    "    dataset = dataset_train_part\n",
    "    # Set fixed random number seed\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = CV \n",
    "    \n",
    "    # K-fold Cross Validation model evaluation\n",
    "    best_params = None\n",
    "\n",
    "    actual_it = 0\n",
    "    total_iterations = len(params_grid[\"epochs\"]) * len(params_grid[\"optimizer\"]) * len(params_grid[\"hidden_size\"]) * len(params_grid[\"learning_rate\"]) * len(params_grid[\"batch_size\"]) * len(params_grid[\"weight_decay\"]) * len(params_grid[\"momentum\"]) \n",
    "\n",
    "\n",
    "    configurations = []\n",
    "\n",
    "    for epochs, opt, hidden_size, learning_rate, batch_size, weight_decay, momentum in product(params_grid[\"epochs\"],params_grid[\"optimizer\"], params_grid[\"hidden_size\"], params_grid[\"learning_rate\"], params_grid[\"batch_size\"], params_grid[\"weight_decay\"], params_grid[\"momentum\"]):\n",
    "        #print the actual percentage of the grid search\n",
    "        print(f'Actual iter {(actual_it/total_iterations)*100}%')\n",
    "        for seed in seeds:\n",
    "            print(\"Working with seed:\",seed)\n",
    "            torch.manual_seed(seed)\n",
    "            (validation_avg_loss_fold,train_avg_loss_fold,current_config) = execute_folds(kfold=kfold,dataset=dataset,learning_rate=learning_rate,epochs=epochs,hidden_size=hidden_size,input_size=input_size,loss_function=loss_function,momentum=momentum\n",
    "                                                            ,opt=opt,output_size=output_size,weight_decay=weight_decay,batch_size=batch_size)\n",
    "            configurations.append((validation_avg_loss_fold, train_avg_loss_fold,current_config))\n",
    "        \n",
    "        actual_it += 1\n",
    "\n",
    "    val_mse = []\n",
    "    train_mse = []\n",
    "    #best \n",
    "    for conf_val in configurations:\n",
    "        val_mse.append(conf_val[0])\n",
    "        train_mse.append(conf_val[1])\n",
    "        if best_params is None or conf_val[0] < best_params[0]:\n",
    "                current_config = conf_val[2]\n",
    "                best_params = (conf_val[0],conf_val[1]\n",
    "                ,current_config['learning_rate'], current_config['epochs'],current_config[\"loss_function\"],current_config['hidden_size'],current_config['momentum'],current_config['opt'],\n",
    "                current_config['weight_decay'],\n",
    "                current_config['batch_size'])\n",
    "\n",
    "    print(\"TRAIN MEAN MSE\",np.mean(train_mse))\n",
    "    print(\"TRAIN STD\",np.std(train_mse))\n",
    "    print(\"VALIDATION MEAN MSE\",np.mean(val_mse))\n",
    "    print(\"VALIDATION STD\",np.std(val_mse))\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the created model and plot training/test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(dataset_train_part,dataset_test_part,best_params,seeds):\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(range(len(dataset_train_part)), gen)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                        dataset_train_part, \n",
    "                        batch_size=best_params[9], sampler=train_subsampler)\n",
    "\n",
    "    test_subsampler =  torch.utils.data.SubsetRandomSampler(range(len(dataset_test_part)), gen)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                        dataset_test_part, \n",
    "                        batch_size=best_params[9], sampler=test_subsampler)\n",
    "    train_error_seed = []\n",
    "    test_error_seed = []\n",
    "    for seed in seeds:\n",
    "        # Start the timer\n",
    "        start = time.time()\n",
    "        print(\"Working with seed:\",seed)\n",
    "        torch.manual_seed(seed)\n",
    "        best_net,train_error,test_error = fit_model(learning_rate=best_params[2],epochs=best_params[3],hidden_size=best_params[5],input_size=17,loss_function=best_params[4],\n",
    "                        momentum=best_params[6],opt=best_params[7],output_size=1,trainloader=trainloader,weight_decay=best_params[8],testloader=testloader) \n",
    "        train_error_seed.append(train_error)\n",
    "        test_error_seed.append(test_error)\n",
    "        end = time.time()\n",
    "        print(\"Refit Time: {:.2f} seconds\".format(end - start))\n",
    "\n",
    "\n",
    "    print(\"TRAIN MEAN MSE\",np.mean(train_error_seed))\n",
    "    print(\"TRAIN STD\",np.std(train_error_seed))\n",
    "    print(\"TEST MEAN MSE\",np.mean(test_error_seed))\n",
    "    print(\"TEST STD\",np.std(test_error_seed))\n",
    "\n",
    "    return best_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 17  \n",
    "output_size = 1\n",
    "\n",
    "'''params_grid_wide = {\n",
    "    \"hidden_size\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.1,0.4,0.7,0.8],\n",
    "    \"batch_size\": [4,8,12,32],\n",
    "    \"weight_decay\": [0],\n",
    "    \"momentum\": [0,0.1,0.4,0.7,0.8],\n",
    "    \"epochs\":[600],\n",
    "    \"optimizer\":[torch.optim.SGD],\n",
    "}\n",
    "\n",
    "\n",
    "params_grid_precise = {\n",
    "    \"hidden_size\": [4,5],\n",
    "    \"learning_rate\": [0.7, 0.08, 0.9],\n",
    "    \"batch_size\": [4],\n",
    "    \"weight_decay\": [0],\n",
    "    \"momentum\": [0.4,0.5,0.6],\n",
    "    \"epochs\":[600],\n",
    "    \"optimizer\":[torch.optim.SGD],\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "params_grid = {\n",
    "    \"hidden_size\": [4],\n",
    "    \"learning_rate\": [0.8],\n",
    "    \"batch_size\": [4],\n",
    "    \"weight_decay\": [0],\n",
    "    \"momentum\": [0.5],\n",
    "    \"epochs\":[600],\n",
    "    \"optimizer\":[torch.optim.SGD],\n",
    "\n",
    "}\n",
    "    \n",
    "\n",
    "dataset_train_part = read_ds(TR_PATH_1)\n",
    "dataset_test_part = read_ds(TS_PATH_1)\n",
    "\n",
    "dataset = dataset_train_part\n",
    "\n",
    "\n",
    "\n",
    "best_params = dogridsearch(dataset_train_part=dataset_train_part,params_grid=params_grid,output_size=output_size,input_size=input_size,seeds = SEEDS)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters found\n",
    "print(f\"Best hidden size: {best_params[5]} \\nBest learning rate: {best_params[2]} \\nBest batch size: {best_params[9]} \\nBest weight decay: {best_params[8]} \\nBest momentum: {best_params[6]}\")\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = train_test_model(dataset_train_part=dataset_train_part,dataset_test_part=dataset_test_part,best_params=best_params,seeds=SEEDS)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.from_numpy(dataset_test_part[:, 1:]).to(torch.float32)\n",
    "val_labels = torch.from_numpy(dataset_test_part[:, [0]]).to(torch.float32)\n",
    "\n",
    "#Print accuracy on test set\n",
    "test_outputs = best_net(test_data).round().int().view(-1)\n",
    "\n",
    "y_pred = best_net(test_data)\n",
    "y_pred = y_pred.round().int().view(-1)\n",
    "print(\"accuracy on test set {:.3f}\".format(accuracy_score( val_labels,y_pred)))\n",
    "print(classification_report(val_labels, \n",
    "                            y_pred, \n",
    "                            target_names=['0', '1']))\n",
    "\n",
    "#print the confusion matrix\n",
    "cm = confusion_matrix(val_labels, test_outputs)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MONK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 17  \n",
    "output_size = 1\n",
    "\n",
    "'''params_grid_wide = {\n",
    "    \"hidden_size\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.1,0.4,0.7,0.8],\n",
    "    \"batch_size\": [4,8,12,32],\n",
    "    \"weight_decay\": [0],\n",
    "    \"momentum\": [0,0.1,0.4,0.7,0.8],\n",
    "    \"epochs\":[600],\n",
    "    \"optimizer\":[torch.optim.SGD],\n",
    "}\n",
    "\n",
    "\n",
    "params_grid_precise = {\n",
    "    \"hidden_size\": [4,5],\n",
    "    \"learning_rate\": [0.7, 0.08, 0.9],\n",
    "    \"batch_size\": [4],\n",
    "    \"weight_decay\": [0],\n",
    "    \"momentum\": [0.4,0.5,0.6],\n",
    "    \"epochs\":[600],\n",
    "    \"optimizer\":[torch.optim.SGD],\n",
    "}\n",
    "'''\n",
    "\n",
    "params_grid = {\n",
    "    \"hidden_size\": [4],\n",
    "    \"learning_rate\": [0.8],\n",
    "    \"batch_size\": [4],\n",
    "    \"weight_decay\": [0],\n",
    "    \"momentum\": [0.5],\n",
    "    \"epochs\":[600],\n",
    "    \"optimizer\":[torch.optim.SGD],\n",
    "\n",
    "\n",
    "}\n",
    "    \n",
    "\n",
    "dataset_train_part = read_ds(TR_PATH_2)\n",
    "dataset_test_part = read_ds(TS_PATH_2)\n",
    "\n",
    "dataset = dataset_train_part\n",
    "\n",
    "best_params = dogridsearch(dataset_train_part=dataset_train_part,params_grid=params_grid,output_size=output_size,input_size=input_size,seeds = [SEED])\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters found\n",
    "print(f\"Best hidden size: {best_params[5]} \\nBest learning rate: {best_params[2]} \\nBest batch size: {best_params[9]} \\nBest weight decay: {best_params[8]} \\nBest momentum: {best_params[6]}\")\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = train_test_model(dataset_train_part=dataset_train_part,dataset_test_part=dataset_test_part,best_params=best_params,seeds=SEEDS)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.from_numpy(dataset_test_part[:, 1:]).to(torch.float32)\n",
    "val_labels = torch.from_numpy(dataset_test_part[:, [0]]).to(torch.float32)\n",
    "\n",
    "#Print accuracy on test set\n",
    "test_outputs = best_net(test_data).round().int().view(-1)\n",
    "\n",
    "y_pred = best_net(test_data)\n",
    "y_pred = y_pred.round().int().view(-1)\n",
    "print(\"accuracy on test set {:.3f}\".format(accuracy_score( val_labels,y_pred)))\n",
    "print(classification_report(val_labels, \n",
    "                            y_pred, \n",
    "                            target_names=['0', '1']))\n",
    "\n",
    "#print the confusion matrix\n",
    "cm = confusion_matrix(val_labels, test_outputs)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MONK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 17  \n",
    "output_size = 1\n",
    "'''params_grid_wide = {\n",
    "    \"hidden_size\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.1,0.4,0.7,0.8],\n",
    "    \"batch_size\": [4,8,12,32],\n",
    "    \"weight_decay\": [0.0001,0.02,0.001,0.1,0.2],\n",
    "    \"momentum\": [0,0.1,0.4,0.7,0.8],\n",
    "    \"epochs\":[600],\n",
    "    \"optimizer\":[torch.optim.SGD],\n",
    "}\n",
    "\n",
    "\n",
    "params_grid_precise = {\n",
    "    \"hidden_size\": [4,5],\n",
    "    \"learning_rate\": [0.1, 0.05, 0.2],\n",
    "    \"batch_size\": [16,32],\n",
    "    \"weight_decay\": [0.01,0.1,0.2],\n",
    "    \"momentum\": [0.4, 0.01, 0.05, 0.1],\n",
    "    \"epochs\":[600],\n",
    "    \"optimizer\":[torch.optim.SGD],\n",
    "}\n",
    "'''\n",
    "params_grid = {\n",
    "    \"hidden_size\": [4],\n",
    "    \"learning_rate\": [0.1],\n",
    "    \"batch_size\": [32],\n",
    "    \"weight_decay\": [0.01],\n",
    "    \"momentum\": [0.4],\n",
    "    \"epochs\":[600],\n",
    "    \"optimizer\":[torch.optim.SGD],\n",
    "}\n",
    "\n",
    "dataset_train_part = read_ds(TR_PATH_3)\n",
    "dataset_test_part = read_ds(TS_PATH_3)\n",
    "\n",
    "dataset = dataset_train_part\n",
    "best_params = dogridsearch(dataset_train_part=dataset_train_part,params_grid=params_grid,output_size=output_size,input_size=input_size,seeds = [SEED])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters found\n",
    "print(f\"Best hidden size: {best_params[5]} \\nBest learning rate: {best_params[2]} \\nBest batch size: {best_params[9]} \\nBest weight decay: {best_params[8]} \\nBest momentum: {best_params[6]}\")\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = train_test_model(dataset_train_part=dataset_train_part,dataset_test_part=dataset_test_part,best_params=best_params,seeds=SEEDS)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.from_numpy(dataset_test_part[:, 1:]).to(torch.float32)\n",
    "val_labels = torch.from_numpy(dataset_test_part[:, [0]]).to(torch.float32)\n",
    "\n",
    "#Print accuracy on test set\n",
    "test_outputs = best_net(test_data).round().int().view(-1)\n",
    "\n",
    "y_pred = best_net(test_data)\n",
    "y_pred = y_pred.round().int().view(-1)\n",
    "print(\"accuracy on test set {:.3f}\".format(accuracy_score( val_labels,y_pred)))\n",
    "print(classification_report(val_labels, \n",
    "                            y_pred, \n",
    "                            target_names=['0', '1']))\n",
    "\n",
    "#print the confusion matrix\n",
    "cm = confusion_matrix(val_labels, test_outputs)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
